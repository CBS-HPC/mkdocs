{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC &amp; Data Science Support at CBS","text":"<p>This is the GitHub repository for HPC &amp; Data Science Support at CBS.</p>"},{"location":"#tutorials-user-utilities","title":"Tutorials &amp; User Utilities","text":"<p>Will be updated</p>"},{"location":"#teaching-activities","title":"Teaching Activities","text":"<p>Will be updated</p>"},{"location":"#development-of-hpc-at-cbs","title":"Development of HPC at CBS","text":"<p>Will be updated</p>"},{"location":"#daily-user-support","title":"Daily User Support","text":"<p>As Deic Front Office at CBS are we in charge off all communications with HPC system adminstrators (Back Office) and DeiC.</p> <p>Ideally, all user requests and troubleshooting should be send to the CBS Front Office(rdm@cbs.dk) as a Single Point of Contact (SPOC) where resulting tickets will be directed accordingly. </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"#research-consultancy","title":"Research Consultancy","text":"<p>Will be updated</p>"},{"location":"contact/","title":"Contact","text":"<ul> <li>Research &amp; Data Management @ CBS (rdm@cbs.dk)</li> <li>Kristoffer Gulmark Poulsen (kgp.lib@cbs.dk)</li> <li>Lars Nondal (ln.lib@cbs.dk)</li> </ul>"},{"location":"events/","title":"Events","text":""},{"location":"events/#upcoming","title":"Upcoming","text":"<p>\u00a0\u00a023-06-15 @ 11.00-12.00: - HPC &amp; Parallel Programming in Python (For Researchers)</p>"},{"location":"events/#past","title":"Past","text":"<p>\u00a0\u00a023-06-08 @ 11.00-12.00: - High Performance Computing (For Researchers)</p> <p>\u00a0\u00a023-06-14 @ 11.00-12.00: - HPC &amp; Parallel Programming in R (For Researchers)</p>"},{"location":"getresources/","title":"Get Resources","text":""},{"location":"getresources/#local-resources","title":"Local Resources","text":"<p>Twice a year CBS is awarded Local HPC ressources that can be freely distributed to our researchers and students. </p> <p>Type 1 </p> <p>Currently, CBS primarily have Local Type 1 resources as the reflects our current user needs. See here to apply.</p> <p>Type 3</p> <p>Please contact RDM Support.</p> <p>Other</p> <p>Please contact RDM Support if you would like to CBS to request Local resources to Type 2 and 5.</p>"},{"location":"getresources/#sandbox-resources","title":"Sandbox Resources","text":"<p>CBS researchers wanting to test out HPC systems Type 2 to 5 can gain acess to sandbox ressources by contacting RDM Support. Find more information here.</p>"},{"location":"getresources/#grant-applications","title":"Grant Applications","text":"<p>Find an overview of currently open application rounds here. Please contact RDM Support as soon as possible if you consider applying as we can aid in the application process.</p> <p>Type 1-3: Researcher can apply for the bi-annual application round for the national HPC resources. </p> <p>Type 5 &amp; other international HPC systems: Researcher can apply for resources at LUMI and other international HPC facilities. </p> <p>Current Calls</p> <ul> <li>EuroHPC JU Call for Proposals for Regular Access Mode  - (Deadline: 2023-07-07)</li> <li>DeiC - Call H1-2024 Call for applications for access to the e- resources - (Deadline: July-August 2023)</li> </ul>"},{"location":"hpc_facilities/","title":"HPC Facilites","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>National HPC Facilities (DeiC)</li> <li>WRDS - Wharton Research Data Services</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> <li>HPC Operational Status</li> </ul>"},{"location":"news/","title":"News","text":"<ul> <li>23-06-12 - New way to use SSH for accessing apps on Interactive HPC</li> <li>23-05-31 - UCloud Maintenance notice - 27/06/2023</li> <li>23-05-02 - New Tutorial \"Batch Processing on UCloud\"</li> <li>23-04-26 - UCloud scheduled maintenance between 8:00-10:00 on Wednesday 26/04/2023.</li> <li>23-04-18 - New Tutorial \"Conda on UCloud\"</li> <li>23-04-12 - New milestone as DeiC Interactive HPC reaches 6,000 users</li> <li>23-03-17 - The cost of success \u2013 user overload on DeiC Interactive HPC</li> <li>23-03-15 - Launch of the DeiC Integration Portal</li> </ul>"},{"location":"tut_docs/","title":"Tutorials &amp; Documentation","text":""},{"location":"tut_docs/#cbs-tutorials","title":"CBS Tutorials","text":"<ul> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Virtual Machines on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul>"},{"location":"tut_docs/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>SDU</p> <ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul> <p>AAU</p> <ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"tut_docs/#type-2","title":"TYPE 2","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"tut_docs/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"tut_docs/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"tut_docs/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"HPC_Facilities/docs/DeiC/","title":"DeiC","text":""},{"location":"HPC_Facilities/docs/DeiC/#national-hpc-facilities","title":"National HPC Facilities","text":"<p>This page provides an overview of the national HPC facilities (overview provided by DeiC). </p>"},{"location":"HPC_Facilities/docs/DeiC/#type-1-interactive-hpc-ucloud","title":"Type 1 \u2013 Interactive HPC (UCloud)","text":"<p>The type 1 system is mainly focused on interactive computing and easy access for users. The system is made of the YouGene cluster hosted at SDU. CBS staff and students can access the cluster resources via UCloud. </p> <p>Get for Type 1 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/docs/DeiC/#type-2-throughput-hpc","title":"Type 2 \u2013 Throughput HPC","text":"<p>Three Type 2 HPC systems are available (Computerome 2.0,GenomeDK and Sophia). This type of HPC system typically has a large number of cores which can be a mix between cost-effective and calculation-efficient units. Type 2 also has the ability to handle large amounts of data and its main focus is on high-throughput performance. </p> <p>Get for Type 2 resources here</p> <p>More information is found here:</p> <p>Computerome 2.0  \u00a0\u00a0 | \u00a0\u00a0 GenomeDK \u00a0\u00a0 | \u00a0\u00a0 Sophia</p>"},{"location":"HPC_Facilities/docs/DeiC/#type-3-large-memory-hpc-hippo","title":"Type 3 \u2013 Large Memory HPC (Hippo)","text":"<p>This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area.  Type 3 is hosted and maintained at SDU. For the cluster specs check here. </p> <p>User guide can be found here </p> <p>Get for Type 3 resources here</p> <p>More information is found here</p>"},{"location":"HPC_Facilities/docs/DeiC/#type-5-capability-hpc-lumi","title":"Type 5 \u2013 Capability HPC (LUMI)","text":"<p>Type 5 is the European pre-exascale supercomputer LUMI. LUMI is an abbreviation for \"Large Unified Modern Infrastructure\", and will be located in CSC's data center in Kajaani, Finland. LUMI is one of three pre-exascale supercomputers to be build as part of the European EuroHPC project.</p> <p>LUMI Capability HPC provides a similar setup to DeiC Throughput HPC but with increased possibilities by virtue of state-of-the-art hardware. Specifically the interconnections between compute nodes is designed to minimize latency thereby addressing the issue of communication induced latency in distributed-memory programs running on separate nodes. Additionally the user can obtain access to large amounts of disk space also with low-latency interconnects. In this way Capability HPC enables computations that are prohibitive with DeiC Throughput HPC due to communication latency. </p> <p>Get for Type 5 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/docs/DeiC/#get-resources","title":"Get Resources","text":""},{"location":"HPC_Facilities/docs/DeiC/#local-resources","title":"Local Resources","text":"<p>Twice a year CBS is awarded Local HPC ressources that can be freely distributed to our researchers and students. </p>"},{"location":"HPC_Facilities/docs/DeiC/#type-1","title":"Type 1:","text":"<p>Currently, CBS primarily have Local Type 1 resources as the reflects our current user needs. See here to apply.</p>"},{"location":"HPC_Facilities/docs/DeiC/#type-3","title":"Type 3","text":""},{"location":"HPC_Facilities/docs/DeiC/#other","title":"Other","text":"<p>Please contact RDM Support if you would like to CBS to request Local resources to Type 2 and 5.</p>"},{"location":"HPC_Facilities/docs/DeiC/#sandbox-resources","title":"Sandbox Resources","text":"<p>CBS researchers wanting to test out HPC systems Type 2 to 5 can gain acess to sandbox ressources by contacting RDM Support. Find more information here.</p>"},{"location":"HPC_Facilities/docs/DeiC/#grant-applications","title":"Grant Applications","text":"<p>Find an overview of currently open application rounds here. Please contact RDM Support as soon as possible if you consider applying as we can aid in the application process.</p> <p>Type 1-3: Researcher can apply for the bi-annual application round for the national HPC resources. </p> <p>Type 5 &amp; other international HPC systems: Researcher can apply for resources at LUMI and other international HPC facilities. </p>"},{"location":"HPC_Facilities/docs/DeiC/#relevant-cbs-github-links","title":"Relevant CBS GitHub Links","text":"<ul> <li>DeiC Interactive HPC (UCloud)</li> <li>UCloud Grant Application Guide</li> </ul>"},{"location":"HPC_Facilities/docs/DeiC/#external-links","title":"External Links","text":"<ul> <li>Apply for national HPC resources</li> <li>Acknowledge the use of national HPC </li> <li>HPC operational status for users</li> <li>The EuroCC Knowledge Pool (Hosted by DeiC)</li> </ul>"},{"location":"HPC_Facilities/docs/DeiC/#this-page-is-adopted-from-the-following-sources","title":"This page is adopted from the following sources:","text":"<p>https://hpc.ruc.dk/national-hpc-facilities/</p> <p>https://www.deic.dk/en/supercomputing/national-hpc-facilities</p>"},{"location":"HPC_Facilities/docs/GrantApp/","title":"Applying for ressources in 6 Simple Steps","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/docs/GrantApp/#step-1-select-apply-for-resources-on-the-ucloud-frontpage","title":"Step 1: Select \"Apply for resources\" on the UCloud frontpage","text":""},{"location":"HPC_Facilities/docs/GrantApp/#step-2-select-apply-for-new-project-instead","title":"Step 2: Select \"Apply for new project instead\"","text":""},{"location":"HPC_Facilities/docs/GrantApp/#step-3-provide-a-title-and-choice-hpc-type-1-or-3","title":"Step 3: Provide a Title and choice HPC type (1 or 3)","text":""},{"location":"HPC_Facilities/docs/GrantApp/#step-4-choice-storage-amount-and-machine-typein-dkk","title":"Step 4: Choice Storage amount and Machine Type(in DKK).","text":""},{"location":"HPC_Facilities/docs/GrantApp/#only-deic-interactive-hpc-sdu-u1-standard-cpu-deic-interactive-hpc-aau-uc-t4-gpu-are-relevant","title":"Only \"DeiC Interactive HPC (SDU) / u1-standard\" (CPU) &amp; \"DeiC Interactive HPC (AAU) / uc-t4\" (GPU) are relevant.","text":""},{"location":"HPC_Facilities/docs/GrantApp/#step-5-provide-a-meaningfull-project-description-select-sas-or-stata-license-if-needed","title":"Step 5: Provide a meaningfull project description &amp; select SAS or STATA License (if needed).","text":""},{"location":"HPC_Facilities/docs/GrantApp/#step-6-press-submit-application","title":"Step 6: Press \"Submit application\"","text":""},{"location":"HPC_Facilities/docs/GrantApp/#now-the-application-will-be-evaluated-by-the-cbs-front-office-at-first-given-opportunity-the-application-will-either-be-accepted-otherwise-you-will-be-contacted-cbs-mail","title":"Now the application will be evaluated by the CBS front office at first given opportunity. The application will either be accepted otherwise you will be contacted (CBS mail).","text":""},{"location":"HPC_Facilities/docs/Hippo/","title":"Type 3 \u2013 Large Memory HPC (Hippo)","text":"<p>This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area.  Type 3 is hosted and maintained at SDU. </p> <p>The following guide covers the follow:</p> <p>Facility Overview \u00a0\u00a0| \u00a0\u00a0 Get started with Large Memory HPC (Hippo) \u00a0\u00a0| \u00a0\u00a0 User Support \u00a0\u00a0| \u00a0\u00a0 Apply for Funds</p>"},{"location":"HPC_Facilities/docs/Hippo/#facility-overview","title":"Facility Overview","text":"<p>The DeiC Large Memory HPC system is a system consisting of large memory nodes (between 1 and 4 TB RAM per node) configured as a traditional Slurm cluster. See here for more information</p>"},{"location":"HPC_Facilities/docs/Hippo/#get-started-with-large-memory-hpc-hippo","title":"Get started with Large Memory HPC (Hippo)","text":"<p>The user guide can be found at this link.</p>"},{"location":"HPC_Facilities/docs/Hippo/#user-support","title":"User Support","text":"<p>All UCloud support should go through the CBS front office(rdm@cbs.dk). If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/docs/Hippo/#apply-for-funds","title":"Apply for Funds","text":""},{"location":"HPC_Facilities/docs/Hippo/#students","title":"Students","text":"<p>In order for CBS students to gain  can only have direct access to Type 3 ress the initial 1000kr credit and 50 GB storage. When you need more/other resources, it must go through your thesis supervisor who needs to apply for funds and invite you to join the UCloud project. </p> <p>If this is not possible then you welcome to contact RDM Support to discuss further.</p> <p>The ressources will be provided within a UCloud project and not to a user \"My Workspace\".</p>"},{"location":"HPC_Facilities/docs/Hippo/#researchers-staff","title":"Researchers &amp; Staff","text":"<p>Further funds can be obtianed in two ways: </p> <ol> <li> <p>Apply to the local CBS ressource pool. You apply from UCloud by sending a UCloud grant application. Information on machine type selection be found here. Otherwise please contact RDM Support.</p> </li> <li> <p>Apply for the bi-annual application round for the national HPC resources. Please contact RDM Support as soon as possible if you consider applying.</p> </li> </ol> <p>For both ways the ressources will be provided to a UCloud project and not to a user \"My Workspace\". Each UCloud project will be given a reference number (DeiC-XX-Y NUMBER).</p> <p>This number should be used to acknowledge the use of national HPC in publications.</p>"},{"location":"HPC_Facilities/docs/License/","title":"Add License to Matlab, STATA and SAS application","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/docs/License/#add-local-license","title":"Add Local License","text":""},{"location":"HPC_Facilities/docs/License/#step-1-upload-local-stata-lic-file-or-sas-txt-file-license-to-ucloud","title":"Step 1: Upload local STATA (.lic file) or SAS (.txt file) license to UCloud","text":""},{"location":"HPC_Facilities/docs/License/#step-2-select-the-license-file-lic-or-txt-while-setting-up-ucloud-job","title":"Step 2: Select the license file (.lic or .txt) while setting up UCloud Job","text":""},{"location":"HPC_Facilities/docs/License/#add-server-license","title":"Add Server License","text":""},{"location":"HPC_Facilities/docs/License/#step-1-apply-for-server-license-through-ucloud-grant-application","title":"Step 1: Apply for Server License through UCloud Grant Application","text":""},{"location":"HPC_Facilities/docs/License/#step-2-activate-license-sas-94-license-shown-as-example","title":"Step 2: Activate License (SAS 9.4 license shown as example)","text":""},{"location":"HPC_Facilities/docs/License/#step-3-select-server-license-while-setting-up-ucloud-job","title":"Step 3: Select server license while setting up UCloud Job","text":""},{"location":"HPC_Facilities/docs/MachineType/","title":"Type 1 - DeiC Interactive HPC","text":"<p>Find more information here.</p>"},{"location":"HPC_Facilities/docs/MachineType/#deic-interactive-hpc-sdu-u1-standard-cpu","title":"DeiC Interactive HPC (SDU) / u1-standard (CPU)","text":"<p>SDU provides CPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as you would on your laptop. See all apps. </p> <p>The following machines are available:</p> <p></p>"},{"location":"HPC_Facilities/docs/MachineType/#u1-standard-64-specs","title":"u1-standard-64 Specs","text":"<p>Dell PowerEdge C6420</p> <p>vCPU:   64 (32 virtual cores) 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz</p> <p>RAM: 384 GB  DDR 4-2666</p> <p>Price: 5,49 DKK/hour</p> <p>Description: The full node consists of 2x Intel(R) Xeon(R) Gold 6130 CPU@2.10 GHz, 32 virtual cores/CPU, and 384 GB of memory.</p>"},{"location":"HPC_Facilities/docs/MachineType/#deic-interactive-hpc-aau-uc-t4-gpu","title":"DeiC Interactive HPC (AAU) / uc-t4 (GPU)","text":"<p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab.</p> <p>The following machines are available:</p> <p></p>"},{"location":"HPC_Facilities/docs/MachineType/#uc-t4-4-specs","title":"uc-t4-4 Specs","text":"<p>vCPU:   40 cores</p> <p>RAM: 160 GB</p> <p>GPU:    4</p> <p>Price:  33,99 DKK/hour</p> <p>Description:    Virtual machine with four NVIDIA T4 GPUs deployed on the AAU OpenStack system.</p>"},{"location":"HPC_Facilities/docs/NGC/","title":"Nationalt Genom Center HPC (Danish Statistics Data)","text":"<p>It is possible to access and process data from Danish Statistics through the Nationalt Genom Center HPC. This service is not funded by CBS. See pricing list.</p> <p>A techinal guide is found here.</p> <p>The linked information is only avaliable in danish.</p> <p>For further information or support please contact RDM Support.</p>"},{"location":"HPC_Facilities/docs/UCloud/","title":"UCloud","text":""},{"location":"HPC_Facilities/docs/UCloud/#type-1-interactive-hpc-ucloud","title":"Type 1 \u2013 Interactive HPC (UCloud)","text":"<p>The easiest-to-use HPC service is DeiC Interactive HPC (Type 1) also known as UCloud. This service is provided by the Danish universities SDU and AAU.</p>"},{"location":"HPC_Facilities/docs/UCloud/#facility-overview","title":"Facility Overview","text":"<p>SDU provides CPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as they would on your laptop. See all apps. </p> <p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab. More information can be found here.</p>"},{"location":"HPC_Facilities/docs/UCloud/#login-onto-ucloud","title":"Login onto UCloud","text":"<p>You can login on to UCloud using WAYF (Where Are You From). Press here to login.</p> <ul> <li>Select Copenhagen Business School as your affiliate institution on the login page. </li> <li>Sign in using your CBS mail account</li> </ul> <p>Upon the first login it is necessary to approve the SDU eScience terms of service. Afterwards, the user is redirected to the UCloud user interface.</p> <p>Note: After login the user can activate two factor authentication by clicking on the avatar icon in the top-right corner of the home screen.</p>"},{"location":"HPC_Facilities/docs/UCloud/#get-started-with-ucloud","title":"Get started with UCloud","text":"<p>All new users in UCloud are awarded a \"My Workspace\" with 1000 DKK of computing (CPU only) resources to the \"DeiC Interactive HPC (SDU)\", as well as 50 GB remote storage. You can use these resources to get acquainted with the system, run test jobs, etc. </p> <p>\"DeiC Interactive HPC (SDU)\" provides broadest ranges of containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI). See all apps.</p> <p>The largest machine (64 cores &amp; 384 GB memory) cost 5.49kr/Hour. So the free 1000 DKK will give you access to approx. 182 hours of inital run time.</p> <p>Start by watching the following UCloud tutorials:</p> <ul> <li>Manage Files and Folders</li> <li>Manage Applications</li> <li>Manage Workspaces</li> <li>Use Cases</li> <li>Webinars</li> <li>UCloud Documentation</li> </ul> <p>More Tutorials and Documentation can be found here</p>"},{"location":"HPC_Facilities/docs/UCloud/#user-support","title":"User Support","text":"<p>All UCloud support should go through the RDM Support. If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/docs/UCloud/#apply-for-funds","title":"Apply for Funds","text":""},{"location":"HPC_Facilities/docs/UCloud/#students","title":"Students","text":"<p>CBS student can only have direct access to the initial 1000kr credit and 50 GB storage. When you need more/other resources, it must go through your thesis supervisor who needs to apply for funds and invite you to join the UCloud project. </p> <p>If this is not possible then you welcome to contact RDM Support to discuss further.</p> <p>The ressources will be provided within a UCloud project and not to a user \"My Workspace\".</p>"},{"location":"HPC_Facilities/docs/UCloud/#researchers-staff","title":"Researchers &amp; Staff","text":"<p>Further funds can be obtianed in two ways: </p> <ol> <li> <p>Apply to the local CBS ressource pool. You apply from UCloud by sending a UCloud grant application. Information on machine type selection be found here. Otherwise please contact RDM Support.</p> </li> <li> <p>Apply for the bi-annual application round for the national HPC resources. Please contact RDM Support as soon as possible if you consider applying.</p> </li> </ol> <p>For both ways the ressources will be provided to a UCloud project and not to a user \"My Workspace\". Each UCloud project will be given a reference number (DeiC-XX-Y NUMBER).</p> <p>This number should be used to acknowledge the use of national HPC in publications.</p>"},{"location":"HPC_Facilities/docs/UCloud/#collaboration-on-ucloud","title":"Collaboration on UCloud","text":""},{"location":"HPC_Facilities/docs/UCloud/#international-collaborators","title":"International Collaborators","text":"<p>International researchers need a \"visiting researcher premission\"(g\u00e6steforskeradgang) to CBS to gain access to UCloud. One can be obtained by contacting CBS HR(hr@cbs.dk).</p> <p>Once this is in place CBS HPC support will contact the UCloud Research Support Team and provide the below shown information. </p> <ul> <li> <p>Full name:</p> </li> <li> <p>Occupation:</p> </li> <li> <p>Organisation (University):</p> </li> <li> <p>Email (University):</p> </li> </ul> <p>Subsequently, the UCloud Research Support Team will contact the researcher to verify their identity through a video meeting. A valid ID is needed. </p>"},{"location":"HPC_Facilities/docs/UCloud/#license-software","title":"License Software","text":"<p>There are several types of licensed software that can be run on UCloud. </p>"},{"location":"HPC_Facilities/docs/UCloud/#matlab","title":"MATLAB","text":"<p>A Matlab server license in order to run the application on UCloud. Once can be acquired through CBS IT help desk at own expense.</p> <ul> <li> <p>Matlab UCloud Application</p> </li> <li> <p>UCloud Matlab Documentation</p> </li> <li> <p>UCloud video tutorial - Matlab walkthrough starts at 16:00 minutes into the video. Shows how activate Matlab with a personal license.</p> </li> </ul>"},{"location":"HPC_Facilities/docs/UCloud/#stata","title":"STATA","text":"<p>Users can either upload their own personal STATA license (.lic file) to UCloud or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>STATA UCloud Application</p> </li> <li> <p>UCloud STATA Documentation</p> </li> </ul>"},{"location":"HPC_Facilities/docs/UCloud/#sas-sas-studio","title":"SAS &amp; SAS Studio","text":"<p>Users can either upload their own personal SAS license (.txt file) or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>SAS UCloud Application</p> </li> <li> <p>UCloud SAS Documentation</p> </li> </ul>"},{"location":"HPC_Facilities/docs/WRDS/","title":"WRDS - Wharton Research Data Services","text":"<p>WRDS- Wharton Research Data Services provides access to financial data, accounting figures as well as banking and management information. </p> <p>CBS students and staff must register to get access. </p> <p>More information can be found here.</p>"},{"location":"HPC_Facilities/docs/WRDS/#accessing-wrds-databases-from-local-pc-ucloud","title":"Accessing WRDS databases from Local PC &amp; UCloud","text":""},{"location":"HPC_Facilities/docs/WRDS/#wrds-documentation","title":"WRDS Documentation","text":"<ul> <li>Python</li> <li>R</li> <li>Matlab</li> <li>SAS</li> <li>STATA</li> </ul>"},{"location":"HPC_Facilities/docs/WRDS/#ucloud-templatesscripts","title":"UCloud Templates/Scripts","text":"<p>Repositiory </p>"},{"location":"HPC_Facilities/docs/WRDS/#wrds-cloud","title":"WRDS Cloud","text":"<p>WRDS Cloud is a HPC service with the possibility to process the data avaliable on WRDS. </p> <p>WRDS Cloud is only acessable for CBS staff and researchers.</p>"},{"location":"HPC_Facilities/docs/WRDS/#available-software","title":"Available Software","text":"<p>The WRDS Cloud provides the following software for your research needs: - SAS 9.4 - R 3.5 - Python 3.6 and 2.7 - Stata 15 (requires special subscription agreement with StataCorp)</p> <p>In addition, it further supports remote access from:</p> <ul> <li>MATLAB 2016a +</li> <li>Native PostgreSQL clients</li> <li>ODBC- or JDBC-compliant clients</li> </ul>"},{"location":"HPC_Facilities/docs/WRDS/#wrds-cloud-documentation","title":"WRDS Cloud Documentation","text":"<ul> <li>Introduction to the WRDS Cloud</li> <li>Using SSH to Connect to the WRDS Cloud </li> </ul>"},{"location":"HPC_Facilities/docs/WRDS/#for-further-information-or-support-please-contact-us-at","title":"For further information or support please contact us at:","text":"<ul> <li>Research &amp; Data Management @ CBS (rdm@cbs.dk)</li> <li>Kristoffer Gulmark Poulsen (kgp.lib@cbs.dk)</li> <li>Lars Nondal (ln.lib@cbs.dk)</li> </ul>"},{"location":"HPC_Facilities/docs/status/","title":"HPC Operational Status","text":"<ul> <li>Type 1 (UCloud)</li> <li>Type 3 (Hippo)</li> <li>Type 5 (LUMI)</li> </ul>"},{"location":"Tutorials/","title":"Index","text":""},{"location":"Tutorials/#tutorials-documentation","title":"Tutorials &amp; Documentation","text":""},{"location":"Tutorials/#cbs-tutorials","title":"CBS Tutorials","text":"<ul> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Virtual Machines on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul>"},{"location":"Tutorials/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":""},{"location":"Tutorials/#sdu","title":"SDU","text":"<ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul>"},{"location":"Tutorials/#aau","title":"AAU","text":"<ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"Tutorials/#type-2","title":"TYPE 2","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"Tutorials/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"Tutorials/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"Tutorials/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"Tutorials/Overview/","title":"Overview","text":""},{"location":"Tutorials/Overview/#tutorials-documentation","title":"Tutorials &amp; Documentation","text":""},{"location":"Tutorials/Overview/#cbs-tutorials","title":"CBS Tutorials","text":"<ul> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Virtual Machines on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul>"},{"location":"Tutorials/Overview/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":""},{"location":"Tutorials/Overview/#sdu","title":"SDU","text":"<ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul>"},{"location":"Tutorials/Overview/#aau","title":"AAU","text":"<ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"Tutorials/Overview/#type-2","title":"TYPE 2","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"Tutorials/Overview/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"Tutorials/Overview/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/Overview/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/Overview/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"Tutorials/Overview/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"Tutorials/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <p>R</p> <p>Python</p> <p>Further documentation can be found on UCloud:</p> <p>Conda on UCloud </p>"},{"location":"Tutorials/Conda/Conda_Python/","title":"UCloud Tutorial: Using Conda for easy management of Python environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_Python/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code>\n# Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_Python/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#installing-and-activate-python-environments","title":"Installing and activate Python environments","text":"<p>https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html </p> <pre><code># Showing available python versions\nconda search python\n\n# Installing a Python environment (Python 3.9 in this example) \nconda create -n myenv python=3.9\n\n# Or install packages during installation.\nconda create -n myenv python=3.9 numpy=1.16\n\n# Shows already installed environments (R-4.2.3 show be displayed)\nconda env list\n\n# Activate environment\nconda activate myenv\n\n#Check which Python is in path\nwhich python\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#vscode-on-ucloud","title":"VScode on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-coder-python-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Coder python UCloud job.","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-coder.html?highlight=coder</p> <p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-vscode-interface-and-open-file-and-activate-myenv-as-python-interpreter","title":"Now you can launch VSCode interface and open file and activate \u201cmyenv\u201d as python interpreter:","text":"<p>Select the menu View -&gt; Command Palette:</p> <p></p> <p>Execute the command &gt; Python: Select Intepreter:</p> <p></p>"},{"location":"Tutorials/Conda/Conda_Python/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info --envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install ipykernel:\n\nconda install ipykernel\n\n# \npython -m ipykernel install --user --name myenv --display-name \"myenv\"\n\n# De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_Python/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python_1","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code>\n# Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#installing-r-environment-using-conda","title":"Installing R environment using Conda","text":""},{"location":"Tutorials/Conda/Conda_R/#installing-mamba-add-in","title":"Installing Mamba add-in","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p> <pre><code># Installing mamba add-in:\nconda install -n base -c conda-forge mamba\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#installing-and-activate-r-environment-with-mamba","title":"Installing and activate R environment with mamba","text":"<p>https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda</p> <pre><code>#Showing available R versions\nmamba search -c conda-forge r-base\n\n#Installing a R environment (R-4.2.3 in this example) \nmamba create -n myenv -y -c conda-forge r-base=4.2.3\n\n#Or install packages during installation.\nmamba create -n myenv -y -c conda-forge r-base=4.2.3 r-tidyverse\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Conda_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n</code></pre>"},{"location":"Tutorials/Conda/Overview/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <p>R</p> <p>Python</p> <p>Further documentation can be found on UCloud:</p> <p>Conda on UCloud </p>"},{"location":"Tutorials/Sync/Rsync/","title":"UCloud Tutorial: Transfer large data to UCloud using Rsync","text":"<p>This application is used to deploy an rsync server, which is a utility to efficiently transfer and synchronize files and directories between two different systems</p> <p>UCloud documentation on Rsync</p>"},{"location":"Tutorials/Sync/Rsync/#installing-ubuntu-on-local-machine-for-windows","title":"Installing Ubuntu on local machine (For Windows)","text":"<p>Video Tutorial found here</p>"},{"location":"Tutorials/Sync/Rsync/#add-public-ssh-key-to-ucloud","title":"Add public SSH key to UCloud","text":"<p>Information on how to generate a SSH key can be found here</p> <p>Copy the public SSH key (.pub file) to UCloud.</p> <p></p>"},{"location":"Tutorials/Sync/Rsync/#apply-for-a-public-ip-on-ucloud","title":"Apply for a Public IP on UCloud","text":"<p>A UCloud public IP is need to use Rsync. This can be obtained through a UCloud grant application by filling out the field shown below:</p> <p></p> <p>The application subsequently needs to be accepted by CBS front office personal.</p>"},{"location":"Tutorials/Sync/Rsync/#start-rsync-job-and-configure-ports-for-public-ip","title":"Start Rsync Job and Configure Ports for Public IP'","text":"<p>Fill out \"Job name\",\"Hours\", \"Machine type\".</p>"},{"location":"Tutorials/Sync/Rsync/#step-1-select-the-public-ip-ssh-public-key-rsync-volume-fields","title":"Step 1: Select the \"Public IP\", \"SSH Public Key\" &amp; \"Rsync Volume\" fields","text":"<ul> <li>\"Public IP\" - see steps below.</li> <li>\"SHH Public Key\" - Select the .pub file you have just uploaded to UCloud.</li> <li>\"Rsync Volume\" - Select the UCloud folder where the files/folders should be transferred to.</li> </ul>"},{"location":"Tutorials/Sync/Rsync/#step-2-if-no-public-ips-are-activated-press-create-public-ip","title":"Step 2: If no Public IPs are activated press \"Create public ip\"","text":""},{"location":"Tutorials/Sync/Rsync/#step-3-select-the-public-ip-product","title":"Step 3: Select the \"public-ip\" product","text":""},{"location":"Tutorials/Sync/Rsync/#step-4-press-the-3-dots-button-and-select-properties","title":"Step 4: press the \"3 dots button\" and select \"Properties\".","text":""},{"location":"Tutorials/Sync/Rsync/#step-5-open-the-firewall-for-ports-22-and-873-as-shown-below","title":"Step 5: Open the firewall for ports 22 and 873 as shown below:","text":""},{"location":"Tutorials/Sync/Rsync/#step-6-select-the-configured-public-ip","title":"Step 6: Select the configured Public IP","text":""},{"location":"Tutorials/Sync/Rsync/#step-7-start-the-job","title":"Step 7: Start the Job","text":""},{"location":"Tutorials/Sync/Rsync/#step-8-when-the-job-is-ready-the-ip-adress-and-the-password-is-shown-as-below","title":"Step 8: When the Job is ready the IP adress and the password is shown as below:","text":""},{"location":"Tutorials/Sync/Rsync/#open-terminal-on-local-machine-for-windows","title":"Open Terminal on local machine (For Windows)","text":"<pre><code># Activate Ubuntu \nwsl\n\n# Navigate to path contain the folder of files to transfer - Alternatively you can open terminal directly in the right directory to skip step below.\ncd \"path/of/folders-or-files\"\n\n\n# NO ENCRYPTION (NOT RECOMMENDED): Transfer the folder \"myfolder\" to work/myfolder on UCloud. No Public key should have been uploaded during Rsync Job configuration.\n\nrsync -avP ./myfolder rsync://ucloud@13x.2x5.1x4.13x:873/volume  # Exchange to the rigth IP adress\n\n\n# SSH ENCRYPTION: Transfer the folder \"myfolder\" to work/myfolder on UCloud. (USing SSH - encrypted)\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p 22\"  ./myfolder/ ucloud@13x.2x5.1x4.13x:/work/myfolder2 \n\n# Change the following to the above line:\n# \"~/.ssh/id_rsa\"  : the path to private SSH key as specified through the Ubuntu terminal. The best way to ensure this is to open a Ubuntu terminal in the \".ssh\" folder\n# \"./myfolder/\"    : the folder folder/files you want to copy over.\n# \"13x.2x5.1x4.13x\": The IP adress\n# \"/work/myfolder\": the path to the Rsync volume.. in this case the folder \"myfolder\" was chosen.\n\n# Password will be prompted. Copy the password over from the Job \"info page\". The password will be hidden in the terminal. \n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#open-ucloud-job-terminal-see-button-in-figure-above-and-double-check-that-the-data-transfer-was-successful","title":"Open UCloud job terminal (see button in figure above) and double check that the data transfer was successful.","text":""},{"location":"Tutorials/VMs/","title":"Virtual Machines on UCloud","text":"<p>How to Generate SSH key</p> <p>Acessing VM using SSH</p>"},{"location":"Tutorials/VMs/Overview/","title":"Virtual Machines on UCloud","text":"<p>How to Generate SSH key</p> <p>Acessing VM using SSH</p>"},{"location":"Tutorials/VMs/connectVM/","title":"SSH to Server through local Terminal","text":"<p>Add public SSH key while starting a VM job</p> <p></p> <p>Identify VM IP when UCloud job is ready.</p> <p></p>"},{"location":"Tutorials/VMs/connectVM/#from-local-terminal-connect-to-vm-by","title":"From Local Terminal connect to VM by:","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#transfer-files-and-folders-ssh-copy","title":"Transfer Files and Folders (SSH-Copy)","text":""},{"location":"Tutorials/VMs/connectVM/#to-vm","title":"To VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path-to-folder-or-files\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#from-vm","title":"From VM","text":"<p>Open a second terminal (1st terminal is connected to the VM)</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/shh/","title":"How to Generate a SSH key","text":"<p>In order to access the VM it is necessary to create Secure Shell Protocol (SSH) keys more specifically a public key (shareable part) and a private key (kept safe locally).</p>"},{"location":"Tutorials/VMs/shh/#on-windows-linux-systems","title":"On Windows &amp; Linux Systems","text":"<p>To generate your public key, find and open terminal and type: </p> <pre><code># For linux only\nsudo apt install openssh-client\n\n\n# For both Windows &amp; Linux\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in C:\\Users\\user/.ssh/id_rsa.\nYour public key has been saved in C:\\Users\\user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n\n</code></pre>"},{"location":"Tutorials/VMs/shh/#manually-locate-open-and-copy-our-public-key-from-id_rsapub-file","title":"Manually locate, open and copy our public key from id_rsa.pub file.","text":""},{"location":"Tutorials/VMs/shh/#on-windows","title":"On Windows","text":"<p>the file might be here: \"C:\\Users\\write_your_user_name.ssh\"</p>"},{"location":"Tutorials/VMs/shh/#on-linux","title":"On Linux","text":"<p>The generated SSH key will be by stored under ~/.ssh/id_rsa.pub by default.</p> <p>More information can be found at https://genome.au.dk/docs/getting-started/#public-key-authentication </p>"},{"location":"UCloud_BatchMode/","title":"UCloud Tutorial: Batch Processing on UCloud","text":"<p>Running non-interactive script in \"batch mode\" can help overcome the UCloud capacity issues as jobs can be started any time and then executed whenever the systems resources permits.  </p> <p>Many applications already have a \"Batch Processing\" UCloud functionality:</p> <ul> <li>Batch Processing</li> <li>RStudio</li> <li>JupyterLab</li> <li>Matlab</li> <li>PyTorch</li> <li>Tensorflow</li> <li>Spark Cluster</li> <li>Terminal/SLURM Cluster</li> </ul> <p>Application that currently does not have this UCloud functionality (e.g. Stata) can instead run batch mode computations using the terminal app. See the following tutorials:</p> <ul> <li>Stata</li> </ul>"},{"location":"UCloud_BatchMode/Overview/","title":"UCloud Tutorial: Batch Processing on UCloud","text":"<p>Running non-interactive script in \"batch mode\" can help overcome the UCloud capacity issues as jobs can be started any time and then executed whenever the systems resources permits.  </p> <p>Many applications already have a \"Batch Processing\" UCloud functionality:</p> <ul> <li>Batch Processing</li> <li>RStudio</li> <li>JupyterLab</li> <li>Matlab</li> <li>PyTorch</li> <li>Tensorflow</li> <li>Spark Cluster</li> <li>Terminal/SLURM Cluster</li> </ul> <p>Application that currently does not have this UCloud functionality (e.g. Stata) can instead run batch mode computations using the terminal app. See the following tutorials:</p> <ul> <li>Stata</li> </ul>"},{"location":"UCloud_BatchMode/Stata/","title":"UCloud Tutorial: Run Stata in Batch Mode on UCloud","text":"<p>This is an approach to adress the UCloud capacity issues. </p> <p>UCloud batch processing apps are scheduled to run as resources permit without end user interaction. It allows </p>"},{"location":"UCloud_BatchMode/Stata/#get-stata-license-and-installation-file-cbs-users","title":"Get Stata license and Installation file (CBS Users)","text":"<p>Follow the instructions to get a Stata license at CBS https://studentcbs.sharepoint.com/sites/ITandCampus/SitePages/en/Free-software.aspx</p> <p>You will recieve an email with license and installation information (see image below).</p> <p></p> <p>Download the installation file (Stata17Linux64.tar) and upload this to your UCloud directory.</p> <p></p>"},{"location":"UCloud_BatchMode/Stata/#installing-stata-on-ucloud","title":"Installing Stata on UCloud","text":""},{"location":"UCloud_BatchMode/Stata/#launch-a-terminal-app-ucloud-job-and-include-the-stata-installation-file-stata17linux64tar","title":"Launch a \"Terminal App\" UCloud Job and include the stata installation file (Stata17Linux64.tar)","text":"<p>Run following commands in the terminal: </p> <pre><code>\n# Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Unzip installation file to temp folder\nsudo -s\nmkdir /tmp/statafiles\ncd /tmp/statafiles\ntar -zxf /work/install/Stata17Linux64.tar.gz\n\n# Install Stata on in \"/work/stata17\". Say yes when asked during installtion\nmkdir /work/stata17 \ncd /work/stata17 \n/tmp/statafiles/install\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Initialize Stata\nsudo /work/stata17/stinit\n\n# Follow instructions and add \"Serial number\", \"Code\" and \"Authorization\" from the Stata license mail\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata \n# or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"UCloud_BatchMode/Stata/#end-job-and-copy-the-stata17-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-directory","title":"End job and copy the \u201cstata17\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud directory.","text":""},{"location":"UCloud_BatchMode/Stata/#activate-stata-installation-in-a-new-terminal-job","title":"Activate Stata installation in a new terminal job","text":"<p>Add the stata17 folder to the job</p> <p></p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata \n# or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"UCloud_BatchMode/Stata/#run-stata-scrip-in-batch-mode-n-a-new-terminal-job","title":"Run Stata scrip in batch mode n a new terminal job","text":"<p>Add the \"stata17\" and other relevant folder to the job:</p> <p></p> <p>Add a bash script(.sh) under \"Batch processing\" as one of the \"Optional Parameters\":</p> <p> </p> <p>Below shown bash script can be downloaded from here. Use this as a template or create your own bash script.</p> <p>More information on how to run Stata in batch mode can be found here: https://www.stata.com/support/faqs/unix/batch-mode/</p> <pre><code>#!/bin/bash\n\n\n# Installing dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata17 on UNIX path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Run stata in Batch mode\nstata -b do filename &amp; # USER SHOULD CHANGE THIS LINE (SEE LINK Above)\n\n</code></pre>"},{"location":"UCloud_SlurmCluster/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"UCloud_SlurmCluster/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"UCloud_SlurmCluster/Overview/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"UCloud_SlurmCluster/Overview/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/Overview/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/Overview/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/Overview/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/Overview/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"UCloud_SlurmCluster/Ray/","title":"Ray","text":""},{"location":"UCloud_SlurmCluster/Ray/#example-using-ray","title":"Example using Ray","text":"<p>In terminal run:</p> <pre><code>python slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Output\n\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n\n\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#open-extra-terminal-for-three-nodes","title":"Open extra Terminal for three Nodes","text":""},{"location":"UCloud_SlurmCluster/Ray/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/Ray/#observed-that-the-work-is-disbrubted-across-all-three-nodes","title":"Observed that the work is disbrubted across all three nodes.","text":"<p>This may look different for different backends (e.g. Dask). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"UCloud_SlurmCluster/Ray/#output-files","title":"Output files","text":""},{"location":"UCloud_SlurmCluster/Ray/#the-autogenerated-slurm-script-slurmtest_0425-1208sh","title":"The autogenerated SLURM script (SlurmTest_0425-1208.sh)","text":"<pre><code>#!/bin/bash\n# shellcheck disable=SC2206\n# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!\n# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!\n\n#SBATCH --job-name=SlurmTest_0425-1208\n#SBATCH --output=SlurmTest_0425-1208.log\n\n### This script works for any number of nodes, Ray will find and manage all resources\n#SBATCH --nodes=3\n#SBATCH --exclusive\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n##SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE} #De-activated by KGP 230317\n\n# Load modules or your own conda environment here\n# module load pytorch/v1.4.0-gpu\n# conda activate ${CONDA_ENV}\n\n\n# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====\n\necho $SLURM_JOB_NODELIST\n\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\") # Getting the node names\n\nnodes_array=($nodes)\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w \"$node_1\" hostname --ip-address) # making redis-address\n\n# if we detect a space character in the head node IP, we'll\n# convert it to an ipv4 address. This step is optional.\nif [[ \"$ip\" == *\" \"* ]]; then\n  IFS=' ' read -ra ADDR &lt;&lt;&lt; \"$ip\"\n  if [[ ${#ADDR[0]} -gt 16 ]]; then\n    ip=${ADDR[1]}\n  else\n    ip=${ADDR[0]}\n  fi\n  echo \"IPV6 address detected. We split the IPV4 address as $ip\"\nfi\n\nport=6379\nip_head=$ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"STARTING HEAD at $node_1\"\nsrun --nodes=1 --ntasks=1 -w \"$node_1\" ray start --head --node-ip-address=\"$ip\" --port=$port --block &amp;\nsleep 30\n\n#worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\n#export NB_WORKERS=$((${SLURM_JOB_NUM_NODES-1})) #number of nodes other than the head node\n#echo ${NB_WORKERS}\n\nexport NB_WORKERS=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\necho \"STARTING ${NB_WORKERS} WORKERS\"\nfor ((i = 1; i &lt;= NB_WORKERS; i++)); do\n  node_i=${nodes_array[$i]}\n  echo \"STARTING WORKER $i at $node_i\"\n  srun --nodes=1 --ntasks=1 -w \"$node_i\" ray start --address \"$ip_head\" --block &amp;\n  sleep 5\ndone\n\n# ===== Call your code below =====\necho \"RUNNING CODE: python /work/data/SklearnRay.py\"\npython /work/data/SklearnRay.py\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#autogenerated-log-file-slurmtest_0425-1208log","title":"Autogenerated log file (SlurmTest_0425-1208.log)","text":"<pre><code>node[0-2]\nIPV6 address detected. We split the IPV4 address as 10.42.47.86\nIP Head: 10.42.47.86:6379\nSTARTING HEAD at node0\n2023-04-25 12:08:40,054 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\nSTARTING 2 WORKERS\nSTARTING WORKER 1 at node1\n2023-04-25 12:08:38,026 INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-04-25 12:08:38,026 INFO scripts.py:710 -- Local node IP: 10.42.47.86\n2023-04-25 12:08:41,222 SUCC scripts.py:747 -- --------------------\n2023-04-25 12:08:41,222 SUCC scripts.py:748 -- Ray runtime started.\n2023-04-25 12:08:41,223 SUCC scripts.py:749 -- --------------------\n2023-04-25 12:08:41,223 INFO scripts.py:751 -- Next steps\n2023-04-25 12:08:41,223 INFO scripts.py:752 -- To connect to this Ray runtime from another node, run\n2023-04-25 12:08:41,223 INFO scripts.py:755 --   ray start --address='10.42.47.86:6379'\n2023-04-25 12:08:41,223 INFO scripts.py:771 -- Alternatively, use the following Python code:\n2023-04-25 12:08:41,223 INFO scripts.py:773 -- import ray\n2023-04-25 12:08:41,223 INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='10.42.47.86')\n2023-04-25 12:08:41,223 INFO scripts.py:790 -- To see the status of the cluster, use\n2023-04-25 12:08:41,223 INFO scripts.py:791 --   ray status\n2023-04-25 12:08:41,223 INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.\n2023-04-25 12:08:41,224 INFO scripts.py:809 -- To terminate the Ray runtime, run\n2023-04-25 12:08:41,224 INFO scripts.py:810 --   ray stop\n2023-04-25 12:08:41,224 INFO scripts.py:891 -- --block\n2023-04-25 12:08:41,224 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:41,224 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nSTARTING WORKER 2 at node2\n2023-04-25 12:08:48,882 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:48,933 I 2244 2244] global_state_accessor.cc:356: This node has an IP address of 10.42.28.36, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:48,859 INFO scripts.py:866 -- Local node IP: 10.42.28.36\n2023-04-25 12:08:48,935 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:48,935 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:48,935 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:48,935 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:48,935 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:48,935 INFO scripts.py:891 -- --block\n2023-04-25 12:08:48,935 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:48,935 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nRUNNING CODE: python /work/data/SklearnRay.py\n2023-04-25 12:08:54,215 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:54,271 I 956 956] global_state_accessor.cc:356: This node has an IP address of 10.42.34.213, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:54,135 INFO scripts.py:866 -- Local node IP: 10.42.34.213\n2023-04-25 12:08:54,274 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:54,275 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:54,275 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:54,275 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:54,275 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:54,275 INFO scripts.py:891 -- --block\n2023-04-25 12:08:54,275 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:54,275 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n2023-04-25 12:09:24,758 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.42.47.86:6379...\n2023-04-25 12:09:24,775 INFO worker.py:1553 -- Connected to Ray cluster.\n2023-04-25 12:09:25,073 WARNING pool.py:604 -- The 'context' argument is not supported using ray. Please refer to the documentation for how to control ray initialization.\nFitting 10 folds for each of 500 candidates, totalling 5000 fits\n209.00055767036974\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nslurmstepd-node2: error: *** STEP 2.3 ON node2 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node0: error: *** STEP 2.1 ON node0 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node1: error: *** STEP 2.2 ON node1 CANCELLED AT 2023-04-25T12:12:54 ***\nsrun: error: node2: task 0: Exited with exit code 1\nsrun: error: node1: task 0: Exited with exit code 1\nsrun: error: node0: task 0: Exited with exit code 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/","title":"UCloud Tutorial: Run Multi-node SLURM Cluster on UCloud","text":""},{"location":"UCloud_SlurmCluster/SLURM/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>In addition to the normal setting fill out the following options (See figure below).</p> <p>In this example launched as cluster consisting of 3 nodes with three folder added to the launch:</p> <ul> <li>\"miniconda3\"  - contains the conda environment I want to deploy across the different nodes.</li> <li>\"SLURM_deployment\" - contains the easy-to-use deployment scripts provided in this tutorial. </li> <li>\"SLURM_scripts\" - contains the user specific script and data to run on the cluster.</li> </ul> <p>In this example Conda is used for package and evironment management. Check here for more information on Conda on UCloud.</p> <p></p>"},{"location":"UCloud_SlurmCluster/SLURM/#when-the-job-has-started-open-terminal-for-node-1","title":"When the job has started open Terminal for Node 1","text":"<p>Run following commands in the terminal: </p> <pre><code>\n# activate SLURM Cluster if not activated in the step above\ninit_slurm_cluster\n\n# List Avaliable nodes\nsinfo -N -l\n\n</code></pre> <p>The controller node is always the first node. Called \"node0\" in within SLURM but called \"Node 1\" in the UCloud interface). All additional nodes are named sequentially. For example, a cluster consisting of three full u1-standard nodes is configured as follows:</p> <pre><code>NODELIST   NODES PARTITION     STATE CPUS   S:C:T MEMORY\nnode0         1     CLOUD*     idle   64   1:64:1 385024\nnode1         1     CLOUD*     idle   64   1:64:1 385024\nnode2         1     CLOUD*     idle   64   1:64:1 385024\n</code></pre> <p>But called Node 1, Node 2 and Node 3 in the UCloud interface.</p>"},{"location":"UCloud_SlurmCluster/SLURM/#acitvate-conda-environment","title":"Acitvate Conda Environment","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which environment is in path (e.g. X = python,R..)\nwhich X # (e.g. X = python,R..)\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/X # (e.g. X = python,R..)\n\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-deployment-scripts","title":"SLURM deployment scripts","text":"<p>The SLURM deployment script (\"slurm-launch.py\") have been adopted from  Ray documentation to support the addition of other python libraries (Dask, ipyparallel) and other languages (e.g. R).</p>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-launchpy","title":"slurm-launch.py","text":"<p>\"slurm-launch.py\" auto-generates SLURM scripts and launch. slurm-launch.py uses an underlying template (e.g. \"slurm-template_ray.sh\" or \"slurm-template_dask.sh\") and fills out placeholders given user input.</p> <pre><code>\n# Change path:\ncd /work/SLURM_deployment\n\n# Python with Ray\npython slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Python with Dask\npython slurm-launch.py --script slurm-template_dask.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnDask.py\" --num-nodes 3 --nprocs 8 --nthreads 1\n\n# R with doParallel\npython slurm-launch.py --script slurm-template_R.sh --exp-name SlurmTest --command \"Rscript --vanilla /work/SLURM_scripts/doParallel.r\" --num-nodes 3 --nprocs 8 --nthreads 1 \n\n\n# Example of Output\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#addditionel-options","title":"Addditionel options","text":"<pre><code>\n--exp-name          # The experiment name. Will generate {exp-name}_{date}-{time}.sh and {exp-name}_{date}-{time}.log.\n--command           # The command you wish to run. For example: rllib train XXX or python XXX.py.\n--node (-w)         # The specific nodes you wish to use, in the same form as the output of sinfo. Nodes are automatically assigned if not specified.\n--num-nodes (-n)    # The number of nodes you wish to use. Default: 1.\n--partition (-p):   # The partition you wish to use. Default: \u201c\u201d, will use user\u2019s default partition.\n--load-env:         # The command to setup your environment. For example: module load cuda/10.1. Default: \u201c\u201d.\n--nprocs: \n--nthreads:\n\n\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#open-extra-terminal-for-the-three-nodes","title":"Open extra terminal for the three nodes","text":""},{"location":"UCloud_SlurmCluster/SLURM/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/SLURM/#observed-that-the-work-is-distibuted-across-all-three-nodes","title":"Observed that the work is distibuted across all three nodes.","text":"<p>This may look different for different frameworks (e.g. Ray, Dask, R). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"}]}