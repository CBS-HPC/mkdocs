{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC &amp; Data Science Support at CBS","text":"<p>This is the GitHub repository for HPC &amp; Data Science Support at CBS. The team is dedicated to providing assistance and support to CBS researchers and students in their research utilizing the different HPC systems available at CBS. The repository contains various resources and information related to our services and activities.</p>"},{"location":"#activities","title":"Activities","text":"Tutorials &amp; User UtilitiesTeaching ActivitiesDevelopment of HPCDaily User SupportResearch Consultancy <p> The HPC &amp; Da Science Support team provides tutorials and user utilities to assist and support CBS researchers and students in their research utilizing the different HPC systems available at CBS. The tutorials and utilities covering various topics such as:</p> <ul> <li>Use cases for different HPC systems</li> <li>Efficient and secure data tranfer</li> <li>parallel computing</li> <li>environment management (e.g Conda)</li> </ul> <p>These tutorials offer step-by-step guidance, empowering users to effectively utilize HPC resources for their research. </p> <p> We conducts teaching activities through researcher and student webinars. Titles include \"High Performance Computing\", \"HPC &amp; Parallel Programming in R,\" and \"HPC &amp; Parallel Programming in Python\" with more in the pipeline.</p> <p>See \"Events\" section for more information.</p> <p>Upon receiving requests from course coordinators, we are also available to participate in teaching activities for courses at CBS.</p> <p> We are committed to the continuous development of HPC resources at CBS. This is both by ensuring that researchers have access to the right facilities, both short- and long-term, but also by providing a clear learning strategy for research to develop their HPC &amp; data science skillset.</p> <p> As Deic Front Office at CBS are we in charge off all communications with HPC system adminstrators (Back Office) and DeiC.</p> <p>Ideally, all user requests and troubleshooting should be send to the CBS Front Office(rdm@cbs.dk) as a Single Point of Contact (SPOC) where resulting tickets will be directed accordingly. </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p> <p> We provide consulting services to researchers and research projects, assisting them with their HPC requirements. Our support includes, but is not limited to, the following examples:</p> <ul> <li>HPC grant application guidance</li> <li>Assessing user needs for HPC resources</li> <li>Workflow and code optimization assistance</li> </ul> <p>By offering expert consultation, we help researchers identify and address their specific HPC needs, ensuring they can effectively utilize the available resources and optimize their workflows and code for maximum performance and efficiency.</p>"},{"location":"#database-tools","title":"Database Tools","text":"<ul> <li> <p>Moody's Datahub</p> </li> <li> <p>WRDS - Wharton Research Data Services</p> </li> </ul>"},{"location":"#hpcucloud-tutorials","title":"HPC/UCloud Tutorials","text":"GeneralRPythonSTATA / SAS / MatlabGPUsLarge Memory HPC <ul> <li>Getting Started with HPC (UCloud)</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>How to Generate SSH key</li> <li>SSH Connection to UCloud using VSCode</li> </ul> <ul> <li>Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library</li> <li>Use Conda on UCloud to manage R-packages</li> <li>SLURM Clusters on UCloud</li> </ul> <ul> <li>Use Conda on UCloud to manage Python-libraries</li> <li>SLURM Clusters on UCloud</li> </ul> <p>STATA</p> <ul> <li>Run Stata on UCloud</li> <li>Install Stata on UCloud</li> <li>Run Stata in jupyter-notebooks</li> <li>Run Stata on Type 3 </li> </ul> <p>SAS</p> <ul> <li>Run SAS on UCloud</li> </ul> <p>Matlab</p> <ul> <li>Run Matlab on UCloud</li> </ul> <ul> <li>Which GPU to Choose?</li> <li>Access GPUs on UCloud</li> <li>GPU Libraries for Python and R</li> <li>Conda: for easy workflow deployment on AAU GPU VMs</li> <li>Run Python and R jupyter notebooks on AAU VMs</li> <li>Setting up jupyter-notebook with GPUs on AAU using Docker images (Hosted by RUC)</li> <li>Pytorch: Train your deep-learning models on UCloud GPUs</li> <li>Tensorflow: Train your deep-learning models on UCloud GPUs</li> <li>RAPIDS-cuML: Train your Scikit-learn models on UCloud GPUs</li> <li>RAPIDS-cuDF: How To Speed Up Pandas in Python By 150x</li> </ul> <ul> <li>Getting Started with large memory HPC (UCloud)</li> <li>Type 3 user guide (from SDU)</li> <li>Run Stata on Type 3 </li> <li>Use Conda to manage Jupyterlab environments on Type 3</li> </ul>"},{"location":"#data-science-links","title":"Data Science Links","text":"Reproducible Data ScienceOnline Learning <ul> <li>The Turing Way - Guide for Reproducible Research in Data Science </li> <li>Cookiecutter Data Science - Reproducible Project Structure</li> <li>The Turing Way - Introduction to version control with Git </li> <li>Coderefinery - Introduction to version control with Git</li> <li>How to Create a Conda Environment Based on a YAML File: A Guide for Data Scientists</li> </ul> <ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> <li>Kaggle.com: Online interactive data science courses</li> <li>Kaggle.com: Python</li> <li>Kaggle.com: Getting staRted in R: First Steps</li> <li>Datacarpentry - Introduction to Stata for Economics</li> </ul>"},{"location":"#hpc-operational-status","title":"HPC Operational Status","text":"<ul> <li>TYPE 1 (UCloud)</li> <li>TYPE 3 (Hippo)</li> <li>TYPE 5 (LUMI)</li> </ul> <p>Planned Maintenance</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#documentation","title":"Documentation","text":"<p>All documentation on this website are made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.</p> <p>You are free:</p> <pre><code>to Share---copy and redistribute the material in any medium or format\nto Adapt---remix, transform, and build upon the material\n</code></pre> <p>for any purpose, even commercially.</p> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms:</p> <pre><code>Attribution---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nNo additional restrictions---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.\n</code></pre> <p>With the understanding that:</p> <pre><code>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n</code></pre>"},{"location":"LICENSE/#codesoftware","title":"Code/Software","text":"<p>All code are made available under the OSI-approved MIT license.</p> <pre><code>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy,     modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n</code></pre>"},{"location":"contact/","title":"Contact","text":"<ul> <li>Research &amp; Data Management @ CBS (rdm@cbs.dk)</li> <li>Kristoffer Gulmark Poulsen (kgp.lib@cbs.dk)</li> <li>Lars Nondal (ln.lib@cbs.dk)</li> </ul>"},{"location":"database_tools/","title":"Database Tools","text":""},{"location":"database_tools/#database-tools","title":"Database Tools","text":"<ul> <li> <p>Moody's Datahub</p> </li> <li> <p>WRDS - Wharton Research Data Services</p> </li> </ul>"},{"location":"events/","title":"Teaching Events","text":"For ResearchersFor StudentsFor All <p>\u00a0\u00a024-12-5 @ 11.00-12.00: - Train your ML/AI Model on GPUs - [Download Slides]</p> <p>\u00a0\u00a024-11-5 @ 11.00-12.00: - HPC &amp; Parallel Programming in Python - [Download Slides]</p> <p>\u00a0\u00a024-10-24 @ 11.00-12.00: - HPC &amp; Parallel Programming in R - [Download Slides]</p> <p>\u00a0\u00a024-10-07 @ 14.00-15.00: - High Performance Computing - [Download Slides]</p> <p></p> <p>\u00a0\u00a0Fall 2024: - HPC &amp; Parallel Programming in Python - [Download Slides]</p> <p>\u00a0\u00a0Fall 2024: - HPC &amp; Parallel Programming in R - [Download Slides]</p> <p>\u00a0\u00a0Fall 2024: - High Performance Computing - [Download Slides]</p> <p></p>"},{"location":"getresources/","title":"Get Resources","text":""},{"location":"getresources/#national-resources","title":"National Resources","text":"<p>Need more Power - Free HPC-Cloud Computing Resources in 2024 -- Apply Before March 12!</p>"},{"location":"getresources/#local-resources","title":"Local Resources","text":"<p>Once a year CBS is awarded Local HPC ressources that can be freely distributed to our researchers and students. CBS primarily have Local Type 1 resources as the reflects our current user needs:</p>"},{"location":"getresources/#how-to-get-local-resources","title":"How to get Local Resources","text":"TYPE 1 (UCloud)TYPE 3 (Hippo)Other <ul> <li> <p>You apply from UCloud by sending a UCloud grant application. </p> </li> <li> <p>Information on machine type selection be found here. </p> </li> <li> <p>Otherwise please contact RDM Support.</p> </li> </ul> <p>Students</p> <ul> <li> <p>CBS students can only under very specific situation get access to Type 3 HPC. If this is of interrest you are welcome to contact RDM Support to discuss further.</p> </li> <li> <p>Please contact RDM Support if you would like to CBS to request Local resources to Type 3.</p> </li> </ul> <p>Staff</p> <ul> <li> <p>You apply from UCloud by sending a UCloud grant application. </p> </li> <li> <p>In 2023 CBS has a very limited Type 3 resource pool. Type 3 grant applications are therefore only granted after thorough evulation by the RDM Support.</p> </li> <li> <p>For more information please contact RDM Support.</p> </li> </ul> <p>Please contact RDM Support if you would like to CBS to request Local resources to Type 2 and 5.</p>"},{"location":"getresources/#sandbox-resources","title":"Sandbox Resources","text":"<p>CBS researchers wanting to test out HPC systems Type 2 to 5 can gain acess to sandbox ressources by contacting RDM Support. Find more information here.</p>"},{"location":"getresources/#grant-applications","title":"Grant Applications","text":"<p>Find an overview of currently open application rounds below. Please contact RDM Support as soon as possible if you consider applying as we can aid in the application process.</p> <ul> <li> <p>TYPE 1 to 3: Researcher can apply for the bi-annual application round for the national HPC resources. </p> </li> <li> <p>TYPE 5 &amp; other international HPC systems: Researcher can apply for resources at LUMI and other international HPC facilities. </p> </li> </ul>"},{"location":"getresources/#current-calls","title":"Current Calls","text":"<ul> <li>DeiC - Call H1-2024 Call for applications for access to the e- resources - (Deadline: September 5th, 2023)</li> </ul>"},{"location":"getresources/#external-links","title":"External Links","text":"<ul> <li>Apply for national HPC resources</li> <li>Acknowledge the use of national HPC </li> <li>The EuroCC Knowledge Pool (Hosted by DeiC)</li> </ul>"},{"location":"hpc/","title":"HPC Facilites","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>Type 3 \u2013 Large Memory HPC (Hippo)</li> <li>National HPC Facilities (DeiC)</li> <li>WRDS - Wharton Research Data Services</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> </ul>"},{"location":"news/","title":"News","text":"<ul> <li>24-05-21 - New UCloud User Interface</li> <li>23-10-25 - Whisper large language model from OpenAI for Transcription of audio files (UCloud)</li> <li>23-08-24 - Larger GPU machine available for UCloud</li> <li>23-07-24 - CodeRefinery workshop September 19-21 and 26-28, 2023</li> <li>23-07-14 - DeiC - Call H1-2024 Call for applications for access to the e- resources</li> <li>23-06-27 - Interactive HPC lives up to highest international standards with ISO 27001</li> <li>23-06-12 - New way to use SSH for accessing apps on Interactive HPC</li> <li>23-05-31 - UCloud Maintenance notice - 27/06/2023</li> <li>23-04-26 - UCloud scheduled maintenance between 8:00-10:00 on Wednesday 26/04/2023.</li> <li>23-04-12 - New milestone as DeiC Interactive HPC reaches 6,000 users</li> <li>23-03-17 - The cost of success \u2013 user overload on DeiC Interactive HPC</li> <li>23-03-15 - Launch of the DeiC Integration Portal</li> </ul>"},{"location":"tut_docs/","title":"Tutorials &amp; Documentation","text":""},{"location":"tut_docs/#cbs-tutorials","title":"CBS Tutorials","text":"Type 1: CPUType 1: GPU <ul> <li>Getting Started with HPC (UCloud)</li> <li>Speed up your Linear Algebra calculations by choosing the right BLAS/LAPACK Library</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>How to Generate SSH key</li> <li>SSH Connection to UCloud using VSCode</li> </ul> <ul> <li>Access GPUs on UCloud</li> <li>Conda: for easy workflow deployment on AAU GPU VMs</li> <li>Run Python and R jupyter notebooks on AAU VMs</li> <li>Pytorch: Train your deep-learning models on UCloud GPUs</li> <li>Tensorflow: Train your deep-learning models on UCloud GPUs</li> <li>RAPIDS-cuML: Train your Scikit-learn models on UCloud GPUs</li> <li>RAPIDS-cuDF: How To Speed Up Pandas in Python By 150x</li> <li>Connect local VSCODE to UCloud job (SSH)</li> </ul>"},{"location":"tut_docs/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"SDUAAU <ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul> <ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"tut_docs/#type-2-throughput","title":"TYPE 2 (Throughput)","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"tut_docs/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"tut_docs/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"tut_docs/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"HPC_Facilities/DeiC/","title":"National HPC Facilities","text":"<p>This page provides an overview of the national HPC facilities (Content is provided by DeiC). </p>"},{"location":"HPC_Facilities/DeiC/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>The type 1 system is mainly focused on interactive computing and easy access for users. The system is made of the YouGene cluster hosted at SDU. CBS staff and students can access the cluster resources via UCloud. </p> <p>Get for Type 1 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/DeiC/#type-2-throughput-hpc","title":"TYPE 2 (Throughput HPC)","text":"<p>Three Type 2 HPC systems are available (Computerome 2.0,GenomeDK and Sophia). This type of HPC system typically has a large number of cores which can be a mix between cost-effective and calculation-efficient units. Type 2 also has the ability to handle large amounts of data and its main focus is on high-throughput performance. </p> <p>Get for Type 2 resources here.</p> <p>More information is found here:</p> <p>Computerome 2.0  \u00a0\u00a0 | \u00a0\u00a0 GenomeDK \u00a0\u00a0 | \u00a0\u00a0 Sophia</p>"},{"location":"HPC_Facilities/DeiC/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<p>This is a large memory HPC. This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area. </p> <p>Type 3 is hosted and maintained at SDU. For the cluster specs check here. </p> <p>User guide can be found here. </p> <p>Get for Type 3 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/DeiC/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<p>Type 5 is the European pre-exascale supercomputer LUMI. LUMI is an abbreviation for \"Large Unified Modern Infrastructure\", and will be located in CSC's data center in Kajaani, Finland. LUMI is one of three pre-exascale supercomputers to be build as part of the European EuroHPC project.</p> <p>LUMI Capability HPC provides a similar setup to DeiC Throughput HPC but with increased possibilities by virtue of state-of-the-art hardware. Specifically the interconnections between compute nodes is designed to minimize latency thereby addressing the issue of communication induced latency in distributed-memory programs running on separate nodes. Additionally the user can obtain access to large amounts of disk space also with low-latency interconnects. In this way Capability HPC enables computations that are prohibitive with DeiC Throughput HPC due to communication latency. </p> <p>Get for Type 5 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/GrantApp/","title":"Applying for ressources in 6 Simple Steps","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/GrantApp/#step-1-select-apply-for-resources-on-the-ucloud-frontpage","title":"Step 1: Select \"Apply for resources\" on the UCloud frontpage","text":""},{"location":"HPC_Facilities/GrantApp/#step-2-select-a-new-or-current-project","title":"Step 2: Select a new or current project","text":""},{"location":"HPC_Facilities/GrantApp/#step-3-allocation-duration","title":"Step 3: Allocation Duration","text":""},{"location":"HPC_Facilities/GrantApp/#step-3-select-grant-giver-hpc-type-1-or-3","title":"Step 3: Select Grant giver HPC Type 1 or 3:","text":"<p>For a vast majority of users only Type 1 is relevant</p> <p></p>"},{"location":"HPC_Facilities/GrantApp/#step-4-choose-between-three-hpc-providers-type-1","title":"Step 4: Choose between three HPC Providers (Type 1)","text":"<p>When Selecting either SDU/K8 or AAU/K8 it is important to also select storage space at either SDU/K8 or AAU/K8.</p> <p>System specifications can be found here.</p> DeiC Interactive HPC (SDU/K8)DeiC Interactive HPC (AAU/K8)DeiC Interactive HPC (AAU) <p></p> <p></p> <p></p>"},{"location":"HPC_Facilities/GrantApp/#step-5-public-ip-or-software-license","title":"Step 5: Public IP or software license","text":"<p>Public IPs or software licenses (SAS or STATA) need to be coupled with SDU/K8 resources.</p> <p></p>"},{"location":"HPC_Facilities/GrantApp/#step-6-project-description","title":"Step 6: project description","text":"<p>Provide a meaningfull project plan and summary. </p>"},{"location":"HPC_Facilities/GrantApp/#step-7-press-submit-application","title":"Step 7: Press \"Submit application\"","text":"<p>Now the application will be evaluated by the CBS front office at first given opportunity. The application will either be accepted otherwise you will be contacted (CBS mail).</p> <p></p>"},{"location":"HPC_Facilities/Hippo/","title":"Type 3 \u2013 Large Memory HPC (Hippo)","text":"<p>This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area. Type 3 is hosted and maintained at SDU. </p> <p>The DeiC Large Memory HPC system is a system consisting of large memory nodes (between 1 and 4 TB RAM per node).</p> <p>System specifications can be found here.</p>"},{"location":"HPC_Facilities/Hippo/#get-started","title":"Get started","text":"<p>Large Memory HPC (Hippo) is integrated with UCloud which providing an easy-to-use interface.</p> <p>The UCloud integration provides three base applications RStudio, JupyterLab and Slurm. See apps.</p> <ul> <li> <p>RStudio and JupyterLab facilites interactive jobs on a single Type3 node for a range of popular programming languages.</p> </li> <li> <p>Slurm provides batch job processing across multiple Type 3 nodes.</p> </li> </ul> <p>To get resources read here.</p> <p>Start by reading the following Type 3 tutorials:</p> <ul> <li>Type 3 user guide (from SDU)</li> <li>Run Stata on Type 3 </li> <li>Use Conda to manage Jupyterlab environments on Type 3</li> </ul>"},{"location":"HPC_Facilities/Hippo/#user-support","title":"User Support","text":"<p>All UCloud support should go through the RDM Support. If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/License/","title":"Add License to STATA and SAS application","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/License/#add-local-license","title":"Add Local License","text":""},{"location":"HPC_Facilities/License/#step-1-upload-local-stata-lic-file-or-sas-txt-file-license-to-ucloud","title":"Step 1: Upload local STATA (.lic file) or SAS (.txt file) license to UCloud","text":""},{"location":"HPC_Facilities/License/#step-2-select-the-license-file-lic-or-txt-while-setting-up-ucloud-job","title":"Step 2: Select the license file (.lic or .txt) while setting up UCloud Job","text":""},{"location":"HPC_Facilities/License/#add-server-license","title":"Add Server License","text":""},{"location":"HPC_Facilities/License/#step-1-apply-for-server-license-through-ucloud-grant-application","title":"Step 1: Apply for Server License through UCloud Grant Application","text":""},{"location":"HPC_Facilities/License/#step-2-activate-license-sas-94-license-shown-as-example","title":"Step 2: Activate License (SAS 9.4 license shown as example)","text":""},{"location":"HPC_Facilities/License/#step-3-select-server-license-while-setting-up-ucloud-job","title":"Step 3: Select server license while setting up UCloud Job","text":""},{"location":"HPC_Facilities/MachineType/","title":"Type 1 \u2013 Interactive HPC (UCloud)","text":"<p>The easiest-to-use HPC service is DeiC Interactive HPC (Type 1) also known as UCloud. </p> <p>This service is provided by the Danish universities SDU and AAU and consist of three HPC systems:</p> Institution System User Access User-Friendly Sensitive data SDU/K8 Kubernetes GUI / Interactive Yes Yes AAU/K8 Kubernetes GUI / Interactive Yes Yes AAU Open Stack SSH / terminal No No"},{"location":"HPC_Facilities/MachineType/#costs","title":"Costs","text":"<p>Machine types are devided into CPU and GPU based machines with their costs being  measured in core hours and GPU core-hours respectively.</p> <p>A 1 (GPU) core machine cost 1 (GPU) core-hour pr. hour and a 64 (GPU) core machine cost 1 (GPU) core-hour pr. hour.</p>"},{"location":"HPC_Facilities/MachineType/#machine-type","title":"Machine Type","text":"OverviewSDU/K8AAU/K8AAU <p>These HPC systems provides users with the following machine types:</p> Name Institution CPU (Cores) RAM (GB) GPU (Cores) User Access User-Friendly u1-standard SDU/K8 64 384 N/A GUI / Interactive Yes u3-gpu SDU/K8 192 768 4x NVIDIA H100 GUI / Interactive Yes uc1-gc AAU/K8 128 512 N/A GUI / Interactive Yes uc1-A10 AAU/K8 64 256 4x NVIDIA A10 GUI / Interactive Yes uc-t4 AAU 60 240 6x Nvidia T4 SSH / terminal No uc-a10 AAU 64 240 4x Nvidia A10 SSH / terminal No uc-a40 AAU 64 480 3x Nvidia A40 SSH / terminal No uc-a100 AAU 64 480 2x Nvidia A100 SSH / terminal No <p>(SDU/K8) provides CPU and GPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as they would on your laptop. See all apps.</p> <p>CPU Nodes (u1-standard):</p> <pre><code>Dell PowerEdge C6420\n64 Core Intel Xeon Gold 6130\n384 GB DDR4-2400\n</code></pre> <p></p> <p>GPU Nodes (u3-gpu):</p> <pre><code>Lenovo ThinkSystem SR675 V3  \n192 Core AMD EPYC 9454 \n768 GB DDR5-4800 \n4x NVIDIA Hopper H100-SXM5, 80 GB\n</code></pre> <p></p> <p>Storage:</p> <pre><code>u1-cephfs - The storage system for DeiC interactive HPC\n</code></pre> <p>(AAU/K8) provides CPU and GPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as they would on your laptop. See all apps.</p> <p>CPU Nodes (uc1-gc):</p> <pre><code>128 Core AMD EPYC 7702 \n512 GBs of RAM\n</code></pre> <p></p> <p>GPU Nodes (uc1-A10):</p> <pre><code>64 Core Intel Xeon Gold 6326\n256 GBs of RAM\n4x NVIDIA A10 GPU\n</code></pre> <p></p> <p>Storage:</p> <pre><code>uc1-cephfs - The storage system for DeiC interactive HPC (AAU).\n</code></pre> <p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab.</p> NO SENSITIVE DATANvidia T4Nvidia A10Nvidia A40Nvidia A100 <p>Virtual machines hosted by AAU are only suitable for the processing of</p> <ul> <li>public and,</li> <li>low risk internal data.</li> </ul> <p>The specific data classification model applicable to your institution should be consulted before using these compute resources.</p> <p>AUTOMATIC SHUT OFF AND DELETION</p> <p>It is your responsibility to ensure that you have sufficient resources to keep your virtual machine running while you need it. All virtual machines are automatically shut off when the user runs out of resources.</p> <p>To restart the machine, additional funds should be requested from your front office to cover the full duration that the virtual machine is shut off, plus any additional compute time.</p> <p>All data and information related to virtual machines are IRRECOVERABLY deleted if shut off for:</p> <ul> <li>A100 virtual machines: 3 days (72 hours)</li> <li>All other virtual machines: 14 days (336 hours)</li> </ul> <p>Four different machine types based on different Nvidia GPUs (T4, A10 , A40 and A100) with different application purposes. </p> <p>Nvidia T4 is avaliable on the following machines:</p> <p></p> <p>Nvidia A10 is avaliable on the following machines:</p> <p></p> <p>Nvidia A40 is avaliable on the following machines:</p> <p></p> <p>Nvidia A100 is avaliable on the following machines:</p> <p></p>"},{"location":"HPC_Facilities/NGC/","title":"Nationalt Genom Center HPC (Danish Statistics Data)","text":"<p>It is possible to access and process data from Danish Statistics through the Nationalt Genom Center HPC. This service is not funded by CBS. See pricing list.</p> <p>A techinal guide is found here.</p> <p>The linked information is only avaliable in danish.</p> <p>For further information or support please contact RDM Support.</p>"},{"location":"HPC_Facilities/Overview/","title":"Overview","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>National HPC Facilities (DeiC)</li> <li>[WRDS - Wharton Research Data Services](/HPC_Facilities/WRDS/</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> </ul>"},{"location":"HPC_Facilities/UCloud/","title":"Type 1 \u2013 Interactive HPC (UCloud)","text":"<p>The easiest-to-use HPC service is DeiC Interactive HPC (Type 1) also known as UCloud. This service is provided by the Danish universities SDU and AAU and consist of three HPC Facilites:</p> <p>(SDU/K8) and (AAU/K8) provides CPU and GPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as they would on your laptop. See all apps.</p> <p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab. </p> <p>System specifications can be found here.</p>"},{"location":"HPC_Facilities/UCloud/#login-on-ucloud","title":"Login on UCloud","text":"<p>You can login on to UCloud using WAYF (Where Are You From). Press here to login.</p> <ul> <li>Select Copenhagen Business School as your affiliate institution on the login page. </li> <li>Sign in using your CBS mail account</li> </ul> <p>Upon the first login it is necessary to approve the SDU eScience terms of service. Afterwards, the user is redirected to the UCloud user interface.</p> <p>Note: After login the user can activate two factor authentication by clicking on the avatar icon in the top-right corner of the home screen.</p>"},{"location":"HPC_Facilities/UCloud/#getting-started","title":"Getting started","text":"<p>All new users in UCloud are awarded a \"My Workspace\" with 14.000 Core-Hours (CoreH) of computing (CPU only) resources to the \"DeiC Interactive HPC (SDU)/K8\", as well as 50 GB remote storage. You can use these resources to get acquainted with the system, run test jobs, etc. </p> <p>The largest machine (64 cores &amp; 384 GB memory) cost 64 coreH pr. hour. So the free 14.000 core hours will give you access to approx. 182 hours of inital run time.</p> <p>For additional resources see here.</p> <p>Start by watching the following UCloud tutorials:</p> <ul> <li>Manage Files and Folders</li> <li>Manage Applications</li> <li>Manage Projects</li> <li>Use Cases</li> <li>Webinars</li> <li>UCloud Documentation</li> </ul> <p>More Tutorials and Documentation can be found here</p>"},{"location":"HPC_Facilities/UCloud/#user-support","title":"User Support","text":"<p>All UCloud support should go through the RDM Support. If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/UCloud/#collaboration","title":"Collaboration","text":"<p> International Collaborators</p> <p>International researchers need a \"visiting researcher premission\"(g\u00e6steforskeradgang) to CBS to gain access to UCloud. One can be obtained by contacting CBS HR (hr@cbs.dk).</p> <p>Once this is in place CBS HPC support will contact the UCloud Research Support Team and provide the below shown information. </p> <ul> <li> <p>Full name:</p> </li> <li> <p>Occupation:</p> </li> <li> <p>Organisation (University):</p> </li> <li> <p>Email (University):</p> </li> </ul> <p>Subsequently, the UCloud Research Support Team will contact the researcher to verify their identity through a video meeting. A valid ID is needed. </p>"},{"location":"HPC_Facilities/UCloud/#license-software","title":"License Software","text":"<p>There are several types of licensed software that can be run on UCloud. </p> MATLABSTATASAS &amp; SAS Studio <p>  A Matlab server license is needed in order to run the application on UCloud. Once can be acquired through CBS IT help desk at own expense.</p> <ul> <li> <p>Matlab UCloud Application</p> </li> <li> <p>UCloud Matlab Documentation</p> </li> <li> <p>UCloud video tutorial </p> <ul> <li>Matlab walkthrough starts at 16:00 minutes into the video. </li> <li>Shows how activate Matlab with a personal license.</li> </ul> </li> </ul> <p> Users can either upload their own personal STATA license (.lic file) to UCloud or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>STATA UCloud Application</p> </li> <li> <p>UCloud STATA Documentation</p> </li> </ul> <p> Users can either upload their own personal SAS license (.txt file) or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>SAS UCloud Application</p> </li> <li> <p>UCloud SAS Documentation</p> </li> </ul>"},{"location":"HPC_Facilities/WRDS/","title":"WRDS - Wharton Research Data Services","text":"<p>WRDS- Wharton Research Data Services provides access to financial data, accounting figures as well as banking and management information. CBS students and staff must register to get access. </p> <p>More information can be found here.</p> <p>For further support lease contact RDM Support.</p>"},{"location":"HPC_Facilities/WRDS/#accessing-wrds-databases","title":"Accessing WRDS databases:","text":""},{"location":"HPC_Facilities/WRDS/#local-pc-ucloud","title":"Local PC &amp; UCloud","text":"<ul> <li>Python</li> <li>R</li> <li>Matlab</li> <li>SAS</li> <li>STATA</li> <li>UCloud Templates/Scripts </li> </ul>"},{"location":"HPC_Facilities/WRDS/#wrds-cloud","title":"WRDS Cloud","text":"<p>WRDS Cloud is a HPC service with the possibility to process the data avaliable on WRDS. WRDS Cloud is only acessable for CBS staff and researchers.</p>"},{"location":"HPC_Facilities/WRDS/#available-software","title":"Available Software","text":"<p>The WRDS Cloud provides the following software:</p> <ul> <li>SAS 9.4</li> <li>R 3.5</li> <li>Python 3.6 and 2.7</li> <li>Stata 15 (requires special subscription agreement with StataCorp)</li> </ul> <p>In addition, it further supports remote access from:</p> <ul> <li>MATLAB 2016a +</li> <li>Native PostgreSQL clients</li> <li>ODBC- or JDBC-compliant clients</li> </ul>"},{"location":"HPC_Facilities/WRDS/#wrds-cloud-documentation","title":"WRDS Cloud Documentation","text":"<ul> <li>Introduction to the WRDS Cloud</li> <li>Using SSH to Connect to the WRDS Cloud </li> </ul>"},{"location":"HPC_Facilities/status/","title":"HPC Operational Status","text":"<ul> <li>Type 1 (UCloud)</li> <li>Type 3 (Hippo)</li> <li>Type 5 (LUMI)</li> </ul>"},{"location":"Tutorial_Docs/BLAS/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library","text":"<p>When it comes to numerical computations and linear algebra in the R programming language, the BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries play crucial roles. These libraries provide a collection of efficient and optimized routines for various linear algebra operations, such as matrix multiplication, solving linear systems, eigenvalue computations, and more.</p> <p>BLAS, the Basic Linear Algebra Subprograms, is a standard interface specification for low-level linear algebra operations. It defines a set of routines that perform basic vector and matrix operations efficiently. The BLAS routines are highly optimized and implemented in highly efficient machine code to take advantage of the specific hardware architecture. R relies on the BLAS library for fundamental linear algebra operations, providing performance improvements for numerical computations.</p> <p>LAPACK, the Linear Algebra Package, builds upon the BLAS library and provides higher-level routines for solving more complex linear algebra problems. LAPACK offers a comprehensive set of algorithms for solving systems of linear equations, eigenvalue problems, least-squares problems, and singular value decompositions. These routines are widely used in various scientific and engineering applications.</p>"},{"location":"Tutorial_Docs/BLAS/#r","title":"R","text":"<ul> <li>Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.</li> </ul>"},{"location":"Tutorial_Docs/BatchMode/","title":"UCloud Tutorial: Batch Processing on UCloud","text":"<p>Running non-interactive script in \"batch mode\" can help overcome the UCloud capacity issues as jobs can be started any time and then executed whenever the systems resources permits.  </p> <p>Many applications already have a \"Batch Processing\" UCloud functionality:</p> <ul> <li>Batch Processing</li> <li>RStudio</li> <li>JupyterLab</li> <li>Matlab</li> <li>PyTorch</li> <li>Tensorflow</li> <li>Spark Cluster</li> <li>Terminal/SLURM Cluster</li> </ul> <p>Application that currently does not have this UCloud functionality (e.g. Stata) can instead run batch mode computations using the terminal app. See the following tutorials:</p> <ul> <li>Stata</li> </ul>"},{"location":"Tutorial_Docs/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <ul> <li> <p>R</p> </li> <li> <p>Python</p> </li> </ul> <p>Further documentation can be found on UCloud:</p> <ul> <li>Conda on UCloud </li> </ul>"},{"location":"Tutorial_Docs/VMs/","title":"Access GPUs on UCloud","text":"<ul> <li> <p>GPUs are accessible on UCloud through virtual machines (VMs) hosted by AAU. </p> </li> <li> <p>GPU resources can  be obtained through a UCloud grant application.</p> </li> <li> <p>Information of the different GPU systems can be found here.</p> </li> </ul>"},{"location":"Tutorial_Docs/VMs/#tutorials","title":"Tutorials","text":"<ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access at data transfer to AAU VMs using SSH</p> </li> </ul>"},{"location":"Tutorials/BLAS/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library","text":"<p>When it comes to numerical computations and linear algebra in the R programming language, the BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries play crucial roles. These libraries provide a collection of efficient and optimized routines for various linear algebra operations, such as matrix multiplication, solving linear systems, eigenvalue computations, and more.</p> <p>BLAS, the Basic Linear Algebra Subprograms, is a standard interface specification for low-level linear algebra operations. It defines a set of routines that perform basic vector and matrix operations efficiently. The BLAS routines are highly optimized and implemented in highly efficient machine code to take advantage of the specific hardware architecture. R relies on the BLAS library for fundamental linear algebra operations, providing performance improvements for numerical computations.</p> <p>LAPACK, the Linear Algebra Package, builds upon the BLAS library and provides higher-level routines for solving more complex linear algebra problems. LAPACK offers a comprehensive set of algorithms for solving systems of linear equations, eigenvalue problems, least-squares problems, and singular value decompositions. These routines are widely used in various scientific and engineering applications.</p>"},{"location":"Tutorials/BLAS/#r","title":"R","text":"<ul> <li>Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.</li> </ul>"},{"location":"Tutorials/BLAS/#python","title":"Python","text":"<ul> <li>Speed up your Numpy calculations</li> </ul>"},{"location":"Tutorials/BLAS/BLAS_Python/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.","text":"<p>In the context of R, there are several libraries available that interface with the BLAS and LAPACK libraries to enhance their functionality or provide alternative implementations. Here are a few notable libraries:</p> <ul> <li> <p>base R: The base R installation includes a reference BLAS implementation that provides basic linear algebra functionality. While this implementation is not as optimized as other libraries, it serves as a fallback option when more optimized libraries are not available.</p> </li> <li> <p>OpenBLAS: OpenBLAS is an optimized BLAS and LAPACK library that offers high-performance linear algebra routines. It is widely used and provides significant speed improvements over the reference BLAS implementation in base R.</p> </li> <li> <p>Intel MKL: The Intel Math Kernel Library (MKL) is a highly optimized set of mathematical functions for various platforms, including CPUs from Intel. It includes efficient implementations of BLAS and LAPACK routines and is known for its excellent performance. MKL is often favored for its outstanding performance on Intel architectures.</p> </li> <li> <p>ACML: The AMD Core Math Library (ACML) is an optimized library for AMD processors. It provides optimized BLAS and LAPACK routines tailored for AMD architectures.</p> </li> <li> <p>vecLib: vecLib is the BLAS and LAPACK library included with macOS. It provides optimized routines for Apple hardware.</p> </li> <li> <p>ATLAS: ATLAS (Automatically Tuned Linear Algebra Software) is another popular BLAS implementation that can be used as an alternative to the reference BLAS library. ATLAS utilizes an automated tuning process to generate highly optimized code specifically tailored to the host system's architecture. It offers improved performance for various linear algebra operations.</p> </li> </ul> <p>These libraries can be linked with R during installation or dynamically loaded during runtime, allowing users to choose the most suitable implementation for their specific hardware and performance requirements.</p> <p>In summary, the BLAS and LAPACK libraries are essential components for numerical computations and linear algebra in R. They provide efficient and optimized routines for fundamental linear algebra operations, and various libraries such as OpenBLAS, Intel MKL, ACML, vecLib, and ATLAS enhance their functionality or provide alternative implementations for improved performance.</p>"},{"location":"Tutorials/BLAS/BLAS_Python/#rstudio-on-ucloud","title":"RStudio on UCloud","text":"<p>When looking at the different R/RStudio version available on UCloud (see below) it can be observed that some have the suffix \"_MKL\". These have \"Intel MKL\" as their default BLAS/LAPACK library, while versions with out the suffix have the \"LibBLAST\" library whihc is the base R library on linux systems.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_Python/#use-the-sessioninfo-function-to-check-which-blaslapack-library-is-deployed","title":"Use the \"sessionInfo()\" function to check which BLAS/LAPACK library is deployed:","text":"<p>MKL</p> <p></p> <p>LibBLAS</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_Python/#benchmarking-the-two-versions","title":"Benchmarking the two versions","text":"<p>Benchmarking a simple matrix multiplication shows that the \"MKL\" is close 78 times faster than the \"LibBLAS\"!!</p> <p>MKL mean = 7.63 milliseconds</p> <p></p> <p>LiBLAS mean = 592.666 milliseconds</p> <p></p> <pre><code>library(microbenchmark)\n\nn &lt;- 1000\nmat1 &lt;- matrix(rnorm(n*n), ncol=n)\nmat2 &lt;- matrix(rnorm(n*n), ncol=n)\n\ntic()\nf &lt;- function() mat1 %*% mat2\nres &lt;- microbenchmark(f(), times=100L)\nres\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#installing-and-changing-blas-lapack-library","title":"Installing and Changing BLAS &amp; LAPACK Library","text":"<p>Select between the available BLAS/LAPACK libraies by posting the command below i the job terminal. </p> <pre><code># BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n# LAPACK Selection\nsudo update-alternatives --config liblapack.so.3-x86_64-linux-gnu\n\n\n\n# BLAS Selection Output: \nThere are 2 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#install-other-libraries","title":"Install other libraries.","text":"<p>Changing the BLAS/LAPACK libraries is not possible for the \"MKL\" versions of the RStudio applications on UCloud. </p> <pre><code># install OpenBLAS\nsudo apt-get install libopenblas-base\n# install ATLAS\nsudo apt-get install libatlas3-base liblapack3\n\n# BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n\n# BLAS Selection Output: \nThere are 4 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3               35        manual mode\n  3            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n  4            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number: ^C\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#select-an-alternative-library-atlas-libraries-in-this-case","title":"Select an alternative library (Atlas libraries in this case)","text":"<p>Open R and confirm the change of BLAS/LAPACK library using \"sessionInfo()\" function.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.","text":"<p>In the context of R, there are several libraries available that interface with the BLAS and LAPACK libraries to enhance their functionality or provide alternative implementations. Here are a few notable libraries:</p> <ul> <li> <p>base R: The base R installation includes a reference BLAS implementation that provides basic linear algebra functionality. While this implementation is not as optimized as other libraries, it serves as a fallback option when more optimized libraries are not available.</p> </li> <li> <p>OpenBLAS: OpenBLAS is an optimized BLAS and LAPACK library that offers high-performance linear algebra routines. It is widely used and provides significant speed improvements over the reference BLAS implementation in base R.</p> </li> <li> <p>Intel MKL: The Intel Math Kernel Library (MKL) is a highly optimized set of mathematical functions for various platforms, including CPUs from Intel. It includes efficient implementations of BLAS and LAPACK routines and is known for its excellent performance. MKL is often favored for its outstanding performance on Intel architectures.</p> </li> <li> <p>ACML: The AMD Core Math Library (ACML) is an optimized library for AMD processors. It provides optimized BLAS and LAPACK routines tailored for AMD architectures.</p> </li> <li> <p>vecLib: vecLib is the BLAS and LAPACK library included with macOS. It provides optimized routines for Apple hardware.</p> </li> <li> <p>ATLAS: ATLAS (Automatically Tuned Linear Algebra Software) is another popular BLAS implementation that can be used as an alternative to the reference BLAS library. ATLAS utilizes an automated tuning process to generate highly optimized code specifically tailored to the host system's architecture. It offers improved performance for various linear algebra operations.</p> </li> </ul> <p>These libraries can be linked with R during installation or dynamically loaded during runtime, allowing users to choose the most suitable implementation for their specific hardware and performance requirements.</p> <p>In summary, the BLAS and LAPACK libraries are essential components for numerical computations and linear algebra in R. They provide efficient and optimized routines for fundamental linear algebra operations, and various libraries such as OpenBLAS, Intel MKL, ACML, vecLib, and ATLAS enhance their functionality or provide alternative implementations for improved performance.</p>"},{"location":"Tutorials/BLAS/BLAS_R/#rstudio-on-ucloud","title":"RStudio on UCloud","text":"<p>When looking at the different R/RStudio version available on UCloud (see below) it can be observed that some have the suffix \"_MKL\". These have \"Intel MKL\" as their default BLAS/LAPACK library, while versions with out the suffix have the \"LibBLAST\" library whihc is the base R library on linux systems.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/#use-the-sessioninfo-function-to-check-which-blaslapack-library-is-deployed","title":"Use the \"sessionInfo()\" function to check which BLAS/LAPACK library is deployed:","text":"<p>MKL</p> <p></p> <p>LibBLAS</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/#benchmarking-the-two-versions","title":"Benchmarking the two versions","text":"<p>Benchmarking a simple matrix multiplication shows that the \"MKL\" is close 78 times faster than the \"LibBLAS\"!!</p> <p>MKL mean = 7.63 milliseconds</p> <p></p> <p>LiBLAS mean = 592.666 milliseconds</p> <p></p> <pre><code>library(microbenchmark)\n\nn &lt;- 1000\nmat1 &lt;- matrix(rnorm(n*n), ncol=n)\nmat2 &lt;- matrix(rnorm(n*n), ncol=n)\n\ntic()\nf &lt;- function() mat1 %*% mat2\nres &lt;- microbenchmark(f(), times=100L)\nres\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#installing-and-changing-blas-lapack-library","title":"Installing and Changing BLAS &amp; LAPACK Library","text":"<p>Select between the available BLAS/LAPACK libraies by posting the command below i the job terminal. </p> <pre><code># BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n# LAPACK Selection\nsudo update-alternatives --config liblapack.so.3-x86_64-linux-gnu\n\n\n\n# BLAS Selection Output: \nThere are 2 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#install-other-libraries","title":"Install other libraries.","text":"<p>Changing the BLAS/LAPACK libraries is not possible for the \"MKL\" versions of the RStudio applications on UCloud. </p> <pre><code># Update\nsudo apt-get update\n# install OpenBLAS\nsudo apt-get install libopenblas-base\n# install ATLAS\nsudo apt-get install libatlas3-base liblapack3\n\n# BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n\n# BLAS Selection Output: \nThere are 4 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3               35        manual mode\n  3            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n  4            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number: ^C\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#select-an-alternative-library-atlas-libraries-in-this-case","title":"Select an alternative library (Atlas libraries in this case)","text":"<p>Open R and confirm the change of BLAS/LAPACK library using \"sessionInfo()\" function.</p> <p></p>"},{"location":"Tutorials/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <p>R</p> <p>Python</p> <p>Further documentation can be found on UCloud:</p> <p>Conda on UCloud </p>"},{"location":"Tutorials/Conda/Conda_Python/","title":"UCloud Tutorial: Using Conda for easy management of Python environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_Python/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_Python/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#installing-and-activate-python-environments","title":"Installing and activate Python environments","text":"<p>https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html </p> <pre><code># Showing available python versions\nconda search python\n\n# Installing a Python environment (Python 3.9 in this example) \nconda create -n myenv python=3.9\n\n# Or install packages during installation.\nconda create -n myenv python=3.9 numpy=1.16\n\n# Shows already installed environments (R-4.2.3 show be displayed)\nconda env list\n\n# Activate environment\nconda activate myenv\n\n#Check which Python is in path\nwhich python\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#vscode-on-ucloud","title":"VScode on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-coder-python-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Coder python UCloud job.","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-coder.html?highlight=coder</p> <p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-vscode-interface-and-open-file-and-activate-myenv-as-python-interpreter","title":"Now you can launch VSCode interface and open file and activate \u201cmyenv\u201d as python interpreter:","text":"<p>Select the menu View -&gt; Command Palette:</p> <p></p> <p>Execute the command &gt; Python: Select Intepreter:</p> <p></p>"},{"location":"Tutorials/Conda/Conda_Python/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info --envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install ipykernel:\n\nconda install ipykernel\n\n# \npython -m ipykernel install --user --name myenv --display-name \"myenv\"\n\n# De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_Python/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python_1","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#installing-and-activate-r-environments","title":"Installing and activate R environments","text":"<p>https://docs.anaconda.com/free/anaconda/packages/using-r-language/</p> <pre><code># Installing a R environment\nconda create -n  myenv r-essentials r-base\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#install-packages-through-conda","title":"install packages through Conda","text":"<p>When using conda to install R packages, you will need to add r- before the regular package name:</p> <pre><code># For instance, if you want to install rbokeh:\nconda install r-rbokeh\n\n# or for rJava:\nconda install r-rjava\n\n# Update packages:\n\nconda update r-caret\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Conda_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Mamba_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Mamba_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#installing-r-environment-using-conda","title":"Installing R environment using Conda","text":""},{"location":"Tutorials/Conda/Mamba_R/#installing-mamba-add-in","title":"Installing Mamba add-in","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p>"},{"location":"Tutorials/Conda/Mamba_R/#installing-and-activate-r-environment-with-mamba","title":"Installing and activate R environment with mamba","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p> <p>https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda</p> <pre><code>#Installing a R environment (R-4.2.3 in this example) \nconda create --solver=libmamba -n myenv -y -c conda-forge r-base=4.2.3\n\n#Or install packages during installation.\nconda create --solver=libmamba -n myenv -y -c conda-forge r-base=4.2.3 r-tidyverse\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Start R:\nR\n\n# Install a package\ninstall.packages(\u201ctidymodels\u201d)\n\n# Close R:\nquit()\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Mamba_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Install iRkernel:\nconda install -c conda-forge r-irkernel\n\n# Activate R Kernel in Jupter\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n\n# De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Mamba_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-r-environment-using-conda","title":"Installing R environment using Conda","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-mamba-add-in","title":"Installing Mamba add-in","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p> <pre><code># Installing mamba add-in:\nconda install -n base -c conda-forge mamba\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-and-activate-r-environment-with-mamba","title":"Installing and activate R environment with mamba","text":"<p>https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda</p> <pre><code>#Showing available R versions\nmamba search -c conda-forge r-base\n\n#Installing a R environment (R-4.2.3 in this example) \nmamba create -n myenv -y -c conda-forge r-base=4.2.3\n\n#Or install packages during installation.\nmamba create -n myenv -y -c conda-forge r-base=4.2.3 r-tidyverse\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/GPU/gpu_libraries/","title":"GPU libraries","text":"<p>The list of GPU libraries and packages provided below are by no means exhaustive, and the world of GPU-accelerated computing is continually evolving. Users are encouraged to conduct their own due diligence, explore additional packages, and stay updated with the latest developments in the GPU computing ecosystem to best suit their specific requirements and project needs.</p> PythonR <p></p> <ol> <li> <p>CUDA Toolkit - The official parallel computing platform and API developed by NVIDIA for GPU acceleration.</p> </li> <li> <p>CuPy - An open-source GPU-accelerated array library compatible with NumPy.</p> </li> <li> <p>PyTorch - A popular deep learning framework with GPU support for defining and training neural networks.</p> </li> <li> <p>TensorFlow - A widely-used deep learning framework for defining and training machine learning models on GPUs.</p> </li> <li> <p>scikit-cuda - A library providing Python interfaces to various CUDA libraries and functions for GPU acceleration.</p> </li> <li> <p>RAPIDS - A suite of GPU-accelerated data science libraries, including cuDF, cuML, and cuGraph, developed by NVIDIA.</p> </li> <li> <p>Numba - A Just-In-Time (JIT) compiler for Python that can generate optimized machine code for CPU and GPU execution.</p> </li> <li> <p>Theano: (No longer actively developed) - An early deep learning framework that supported GPU acceleration. Many of its concepts have influenced other frameworks.</p> </li> <li> <p>MXNet - A deep learning framework known for its flexibility and efficiency in training neural networks on GPUs.</p> </li> <li> <p>Cupy-cuBLAS - An extension of CuPy that provides GPU-accelerated linear algebra operations using the cuBLAS library.</p> </li> </ol> <p>Note: Make sure to check the official websites or repositories of these libraries for the latest installation instructions and documentation.</p> <p></p> <ol> <li> <p>gputools - Provides GPU-based tools for data manipulation, including matrix operations and basic statistics.</p> </li> <li> <p>gpuR - Offers GPU support for matrix operations and linear algebra in R.</p> </li> <li> <p>RapidsR - An R interface to the RAPIDS suite of GPU-accelerated data science libraries developed by NVIDIA, including cuDF and cuML.</p> </li> <li> <p>mxnet - The R interface to the MXNet deep learning framework, which allows you to train and deploy neural networks on GPUs.</p> </li> <li> <p>tensorflow - The R interface to TensorFlow, a popular deep learning framework with GPU support.</p> </li> <li> <p>cudaBayesreg - A package for Bayesian regression analysis with GPU support.</p> </li> <li> <p>gpuMagic - A package for general-purpose GPU computing in R.</p> </li> <li> <p>H2O4GPU - A GPU-accelerated machine learning library for R, offering a wide range of machine learning algorithms optimized for GPUs.</p> </li> <li> <p>Rtorch - An R interface to PyTorch, a popular deep learning framework, allowing you to create and train neural networks on GPUs.</p> </li> <li> <p>gmatrix - Offers GPU support for matrix operations and linear algebra in R.</p> </li> </ol> <p>Note: Some of these packages may require specific GPU hardware and dependencies. Be sure to check the official documentation and package repositories for installation instructions and system requirements.</p> <p>You can click on the provided links or search for the package names on CRAN or GitHub to find more information about each package and how to install and use them in your R projects.</p>"},{"location":"Tutorials/GPU/pytorch_ddp/","title":"Pytorch: Train your deep-learning models on AAU GPUs","text":"<p>This tutorial show how to deploy \"Distributed Data Parallel (DDP) in PyTorch\" to efficiently train your deep-learning models on the AAU GPUs avalaible through UCloud.</p> <p>See here for a more detailed tutorial on DDP using Pytorch.</p> <p>This tutorial specifically focuses on Part 4: Multi-GPU DDP Training with fault tolerance using Torchrun. </p> <p>The following python scripts are needed to replicate this tutorial: </p> <ul> <li>multigpu_torchrun.py (Can be used as template)</li> <li>datautils.py (Used to load dummy data)</li> </ul> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/GPU/pytorch_ddp/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#activate-conda","title":"Activate Conda","text":"<p>This can be done by either installing a conda from scratch or by deploying er prior installation. Please see  \"Using Conda for easy workflow deployment on AAU GPU VMs\" for information.</p> <pre><code># Download and install miniconda (If needed)\ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#create-or-re-load-a-pytorch-conda-environment","title":"Create or re-load a Pytorch-conda environment","text":"<p>look for latest pytorch installation at https://pytorch.org/get-started/locally/</p> <pre><code># Create pytorch conda environment if non-exist on the Conda installation\nconda deactivate\nconda create --name pytorch\nconda activate pytorch\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#transfer-files-and-folders-ssh-copy-to-vm","title":"Transfer Files and Folders (SSH-Copy) to VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path\\pytorch_folder\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#run-pytorch-training-in-ddp-mode","title":"Run Pytorch training in DDP mode:","text":"<p>In this example a model is trained for 50 epocs with a model snapshot (\"snapshot.pt\") being save every 10 epocs and the final model being saved as \"finalmodel.pt\". </p> <p>Line 78 and 79 needs to be changed to adjust training data and model.   78   train_set = MyTrainDataset(2048)  # load your dataset  79   model = torch.nn.Linear(20, 1)  # load your model</p> <p>\"--nproc_per_node=\" determines the number of GPUs to use. \"gpu\" pytorch will utilise all avaliable GPUs.</p> <pre><code>torchrun --standalone --nproc_per_node=gpu multigpu_torchrun.py 50 10\n</code></pre> <pre><code>$ torchrun --standalone --nproc_per_node=2 multigpu_torchrun.py 50 10\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nWARNING:torch.distributed.run:\n*****************************************\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n*****************************************\n[GPU0] Epoch 0 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 0 | Batchsize: 32 | Steps: 32\nEpoch 0 | Training snapshot saved at snapshot.pt\n[GPU1] Epoch 1 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 1 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 2 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 2 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 3 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 3 | Batchsize: 32 | Steps: 32\n...\n[GPU0] Epoch 7 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 7 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 8 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 8 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 9 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 9 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 10 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 10 | Batchsize: 32 | Steps: 32\nEpoch 10 | Training snapshot saved at snapshot.pt\n[GPU0] Epoch 11 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 11 | Batchsize: 32 | Steps: 32\n...\n[GPU0] Epoch 47 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 47 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 48 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 48 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 49 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 49 | Batchsize: 32 | Steps: 32\nFinal model saved at finalmodel.pt\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#track-the-gpu-usage","title":"Track the GPU usage","text":"<pre><code>nvidia-smi -l 5 # Will update every 5 seconds\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A      2312      C   python                           1324MiB |\n|    0   N/A  N/A      2381      C   ...a3/envs/rapids/bin/python     1042MiB |\n|    1   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    1   N/A  N/A      2383      C   ...a3/envs/rapids/bin/python     1042MiB |\n+-----------------------------------------------------------------------------+\nTue Aug 29 11:04:31 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P0    49W /  70W |   2389MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   36C    P0    53W /  70W |   1067MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#transfer-results-and-conda-enviroment-local-machine-ssh-copy","title":"Transfer Results and Conda enviroment local machine (SSH-Copy)","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cudf/","title":"RAPIDS-cuDF: How To Speed Up Pandas in Python By 150x","text":"<p>This tutorial show how to deploy cuDF - GPU DataFrames on the AAU GPUs avalaible through UCloud. </p> <p>\"cuDF is a GPU DataFrame library for loading joining, aggregating, filtering, and otherwise manipulating data. cuDF leverages libcudf, a blazing-fast C++/CUDA dataframe library and the Apache Arrow columnar format to provide a GPU-accelerated pandas API.\"</p> <p>It is suggested to watch the following youtube video showcasing the easy of use and increased performance for larger dataframe operation.</p> <p>Installation and configuration of Rapids on AAU GPU machines are described in detail here.</p> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> <li> <p>RAPIDS-cuML: Train your Scikit-learn models on AAU GPUs </p> </li> </ul>"},{"location":"Tutorials/GPU/rapids_cuml/","title":"RAPIDS-cuML: Train your Scikit-learn models on AAU GPUs","text":"<p>This tutorial show how to deploy RAPIDS-cuML-GPU Machine Learning Algorithms to efficiently train your scikit-learn models on the AAU GPUs avalaible through UCloud. </p> <p>\"cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions. cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents. For details on performance, see the cuML Benchmarks Notebook.\"</p> <p>In most cases, cuML's Python API matches the API from scikit-learn. which will make it easy to navigate from scikit-learn to RAPIDS-cuML</p> <p>A table of the supported algoritmns can be found here. </p> <p>This tutorial will use a random forrest which can be found on the cuML notebook examples</p> <p>The following python script is needed to replicate this tutorial: </p> <ul> <li>multigpu_rapids_cuml.py (Can be used as template)</li> </ul> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/GPU/rapids_cuml/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#activate-conda","title":"Activate Conda","text":"<p>This can be done by either installing a conda from scratch or by deploying er prior installation. Please see  \"Using Conda for easy workflow deployment on AAU GPU VMs\" for information.</p> <pre><code># Download and install miniconda (If needed)\ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#create-or-re-load-a-rapids-conda-environment","title":"Create or re-load a RAPIDS Conda environment","text":"<p>look for latest RAPIDS installation at https://docs.rapids.ai/install</p> <pre><code># Create pytorch conda environment if non-exist on the Conda installation\nconda deactivate\nconda create --solver=libmamba -n rapids -c rapidsai -c conda-forge -c nvidia  \\\n    rapids=23.08 python=3.10 cuda-version=11.8\n\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#transfer-files-and-folders-ssh-copy-to-vm","title":"Transfer Files and Folders (SSH-Copy) to VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path\\pytorch_folder\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#run-a-random-forrest-training-on-multiple-gpus","title":"Run a Random Forrest training on multiple GPUs:","text":"<pre><code>python multigpu_rapids_cuml.py\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#track-the-gpu-usage","title":"Track the GPU usage","text":"<pre><code>nvidia-smi -l 5 # Will update every 5 seconds\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A      2312      C   python                           1324MiB |\n|    0   N/A  N/A      2381      C   ...a3/envs/rapids/bin/python     1042MiB |\n|    1   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    1   N/A  N/A      2383      C   ...a3/envs/rapids/bin/python     1042MiB |\n+-----------------------------------------------------------------------------+\nTue Aug 29 11:04:31 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P0    49W /  70W |   2389MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   36C    P0    53W /  70W |   1067MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#transfer-results-and-conda-enviroment-local-machine-ssh-copy","title":"Transfer Results and Conda enviroment local machine (SSH-Copy)","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/","title":"Tensorflow: Train your deep-learning models on AAU GPUs","text":"<p>This tutorial show how to deploy \"Distributed Data Parallel (DDP) using Tensorflow/Keras\" to efficiently train your deep-learning models on the AAU GPUs avalaible through UCloud.</p> <p>See here for a more detailed tutorial on DDP using Tensorflow.</p> <p>This tutorial specifically focuses on Multi-GPU DDP Training with fault tolerance</p> <p>The following python script is needed to replicate this tutorial: </p> <ul> <li>multigpu_torchrun.py (Can be used as template)</li> </ul> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/GPU/tf_ddp/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#activate-conda","title":"Activate Conda","text":"<p>This can be done by either installing a conda from scratch or by deploying er prior installation. Please see  \"Using Conda for easy workflow deployment on AAU GPU VMs\" for information.</p> <pre><code># Download and install miniconda (If needed)\ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#create-or-re-load-a-tensorflowkeras-conda-environment","title":"Create or re-load a Tensorflow/Keras Conda environment","text":"<pre><code># Create pytorch conda environment if non-exist on the Conda installation\nconda deactivate\nconda create --name tensorflow\nconda activate tensorflow\nconda install -c anaconda tensorflow-gpu\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#transfer-files-and-folders-ssh-copy-to-vm","title":"Transfer Files and Folders (SSH-Copy) to VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path\\pytorch_folder\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#run-tensorflow-training-in-ddp-mode","title":"Run Tensorflow training in DDP mode:","text":"<p>In this example a model is trained for 10 epocs using 2 GPUs with a model checkpoint being saved with \"ckpt\" folder for each epoc and the final model being saved as \"final_model.keras\". </p> <p>functions \"get_compiled_model\" and \"get_dataset\" need to be changed to adjust training data and model.</p> <pre><code># Run the model with 10 epcos and 2 GPUs\npython multigpu_tensorflow.py 10 2\n\n# Run the model with 10 epcos and all avaiable GPUs\npython multigpu_tensorflow.py 10\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#track-the-gpu-usage","title":"Track the GPU usage","text":"<pre><code>nvidia-smi -l 5 # Will update every 5 seconds\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A      2312      C   python                           1324MiB |\n|    0   N/A  N/A      2381      C   ...a3/envs/rapids/bin/python     1042MiB |\n|    1   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    1   N/A  N/A      2383      C   ...a3/envs/rapids/bin/python     1042MiB |\n+-----------------------------------------------------------------------------+\nTue Aug 29 11:04:31 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P0    49W /  70W |   2389MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   36C    P0    53W /  70W |   1067MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#transfer-results-and-conda-enviroment-local-machine-ssh-copy","title":"Transfer Results and Conda enviroment local machine (SSH-Copy)","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/SLURM/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"Tutorials/SLURM/#files","title":"Files","text":""},{"location":"Tutorials/SLURM/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"Tutorials/SLURM/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"Tutorials/SLURM/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"Tutorials/SLURM/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"Tutorials/SLURM/Ray/","title":"Ray","text":""},{"location":"Tutorials/SLURM/Ray/#example-using-ray","title":"Example using Ray","text":"<p>In terminal run:</p> <pre><code>python slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Output\n\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"Tutorials/SLURM/Ray/#open-extra-terminal-for-three-nodes","title":"Open extra Terminal for three Nodes","text":""},{"location":"Tutorials/SLURM/Ray/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"Tutorials/SLURM/Ray/#observed-that-the-work-is-disbrubted-across-all-three-nodes","title":"Observed that the work is disbrubted across all three nodes.","text":"<p>This may look different for different backends (e.g. Dask). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"Tutorials/SLURM/Ray/#output-files","title":"Output files","text":""},{"location":"Tutorials/SLURM/Ray/#the-autogenerated-slurm-script-slurmtest_0425-1208sh","title":"The autogenerated SLURM script (SlurmTest_0425-1208.sh)","text":"<pre><code>#!/bin/bash\n# shellcheck disable=SC2206\n# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!\n# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!\n\n#SBATCH --job-name=SlurmTest_0425-1208\n#SBATCH --output=SlurmTest_0425-1208.log\n\n### This script works for any number of nodes, Ray will find and manage all resources\n#SBATCH --nodes=3\n#SBATCH --exclusive\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n##SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE} #De-activated by KGP 230317\n\n# Load modules or your own conda environment here\n# module load pytorch/v1.4.0-gpu\n# conda activate ${CONDA_ENV}\n\n\n# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====\n\necho $SLURM_JOB_NODELIST\n\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\") # Getting the node names\n\nnodes_array=($nodes)\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w \"$node_1\" hostname --ip-address) # making redis-address\n\n# if we detect a space character in the head node IP, we'll\n# convert it to an ipv4 address. This step is optional.\nif [[ \"$ip\" == *\" \"* ]]; then\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$ip\"\nif [[ ${#ADDR[0]} -gt 16 ]]; then\nip=${ADDR[1]}\nelse\nip=${ADDR[0]}\nfi\necho \"IPV6 address detected. We split the IPV4 address as $ip\"\nfi\n\nport=6379\nip_head=$ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"STARTING HEAD at $node_1\"\nsrun --nodes=1 --ntasks=1 -w \"$node_1\" ray start --head --node-ip-address=\"$ip\" --port=$port --block &amp;\nsleep 30\n\n#worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\n#export NB_WORKERS=$((${SLURM_JOB_NUM_NODES-1})) #number of nodes other than the head node\n#echo ${NB_WORKERS}\n\nexport NB_WORKERS=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\necho \"STARTING ${NB_WORKERS} WORKERS\"\nfor ((i = 1; i &lt;= NB_WORKERS; i++)); do\nnode_i=${nodes_array[$i]}\necho \"STARTING WORKER $i at $node_i\"\nsrun --nodes=1 --ntasks=1 -w \"$node_i\" ray start --address \"$ip_head\" --block &amp;\nsleep 5\ndone\n\n# ===== Call your code below =====\necho \"RUNNING CODE: python /work/data/SklearnRay.py\"\npython /work/data/SklearnRay.py\n</code></pre>"},{"location":"Tutorials/SLURM/Ray/#autogenerated-log-file-slurmtest_0425-1208log","title":"Autogenerated log file (SlurmTest_0425-1208.log)","text":"<pre><code>node[0-2]\nIPV6 address detected. We split the IPV4 address as 10.42.47.86\nIP Head: 10.42.47.86:6379\nSTARTING HEAD at node0\n2023-04-25 12:08:40,054 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\nSTARTING 2 WORKERS\nSTARTING WORKER 1 at node1\n2023-04-25 12:08:38,026 INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-04-25 12:08:38,026 INFO scripts.py:710 -- Local node IP: 10.42.47.86\n2023-04-25 12:08:41,222 SUCC scripts.py:747 -- --------------------\n2023-04-25 12:08:41,222 SUCC scripts.py:748 -- Ray runtime started.\n2023-04-25 12:08:41,223 SUCC scripts.py:749 -- --------------------\n2023-04-25 12:08:41,223 INFO scripts.py:751 -- Next steps\n2023-04-25 12:08:41,223 INFO scripts.py:752 -- To connect to this Ray runtime from another node, run\n2023-04-25 12:08:41,223 INFO scripts.py:755 --   ray start --address='10.42.47.86:6379'\n2023-04-25 12:08:41,223 INFO scripts.py:771 -- Alternatively, use the following Python code:\n2023-04-25 12:08:41,223 INFO scripts.py:773 -- import ray\n2023-04-25 12:08:41,223 INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='10.42.47.86')\n2023-04-25 12:08:41,223 INFO scripts.py:790 -- To see the status of the cluster, use\n2023-04-25 12:08:41,223 INFO scripts.py:791 --   ray status\n2023-04-25 12:08:41,223 INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.\n2023-04-25 12:08:41,224 INFO scripts.py:809 -- To terminate the Ray runtime, run\n2023-04-25 12:08:41,224 INFO scripts.py:810 --   ray stop\n2023-04-25 12:08:41,224 INFO scripts.py:891 -- --block\n2023-04-25 12:08:41,224 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:41,224 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nSTARTING WORKER 2 at node2\n2023-04-25 12:08:48,882 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:48,933 I 2244 2244] global_state_accessor.cc:356: This node has an IP address of 10.42.28.36, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:48,859 INFO scripts.py:866 -- Local node IP: 10.42.28.36\n2023-04-25 12:08:48,935 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:48,935 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:48,935 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:48,935 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:48,935 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:48,935 INFO scripts.py:891 -- --block\n2023-04-25 12:08:48,935 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:48,935 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nRUNNING CODE: python /work/data/SklearnRay.py\n2023-04-25 12:08:54,215 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:54,271 I 956 956] global_state_accessor.cc:356: This node has an IP address of 10.42.34.213, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:54,135 INFO scripts.py:866 -- Local node IP: 10.42.34.213\n2023-04-25 12:08:54,274 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:54,275 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:54,275 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:54,275 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:54,275 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:54,275 INFO scripts.py:891 -- --block\n2023-04-25 12:08:54,275 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:54,275 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n2023-04-25 12:09:24,758 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.42.47.86:6379...\n2023-04-25 12:09:24,775 INFO worker.py:1553 -- Connected to Ray cluster.\n2023-04-25 12:09:25,073 WARNING pool.py:604 -- The 'context' argument is not supported using ray. Please refer to the documentation for how to control ray initialization.\nFitting 10 folds for each of 500 candidates, totalling 5000 fits\n209.00055767036974\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nslurmstepd-node2: error: *** STEP 2.3 ON node2 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node0: error: *** STEP 2.1 ON node0 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node1: error: *** STEP 2.2 ON node1 CANCELLED AT 2023-04-25T12:12:54 ***\nsrun: error: node2: task 0: Exited with exit code 1\nsrun: error: node1: task 0: Exited with exit code 1\nsrun: error: node0: task 0: Exited with exit code 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/","title":"UCloud Tutorial: Run Multi-node SLURM Cluster on UCloud","text":"TutorialFiles"},{"location":"Tutorials/SLURM/SLURM/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>In addition to the normal setting fill out the following options (See figure below).</p> <p>In this example launched as cluster consisting of 3 nodes with three folder added to the launch:</p> <ul> <li>\"miniconda3\"  - contains the conda environment I want to deploy across the different nodes.</li> <li>\"SLURM_deployment\" - contains the easy-to-use deployment scripts provided in this tutorial. </li> <li>\"SLURM_scripts\" - contains the user specific script and data to run on the cluster.</li> </ul> <p>In this example Conda is used for package and evironment management. Check here for more information on Conda on UCloud.</p> <p></p>"},{"location":"Tutorials/SLURM/SLURM/#when-the-job-has-started-open-terminal-for-node-1","title":"When the job has started open Terminal for Node 1","text":"<p>Run following commands in the terminal: </p> <pre><code># activate SLURM Cluster if not activated in the step above\ninit_slurm_cluster\n\n# List Avaliable nodes\nsinfo -N -l\n</code></pre> <p>The controller node is always the first node. Called \"node0\" in within SLURM but called \"Node 1\" in the UCloud interface). All additional nodes are named sequentially. For example, a cluster consisting of three full u1-standard nodes is configured as follows:</p> <pre><code>NODELIST   NODES PARTITION     STATE CPUS   S:C:T MEMORY\nnode0         1     CLOUD*     idle   64   1:64:1 385024\nnode1         1     CLOUD*     idle   64   1:64:1 385024\nnode2         1     CLOUD*     idle   64   1:64:1 385024\n</code></pre> <p>But called Node 1, Node 2 and Node 3 in the UCloud interface.</p>"},{"location":"Tutorials/SLURM/SLURM/#acitvate-conda-environment","title":"Acitvate Conda Environment","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which environment is in path (e.g. X = python,R..)\nwhich X # (e.g. X = python,R..)\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/X # (e.g. X = python,R..)\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/#slurm-deployment-scripts","title":"SLURM deployment scripts","text":"<p>The SLURM deployment script (\"slurm-launch.py\") have been adopted from  Ray documentation to support the addition of other python libraries (Dask, ipyparallel) and other languages (e.g. R).</p>"},{"location":"Tutorials/SLURM/SLURM/#slurm-launchpy","title":"slurm-launch.py","text":"<p>\"slurm-launch.py\" auto-generates SLURM scripts and launch. slurm-launch.py uses an underlying template (e.g. \"slurm-template_ray.sh\" or \"slurm-template_dask.sh\") and fills out placeholders given user input.</p> <pre><code># Change path:\ncd /work/SLURM_deployment\n\n# Python with Ray\npython slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Python with Dask\npython slurm-launch.py --script slurm-template_dask.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnDask.py\" --num-nodes 3 --nprocs 8 --nthreads 1\n\n# R with doParallel\npython slurm-launch.py --script slurm-template_R.sh --exp-name SlurmTest --command \"Rscript --vanilla /work/SLURM_scripts/doParallel.r\" --num-nodes 3 --nprocs 8 --nthreads 1 # Example of Output\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/#addditionel-options","title":"Addditionel options","text":"<pre><code>--exp-name          # The experiment name. Will generate {exp-name}_{date}-{time}.sh and {exp-name}_{date}-{time}.log.\n--command           # The command you wish to run. For example: rllib train XXX or python XXX.py.\n--node (-w)         # The specific nodes you wish to use, in the same form as the output of sinfo. Nodes are automatically assigned if not specified.\n--num-nodes (-n)    # The number of nodes you wish to use. Default: 1.\n--partition (-p):   # The partition you wish to use. Default: \u201c\u201d, will use user\u2019s default partition.\n--load-env:         # The command to setup your environment. For example: module load cuda/10.1. Default: \u201c\u201d.\n--nprocs: --nthreads:\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/#open-extra-terminal-for-the-three-nodes","title":"Open extra terminal for the three nodes","text":""},{"location":"Tutorials/SLURM/SLURM/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"Tutorials/SLURM/SLURM/#observed-that-the-work-is-distibuted-across-all-three-nodes","title":"Observed that the work is distibuted across all three nodes.","text":"<p>This may look different for different frameworks (e.g. Ray, Dask, R). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"Tutorials/SLURM/SLURM/#files","title":"Files","text":""},{"location":"Tutorials/SLURM/SLURM/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"Tutorials/SLURM/SLURM/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"Tutorials/SLURM/SLURM/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"Tutorials/SLURM/SLURM/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"Tutorials/SSH/Rsync/","title":"UCloud Tutorial: Transfer large data to UCloud using Rsync","text":"<p>Rsync, short for \"remote synchronization,\" is a powerful and widely used file synchronization and transfer utility. It enables efficient copying and updating of files between different locations, whether they are on the same system or across a network. Rsync is particularly valuable for managing large data sets or performing incremental backups.</p> <p>What sets Rsync apart is its ability to synchronize files by transferring only the differences between the source and destination files. This delta transfer mechanism greatly reduces the amount of data that needs to be transmitted, making Rsync highly efficient, even for large files or slow network connections.</p> <p>Rsync also offers several advanced features, such as compression, encryption, and the ability to preserve various file attributes, such as permissions, timestamps, and symbolic links. It supports both local file copying and remote transfers via SSH, allowing secure synchronization between different systems.</p> <p>With its flexibility, speed, and efficient use of network resources, Rsync has become a go-to tool for tasks like backup and mirroring, remote file distribution, and content deployment. It has a command-line interface, making it scriptable and suitable for both one-time transfers and automated, scheduled tasks.</p> <p>UCloud documentation on Rsync</p>"},{"location":"Tutorials/SSH/Rsync/#installing-ubuntu-on-local-machine-for-windows","title":"Installing Ubuntu on local machine (For Windows)","text":"<p>Rsync is not natively available for Windows. However, you can install Rsync on Windows using a third-party implementation such as Cygwin or DeltaCopy. Alternatively, you can install Ubuntu on Windows which then comes whic includes Rsync.</p> <p>In this guide we will use Rsync through Ubuntu. For more infomation and video tutorials can be found here.</p>"},{"location":"Tutorials/SSH/Rsync/#create-a-ssh-key-within-your-ubuntu-environment","title":"Create a SSH-key within your Ubuntu environment","text":"<p>Despite already having a shh-key (in your windows environment) the easiest will be to create a new SSH-key within your Ubuntu environment. Open a terminal and follow the few steps below.</p> <p>More information on how to generate a SSH key can be found here</p> <pre><code># Activate Ubuntu \nwsl\n\n# For linux only \nsudo apt install openssh-client\n\n# Create key\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/SSH/Rsync/#copy-public-ssh-key","title":"Copy public SSH-key","text":"<pre><code># Open Public Key\nvim /home/user/.ssh/id_rsa.pub\n\n# highlight public key with mouse and copy using \"ctrl+c\"\n</code></pre>"},{"location":"Tutorials/SSH/Rsync/#add-public-ssh-key-to-ucloud","title":"Add public SSH key to UCloud","text":""},{"location":"Tutorials/SSH/Rsync/#step-1-on-ucloud-go-to-resources-shh-keys-create-ssh-key","title":"Step 1: On UCloud go to \"Resources -&gt; SHH-Keys -&gt; Create SSH key\"","text":""},{"location":"Tutorials/SSH/Rsync/#step-2-paste-pulic-key-give-a-meaningful-name-and-press-add-ssh-key","title":"Step 2: Paste pulic key, give a meaningful name and press \"Add SSH key\".","text":"<p>More information can be found in the UCloud documentation.</p>"},{"location":"Tutorials/SSH/Rsync/#start-rsync-job","title":"Start Rsync Job","text":""},{"location":"Tutorials/SSH/Rsync/#step-1-start-rsync-job-by-filling-out-the-necessary-fields","title":"Step 1: Start Rsync Job by filling out the necessary fields","text":""},{"location":"Tutorials/SSH/Rsync/#step-2-when-job-ready-please-locate-the-ssh-port-which-is-randomly-generated-in-the-cas-below-the-shh-port-is-2167","title":"Step 2: When job ready please locate the SSH port which is randomly generated. In the cas below the SHH port is 2167.","text":""},{"location":"Tutorials/SSH/Rsync/#connect-from-local-machine-using-ssh","title":"Connect from local machine using SSH.","text":"<p>Open Terminal and follow steps below.</p> <pre><code># Activate Ubuntu (for Windows)\nwsl\n\n# SHH connect using the command marked with in the figure above.\nssh ucloud@ssh.cloud.sdu.dk -p 2167\n</code></pre> <p>If sucessfull you should get the output shown in the figure below.</p> <p></p>"},{"location":"Tutorials/SSH/Rsync/#transfer-data-using-rsync","title":"Transfer data using Rsync","text":"<pre><code># Navigate to path contain the folder of files to transfer - Alternatively you can open terminal directly in the right directory to skip step below.\ncd \"path/of/folders-or-files\" # Activate Ubuntu (for Windows)\nwsl\n\n# SSH transfer \"myfolder\" to /work directory on UCloud \n\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p 2167\" ./myfolder/ ucloud@ssh.cloud.sdu.dk:/work/myfolder </code></pre>"},{"location":"Tutorials/SSH/VSCode/","title":"UCloud Tutorial: SSH Connection to UCloud using Terminal or VSCode","text":"<p>Direct SSH Connection is available for a range of different UCloud applications: </p> <ul> <li>Terminal</li> <li>Coder</li> <li>RStudio</li> <li>JupyterLab</li> <li>Ubuntu Xfce - virtual desktop environment</li> <li>AlmaLinux Xfce - virtual desktop environment</li> </ul> <p>This tutorial provided a step-by-step guide on how to connect the UCloud application directly to a Terminal or VSCode application on your local PC. </p> <p>It will also be shown how to setup a conda environment and how to run a jupyter notebook through VSCode.</p> <p>Prerequisite reading:</p> <ul> <li>How to Generate SSH key</li> </ul>"},{"location":"Tutorials/SSH/VSCode/#add-public-ssh-key-to-ucloud","title":"Add public SSH key to UCloud","text":""},{"location":"Tutorials/SSH/VSCode/#step-1-on-ucloud-go-to-resources-shh-keys-create-ssh-key","title":"Step 1: On UCloud go to \"Resources -&gt; SHH-Keys -&gt; Create SSH key\"","text":""},{"location":"Tutorials/SSH/VSCode/#step-2-paste-pulic-key-give-a-meaningful-name-and-press-add-ssh-key","title":"Step 2: Paste pulic key, give a meaningful name and press \"Add SSH key\".","text":"<p>More information can be found in the UCloud documentation.</p>"},{"location":"Tutorials/SSH/VSCode/#start-ucloud-job","title":"Start UCloud Job","text":""},{"location":"Tutorials/SSH/VSCode/#step-1-start-ucloud-job-by-filling-out-the-necessary-fields-remember-to-check-the-enable-ssh-server-checkbox","title":"Step 1: Start UCloud Job by filling out the necessary fields. Remember to check the \"Enable SSH server\" checkbox\"","text":""},{"location":"Tutorials/SSH/VSCode/#step-2-when-job-ready-please-locate-and-copy-the-ssh-address","title":"Step 2: When job ready please locate and copy the SSH address","text":""},{"location":"Tutorials/SSH/VSCode/#connect-the-using-a-local-terminal-app","title":"Connect the using a Local Terminal App","text":"<p>Paste the SSH address into a terminal, press enter and follow a few intuitive steps.</p> <p></p>"},{"location":"Tutorials/SSH/VSCode/#connect-using-visual-studio-code-vscode","title":"Connect using Visual Studio Code (VScode)","text":""},{"location":"Tutorials/SSH/VSCode/#open-visual-studio-code-vscode","title":"Open Visual Studio Code (VScode)","text":""},{"location":"Tutorials/SSH/VSCode/#install-remote-ssh-extension","title":"Install \"Remote - SSH\" extension","text":""},{"location":"Tutorials/SSH/VSCode/#access-remote-ucloud-job","title":"Access remote UCloud Job","text":"<ul> <li>Press \"Ctrl + shift + p\" to open Command Palette</li> <li>Search and press \"Remote-SSH:Connect to Host...\"</li> <li>Press \"Add New SSH Host...\"</li> </ul>"},{"location":"Tutorials/SSH/VSCode/#paste-ucloud-job-ssh-address-and-press-enter","title":"Paste UCloud Job SSH address and press Enter","text":"<p>Follow a few intuitive steps.</p>"},{"location":"Tutorials/SSH/VSCode/#navigate-to-work-folder-on-ucloud","title":"Navigate to /work folder on UCloud","text":""},{"location":"Tutorials/SSH/VSCode/#run-a-jupyter-notebook-through-vscode","title":"Run a Jupyter notebook (Through VSCode)","text":""},{"location":"Tutorials/SSH/VSCode/#install-vscode-extension-python-and-jupyter-on-remote-machine","title":"Install VScode Extension \"Python\" and \"Jupyter\" on remote machine","text":""},{"location":"Tutorials/SSH/VSCode/#activate-a-conda-environment-through-vscode","title":"Activate a Conda Environment (Through VSCode)","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n\n# Activate environment:\nconda activate myenv\n\n# Install ipykernel if not installed:\n\nconda install ipykernel\n\n# \npython -m ipykernel install --user --name myenv --display-name \"myenv\"\n</code></pre>"},{"location":"Tutorials/SSH/shh_create/","title":"How to Generate a SSH key","text":"<p>In order to access the VM it is necessary to create Secure Shell Protocol (SSH) keys more specifically a public key (shareable part) and a private key (kept safe locally).</p>"},{"location":"Tutorials/SSH/shh_create/#on-windows-linux-systems","title":"On Windows &amp; Linux Systems","text":"<p>To generate your public key, find and open terminal and type: </p> <pre><code># For linux only\nsudo apt install openssh-client\n\n\n# For both Windows &amp; Linux\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in C:\\Users\\user/.ssh/id_rsa.\nYour public key has been saved in C:\\Users\\user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/SSH/shh_create/#manually-locate-open-and-copy-our-public-key-from-id_rsapub-file","title":"Manually locate, open and copy our public key from id_rsa.pub file.","text":""},{"location":"Tutorials/SSH/shh_create/#on-windows","title":"On Windows","text":"<p>the file might be here: \"C:\\Users\\write_your_user_name.ssh\"</p>"},{"location":"Tutorials/SSH/shh_create/#on-linux","title":"On Linux","text":"<p>The generated SSH key will be by stored under ~/.ssh/id_rsa.pub by default.</p> <p>More information can be found at https://genome.au.dk/docs/getting-started/#public-key-authentication </p>"},{"location":"Tutorials/SSH/ssh_connect/","title":"Ssh connect","text":""},{"location":"Tutorials/SSH/ssh_connect/#ssh-to-server-through-local-terminal","title":"SSH to Server through local Terminal","text":"<p>Add public SSH key while starting a VM job</p> <p></p> <p>Identify VM IP when UCloud job is ready.</p> <p></p>"},{"location":"Tutorials/SSH/ssh_connect/#from-local-terminal-connect-to-vm-by","title":"From Local Terminal connect to VM by:","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/SSH/ssh_connect/#transfer-files-and-folders-ssh-copy","title":"Transfer Files and Folders (SSH-Copy)","text":""},{"location":"Tutorials/SSH/ssh_connect/#to-vm","title":"To VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path-to-folder-or-files\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/SSH/ssh_connect/#from-vm","title":"From VM","text":"<p>Open a second terminal (1st terminal is connected to the VM)</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/STATA/batch/","title":"UCloud Tutorial: Run Stata in Batch Mode on UCloud","text":"<p>This is an approach to adress the UCloud capacity issues. </p> <p>UCloud batch processing apps are scheduled to run as resources permit without end user interaction. It allows </p> <p>Prerequisite reading:</p> <ul> <li>Install Stata on UCloud</li> </ul>"},{"location":"Tutorials/STATA/batch/#run-stata-scrip-in-batch-mode-n-a-new-terminal-job","title":"Run Stata scrip in batch mode n a new terminal job","text":"<p>Add the \"stata17\" and other relevant folder to the job:</p> <p></p> <p>Add a bash script(.sh) under \"Batch processing\" as one of the \"Optional Parameters\":</p> <p> </p> <p>Below shown bash script can be downloaded from here. Use this as a template or create your own bash script.</p> <p>More information on how to run Stata in batch mode can be found here: https://www.stata.com/support/faqs/unix/batch-mode/</p> <pre><code>#!/bin/bash\n\n\n# Installing dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata17 on UNIX path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Run stata in Batch mode\nstata -b do filename &amp; # USER SHOULD CHANGE THIS LINE (SEE LINK Above)\n</code></pre>"},{"location":"Tutorials/STATA/install/","title":"Install Stata on UCloud","text":"<p>This is a guide on how to install Stata on UCloud.</p>"},{"location":"Tutorials/STATA/install/#get-stata-license-and-installation-file-cbs-users","title":"Get Stata license and Installation file (CBS Users)","text":"<p>Follow the instructions to get a Stata license at CBS https://studentcbs.sharepoint.com/sites/ITandCampus/SitePages/en/Free-software.aspx</p> <p>You will recieve an email with license and installation information (see image below).</p> <p></p> <p>Download the installation file (Stata17Linux64.tar) and upload this to your UCloud directory.</p> <p></p>"},{"location":"Tutorials/STATA/install/#installing-stata-on-ucloud","title":"Installing Stata on UCloud","text":""},{"location":"Tutorials/STATA/install/#launch-a-terminal-app-ucloud-job-and-include-the-stata-installation-file-stata17linux64tar","title":"Launch a \"Terminal App\" UCloud Job and include the stata installation file (Stata17Linux64.tar)","text":"<p>Run following commands in the terminal: </p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Unzip installation file to temp folder\nsudo -s\nmkdir /tmp/statafiles\ncd /tmp/statafiles\ntar -zxf /work/install/Stata17Linux64.tar.gz\n\n# Install Stata on in \"/work/stata17\". Say yes when asked during installtion\nmkdir /work/stata17 cd /work/stata17 /tmp/statafiles/install\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Initialize Stata\nsudo /work/stata17/stinit\n\n# Follow instructions and add \"Serial number\", \"Code\" and \"Authorization\" from the Stata license mail\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata # or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"Tutorials/STATA/install/#end-job-and-copy-the-stata17-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-directory","title":"End job and copy the \u201cstata17\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud directory.","text":""},{"location":"Tutorials/STATA/install/#activate-stata-installation-in-a-new-terminal-job","title":"Activate Stata installation in a new terminal job","text":"<p>Add the stata17 folder to the job</p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata # or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"Tutorials/STATA/jupyter/","title":"Run Stata in jupyter-notebooks","text":"<p>This is a guide shows how to setup \"pystata\" in order to run Stata in Python using jupyter notebooks.</p> <p>For more in depth decumnetation see here</p> <p>Prerequisite reading:</p> <ul> <li>Install Stata on UCloud</li> </ul>"},{"location":"Tutorials/STATA/jupyter/#start-a-jupyter-job-in-ucloud","title":"Start a Jupyter Job in UCloud","text":"<p>Add the stata17 folder to the job</p> <p></p>"},{"location":"Tutorials/STATA/jupyter/#activate-stata-and-install-stata-setup","title":"Activate Stata and install stata-setup","text":"<p>Open the Terminal and run the code snippets below.</p> <p></p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Example Output\n/work/stata17/stata # use later\n\n# Install stata-setup using pip\npip install stata-setup\n\n# Install pystata using pip\npip install pystata\n</code></pre>"},{"location":"Tutorials/STATA/jupyter/#run-stata-in-a-jupyter-notebook","title":"Run Stata in a Jupyter notebook","text":""},{"location":"Tutorials/STATA/jupyter/#start-jupyter-interface","title":"Start Jupyter interface","text":""},{"location":"Tutorials/STATA/jupyter/#open-a-new-python-notebook","title":"Open a new python notebook","text":""},{"location":"Tutorials/STATA/jupyter/#configure-the-stata-installation","title":"Configure the stata installation","text":"<pre><code>import stata_setup\n\nstata_setup.config(\"/work/stata17\", \"se\")\n\n# Output\n\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE\u2014Standard Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nCBS Account\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n2. Maximum number of variables is set to 5,000; see help set_maxvar.\n</code></pre>"},{"location":"Tutorials/STATA/jupyter/#run-your-code-using-the-stata-magic-stata-the-configure-the-stata-installation","title":"Run your code using the stata magic (%%stata) the Configure the stata installation","text":"<p>\"%%stata\" - cell magic is used to execute Stata code within a cell.</p> <p>\"%stata\" - line magic provides users a quick way to execute a single-line Stata command.</p> <p>Find more information on the stata magic here.</p> <pre><code>%%stata\n\nsysuse auto, clear\n\nsummarize mpg\n\n# Output\n. . sysuse auto, clear\n(1978 automobile data)\n\n. . summarize mpg\n\nVariable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmpg |         74     21.2973    5.785503         12         41\n\n. </code></pre> <p></p>"},{"location":"Tutorials/Sync/Rsync/","title":"UCloud Tutorial: Transfer large data to UCloud using Rsync","text":"<p>Rsync, short for \"remote synchronization,\" is a powerful and widely used file synchronization and transfer utility. It enables efficient copying and updating of files between different locations, whether they are on the same system or across a network. Rsync is particularly valuable for managing large data sets or performing incremental backups.</p> <p>What sets Rsync apart is its ability to synchronize files by transferring only the differences between the source and destination files. This delta transfer mechanism greatly reduces the amount of data that needs to be transmitted, making Rsync highly efficient, even for large files or slow network connections.</p> <p>Rsync also offers several advanced features, such as compression, encryption, and the ability to preserve various file attributes, such as permissions, timestamps, and symbolic links. It supports both local file copying and remote transfers via SSH, allowing secure synchronization between different systems.</p> <p>With its flexibility, speed, and efficient use of network resources, Rsync has become a go-to tool for tasks like backup and mirroring, remote file distribution, and content deployment. It has a command-line interface, making it scriptable and suitable for both one-time transfers and automated, scheduled tasks.</p> <p>UCloud documentation on Rsync</p>"},{"location":"Tutorials/Sync/Rsync/#installing-ubuntu-on-local-machine-for-windows","title":"Installing Ubuntu on local machine (For Windows)","text":"<p>Rsync is not natively available for Windows. However, you can install Rsync on Windows using a third-party implementation such as Cygwin or DeltaCopy. Alternatively, you can install Ubuntu on Windows which then comes whic includes Rsync.</p> <p>In this guide we will use Rsync through Ubuntu. For more infomation and video tutorials can be found here.</p>"},{"location":"Tutorials/Sync/Rsync/#create-a-ssh-key-within-your-ubuntu-environment","title":"Create a SSH-key within your Ubuntu environment","text":"<p>Despite already having a shh-key (in your windows environment) the easiest will be to create a new SSH-key within your Ubuntu environment. Open a terminal and follow the few steps below.</p> <p>More information on how to generate a SSH key can be found here</p> <pre><code># Activate Ubuntu \nwsl\n\n# For linux only \nsudo apt install openssh-client\n\n# Create key\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#copy-public-ssh-key","title":"Copy public SSH-key","text":"<pre><code># Open Public Key\nvim /home/user/.ssh/id_rsa.pub\n\n# highlight public key with mouse and copy using \"ctrl+c\"\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#add-public-ssh-key-to-ucloud","title":"Add public SSH key to UCloud","text":""},{"location":"Tutorials/Sync/Rsync/#step-1-on-ucloud-go-to-resources-shh-keys-create-ssh-key","title":"Step 1: On UCloud go to \"Resources -&gt; SHH-Keys -&gt; Create SSH key\"","text":""},{"location":"Tutorials/Sync/Rsync/#step-2-paste-pulic-key-give-a-meaningful-name-and-press-add-ssh-key","title":"Step 2: Paste pulic key, give a meaningful name and press \"Add SSH key\".","text":"<p>More information can be found in the UCloud documentation.</p>"},{"location":"Tutorials/Sync/Rsync/#start-rsync-job","title":"Start Rsync Job","text":""},{"location":"Tutorials/Sync/Rsync/#step-1-start-rsync-job-by-filling-out-the-necessary-fields","title":"Step 1: Start Rsync Job by filling out the necessary fields","text":""},{"location":"Tutorials/Sync/Rsync/#step-2-when-job-ready-please-locate-the-ssh-port-which-is-randomly-generated-in-the-cas-below-the-shh-port-is-2167","title":"Step 2: When job ready please locate the SSH port which is randomly generated. In the cas below the SHH port is 2167.","text":""},{"location":"Tutorials/Sync/Rsync/#connect-from-local-machine-using-ssh","title":"Connect from local machine using SSH.","text":"<p>Open Terminal and follow steps below.</p> <pre><code># Activate Ubuntu (for Windows)\nwsl\n\n# SHH connect using the command marked with in the figure above.\nssh ucloud@ssh.cloud.sdu.dk -p 2167\n</code></pre> <p>If sucessfull you should get the output shown in the figure below.</p> <p></p>"},{"location":"Tutorials/Sync/Rsync/#transfer-data-using-rsync","title":"Transfer data using Rsync","text":"<pre><code># Navigate to path contain the folder of files to transfer - Alternatively you can open terminal directly in the right directory to skip step below.\ncd \"path/of/folders-or-files\" # Activate Ubuntu (for Windows)\nwsl\n\n# SSH transfer \"myfolder\" to /work directory on UCloud \n\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p 2167\" ./myfolder/ ucloud@ssh.cloud.sdu.dk:/work/myfolder </code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/","title":"DieC large memory HPC/TYPE 3 (Hippo): Use Conda for easy management of Python and R environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Type3/Conda_Jupyter/#install-conda-on-type-3","title":"Install Conda on Type 3","text":"<pre><code># Download miniconda \ncurl -s -L -o /miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash miniconda_installer.sh -b -f -p miniconda3\n</code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code># Set miniconda3 to path\nexport PATH=\"$HOME/miniconda3/bin:$PATH\"\n\n# activate Conda\nsource $HOME/miniconda3/bin/activate\n\n# Initiate Conda\nconda init &amp;&amp; bash -i\n</code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/#install-python-or-r-environments-using-conda","title":"Install Python or R environments using Conda","text":"<pre><code># Python\nconda create -n myenv_py python\nconda activate myenv_py\nconda install ipykernel\nipython kernel install --name myenv_py --user # Make python available to JupyterLab\n\n# R\nconda create --solver=libmamba -n myenv_R -y -c conda-forge r-base=4.2.1 #\nconda activate myenv_R\nconda install -c conda-forge r-irkernel\nconda install jupyterlab\nR -e \"IRkernel::installspec(name = 'myenv_R', displayname = 'myenv_R')\" # Make R available to JupyterLab\n</code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/#start-jupyter-interface","title":"Start Jupyter interface","text":""},{"location":"Tutorials/Type3/Conda_Jupyter/#add-token-to-open-jupyter","title":"Add token to open jupyter","text":""},{"location":"Tutorials/Type3/Conda_Jupyter/#now-the-conda-environments-are-available-as-a-jupyter-kernel","title":"Now the conda environments are available as a Jupyter Kernel","text":""},{"location":"Tutorials/Type3/Stata/","title":"Install Stata on DieC large memory HPC/TYPE 3 (Hippo)","text":"<p>This is a guide on how to install Stata on DieC large memory HPC/TYPE 3 (Hippo).</p> <p>Prerequisite reading:</p> <ul> <li>Using Conda for easy management of Python and R environments</li> </ul>"},{"location":"Tutorials/Type3/Stata/#get-stata-license-and-installation-file-cbs-users","title":"Get Stata license and Installation file (CBS Users)","text":"<p>Follow the instructions to get a Stata license at CBS https://studentcbs.sharepoint.com/sites/ITandCampus/SitePages/en/Free-software.aspx</p> <p>You will recieve an email with license and installation information (see image below).</p> <p></p> <p>Download the installation file (Stata17Linux64.tar) and upload this to your UCloud directory.</p> <p></p>"},{"location":"Tutorials/Type3/Stata/#installing-stata-on-type-3","title":"Installing Stata on Type 3","text":""},{"location":"Tutorials/Type3/Stata/#launch-a-terminal-app-ucloud-job-and-include-the-stata-installation-file-stata17linux64tar","title":"Launch a \"Terminal App\" UCloud Job and include the stata installation file (Stata17Linux64.tar)","text":"<p>Run following commands in the terminal: </p> <pre><code># Unzip installation file to temp folder\nmkdir /home/user/statafiles\ncd /home/user/statafiles\ntar -zxf /home/user/Stata17Linux64.tar.gz\n\n# Install Stata on in \"/home/user/stata17\". Say yes when asked during installtion\nmkdir /home/user/stata17 cd /home/user/stata17\n\n/home/user/statafiles/install\n\n# Set stata to Unix path\nexport PATH=\"/home/user/stata17:$PATH\"\n\n# Initialize Stata\n/home/user/stata17/stinit\n\n# Follow instructions and add \"Serial number\", \"Code\" and \"Authorization\" from the Stata license mail\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata\n# or\nstata-se\n# or\nstata-mp\n\n# Get following dependency error: \nstata: error while loading shared libraries: libncurses.so.5: cannot open shared object file: No such file or directory\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#install-dependencies-using-easybuild","title":"Install dependencies using Easybuild","text":"<pre><code>eb ncurses-5.9.eb -r\n\nmodule load ncurses/5.9\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#run-stata","title":"Run Stata","text":"<pre><code># Run stata\nstata\n# or\nstata-se\n# or\nstata-mp\n\n# Output\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       BE\u2014Basic Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nType 3\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n\n.\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#stata17-and-easybuild-will-not-be-placed-on-the-hippo-home-folder","title":"\u201cstata17\u201d and \"easybuild\" will not be placed on the Hippo Home folder","text":""},{"location":"Tutorials/Type3/Stata/#activate-stata-on-a-new-type-3-job","title":"Activate Stata on a new Type 3 Job","text":"<p>Add the stata17 folder to the job</p> <p></p> <pre><code># Set stata to Unix path\nexport PATH=\"/home/user/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Load dependies\nmodule load ncurses/5.9\n\n# Run stata\nstata\n# or\nstata-se\n# or\nstata-mp\n\n# Output\n\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       BE\u2014Basic Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nType 3\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n\n.\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#create-a-conda-stata-environment","title":"Create a Conda Stata environment","text":"<p>Assumes that miniconda3 has been installed. For more information on how to install conda on Type 3 see here.</p> <pre><code># Create conda environment\nconda create -n myenv_stata python\nconda activate myenv_stata\nconda install ipykernel\npip install stata-setup\npip install pystata\nipython kernel install --name myenv_stata --user # Make python available to JupyterLab\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#run-stata-in-a-jupyter-notebook","title":"Run Stata in a Jupyter notebook","text":""},{"location":"Tutorials/Type3/Stata/#start-jupyter-interface","title":"Start Jupyter interface","text":""},{"location":"Tutorials/Type3/Stata/#add-token-to-open-jupyter","title":"Add token to open jupyter","text":""},{"location":"Tutorials/Type3/Stata/#open-a-new-python-notebook","title":"Open a new python notebook","text":""},{"location":"Tutorials/Type3/Stata/#configure-the-stata-installation","title":"Configure the stata installation","text":"<pre><code>import stata_setup\n\nstata_setup.config(\"/work/stata17\", \"se\")\n\n# Output\n\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE\u2014Standard Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nCBS Account\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n2. Maximum number of variables is set to 5,000; see help set_maxvar.\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#run-your-code-using-the-stata-magic-stata-the-configure-the-stata-installation","title":"Run your code using the stata magic (%%stata) the Configure the stata installation","text":"<p>\"%%stata\" - cell magic is used to execute Stata code within a cell.</p> <p>\"%stata\" - line magic provides users a quick way to execute a single-line Stata command.</p> <p>Find more information on the stata magic here.</p> <pre><code>%%stata\n\nsysuse auto, clear\n\nsummarize mpg\n\n# Output\n. . sysuse auto, clear\n(1978 automobile data)\n\n. . summarize mpg\n\nVariable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmpg |         74     21.2973    5.785503         12         41\n</code></pre> <p></p>"},{"location":"Tutorials/VMs/","title":"Virtual Machines on UCloud","text":"<p>How to Generate SSH key</p> <p>Acessing VM using SSH</p>"},{"location":"Tutorials/VMs/condaVM/","title":"Conda: for easy workflow deployment on AAU GPU VMs","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following tutorial provides step-by-step guides on how to install and use Conda for R and Python on the AAU GPU VMs available on UCloud.</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a new AAU GPU VM.</p> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> </ul>"},{"location":"Tutorials/VMs/condaVM/#initial-installation-of-conda-on-a-aau-vm-job","title":"Initial installation of Conda on a AAU VM job","text":""},{"location":"Tutorials/VMs/condaVM/#connect-to-vm-using-ssh","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#download-and-install-conda","title":"Download and Install Conda","text":"<pre><code># Download miniconda \ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # Install miniconda\nbash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#create-conda-environment-and-test-gpu-configuration","title":"Create conda environment and test GPU configuration","text":"<pre><code># Create conda environment \nconda deactivate\nconda create --name my_env python\nconda activate my_env\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit cudnn\n\n# Set pre-installed conda libraries to path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#gpu-conda-environment-is-ready-to-use","title":"GPU conda environment is ready to use","text":""},{"location":"Tutorials/VMs/condaVM/#compress-conda-installation-to-targz-file","title":"Compress Conda installation to tar.gz file","text":"<pre><code>tar -czvf /home/ucloud/miniconda3.tar.gz /home/ucloud/miniconda3\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-miniconda3targz-to-local-pc-using-ssh-copy","title":"Transfer \"miniconda3.tar.gz\" to local PC using SSH-Copy","text":"<p>Open a 2nd instance of a terminal app on local machine</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/miniconda3 \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-conda-install-to-a-new-aau-vm-job","title":"Transfer Conda install to a new AAU VM job","text":""},{"location":"Tutorials/VMs/condaVM/#connect-to-vm-using-ssh_1","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#update-vm_1","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-headless-460 nvidia-utils-460 -y\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-miniconda3targz-from-local-pc-to-vm-using-ssh-copy","title":"Transfer \"miniconda3.tar.gz\" from local PC to VM using SSH-Copy","text":"<p>Open a 2nd instance of a terminal app on local machine</p> <pre><code>scp -r \"C:\\path-to-folder\\miniconda.tar.gz ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#unzip-targz","title":"Unzip tar.gz","text":"<p>Move back to the terminal app connected to VM and run following command:</p> <pre><code>tar -xzf miniconda.tar.gz\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#set-conda-on-path-and-initialise","title":"Set Conda on path and initialise","text":"<pre><code># Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# init conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#re-connect-to-vm-using-ssh_1","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#check-nvidia-driver-configuration_1","title":"Check nvidia driver configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output:\nMon Aug  7 09:41:34 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   51C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#activate-conda-environment-and-test-gpu-configuration","title":"Activate conda environment and test GPU configuration","text":"<pre><code># Create conda environment \nconda deactivate\nconda create --name my_env python\nconda activate my_env\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit cudnn\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#gpu-conda-environment-is-ready-to-use_1","title":"GPU conda environment is ready to use","text":""},{"location":"Tutorials/VMs/connectVM/","title":"connectVM","text":""},{"location":"Tutorials/VMs/connectVM/#ssh-to-server-through-local-terminal","title":"SSH to Server through local Terminal","text":"<p>Add public SSH key while starting a VM job</p> <p></p> <p>Identify VM IP when UCloud job is ready.</p> <p></p>"},{"location":"Tutorials/VMs/connectVM/#from-local-terminal-connect-to-vm-by","title":"From Local Terminal connect to VM by:","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#transfer-files-and-folders-ssh-copy","title":"Transfer Files and Folders (SSH-Copy)","text":""},{"location":"Tutorials/VMs/connectVM/#to-vm","title":"To VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path-to-folder-or-files\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#from-vm","title":"From VM","text":"<p>Open a second terminal (1st terminal is connected to the VM)</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/","title":"Run Python and R jupyter notebooks on AAU VMs","text":"<p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/VMs/jupyterVM/#connect-to-vm-using-ssh","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#install-or-activate-conda-installation","title":"Install or activate Conda installation","text":"<p>See \"Conda: for easy workflow deployment on AAU GPU VMs\" for more information.</p>"},{"location":"Tutorials/VMs/jupyterVM/#install-andor-activate-existing-python-or-r-environment-using-conda","title":"Install and/or activate existing Python or R Environment using Conda","text":"<pre><code># Python \nconda deactivate\nconda create --name myenv_python python\nconda activate myenv_python\nconda install ipykernel\nconda install nb_conda_kernels\n\n# R \nconda deactivate\nconda create --solver=libmamba -n myenv_R -y -c conda-forge r-base\nconda activate myenv_R\nconda install -c conda-forge r-irkernel\nconda install nb_conda_kernels\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit cudnn\n\n# Set pre-installed conda libraries to path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#check-jupyter-installtion-and-get-config-directory","title":"Check jupyter installtion and get config-directory","text":"<pre><code>which jupyter\n\n# Example Output:\n/home/ucloud/miniconda3/envs/my_env/bin/jupyter\n\n# Get config-directory\njupyter --config-dir\n\n# Example Output:\n/home/ucloud/.jupyter/\n\n# Create folder if does not exist\nmkdir -p /home/ucloud/.jupyter/\n\n# Create jupyter_config.json  in config-dir\necho '{\"CondaKernelSpecManager\": {\"kernelspec_path\": \"--user\"}}' &gt; /home/ucloud/.jupyter/jupyter_config.json\n\n# Check content of jupyter_config.json\n\ncat /home/ucloud/.jupyter/jupyter_config.json\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#install-nb_conda_kernels","title":"Install nb_conda_kernels","text":"<p>https://github.com/Anaconda-Platform/nb_conda_kernels#installation</p> <pre><code># Export all existing conda environment with ipykernel or r-irkernel installed\npython -m nb_conda_kernels list\n\n# Example Output: \n[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n[ListKernelSpecs] Removing existing kernelspec in /home/ucloud/.local/share/jupyter/kernels/conda-env-jupyter-py\n[ListKernelSpecs] Installed kernelspec conda-env-jupyter-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-jupyter-py\n[ListKernelSpecs] Installed kernelspec conda-env-my_env-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-my_env-py\n[ListKernelSpecs] Removing existing kernelspec in /home/ucloud/.local/share/jupyter/kernels/conda-env-myenv-py\n[ListKernelSpecs] Installed kernelspec conda-env-myenv-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-myenv-py\n[ListKernelSpecs] Removing existing kernelspec in /home/ucloud/.local/share/jupyter/kernels/conda-env-rapids-py\n[ListKernelSpecs] Installed kernelspec conda-env-rapids-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-rapids-py\n[ListKernelSpecs] [nb_conda_kernels] enabled, 4 kernels found\nAvailable kernels:\nconda-env-jupyter-py    /home/ucloud/miniconda3/envs/jupyter/share/jupyter/kernels/python3\nconda-env-myenv-py      /home/ucloud/miniconda3/envs/myenv/share/jupyter/kernels/python3\nconda-env-rapids-py     /home/ucloud/miniconda3/envs/rapids/share/jupyter/kernels/python3\nconda-env-my_env-py     /home/ucloud/miniconda3/envs/my_env/share/jupyter/kernels/python3\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#check-that-the-conda-environment-kernels-are-discovered-by-jupyter","title":"Check that the conda environment kernels are discovered by jupyter:","text":"<pre><code>jupyter kernelspec list\n\n# Example output:\n[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\nAvailable kernels:\npython3                 /home/ucloud/miniconda3/envs/my_env/share/jupyter/kernels/python3\nconda-env-jupyter-py    /home/ucloud/.local/share/jupyter/kernels/conda-env-jupyter-py\nconda-env-my_env-py     /home/ucloud/.local/share/jupyter/kernels/conda-env-my_env-py\nconda-env-myenv-py      /home/ucloud/.local/share/jupyter/kernels/conda-env-myenv-py\nconda-env-rapids-py     /home/ucloud/.local/share/jupyter/kernels/conda-env-rapids-py\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#start-jupyter-notebook-from-remote-server","title":"Start Jupyter Notebook from remote server","text":"<pre><code>jupyter notebook --no-browser --port=8080 # Change the port number if multiple jupyter notebook are started within the same session\n\n# Output\n\n[I 10:26:32.873 NotebookApp] Serving notebooks from local directory: /home/ucloud\n[I 10:26:32.873 NotebookApp] The Jupyter Notebook is running at:\n[I 10:26:32.873 NotebookApp] http://localhost:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n[I 10:26:32.873 NotebookApp]  or http://127.0.0.1:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n[I 10:26:32.873 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:26:32.899 NotebookApp]\n\nTo access the notebook, open this file in a browser:\nfile:///home/ucloud/.local/share/jupyter/runtime/nbserver-3074-open.html\nOr copy and paste one of these URLs:\nhttp://localhost:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\nor http://127.0.0.1:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#ssh-connect-to-vm-using-a-new-terminal-app-on-local-machine","title":"SSH connect to VM using a new terminal app on local machine","text":"<p>Open a 2nd instance of Terminal on Local machine</p> <pre><code>ssh -L 8080:localhost:8080 ucloud@IP_address_from_the_red_mark # Change the port number if multiple jupyter notebook are started within the same session\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#open-jupyter-notebook","title":"Open Jupyter Notebook","text":"<p>Press the link in the output above and it should open a jupyter notebook</p> <p>Now the R and Python kernel should be available (see figure below)</p> <p></p> <p></p>"},{"location":"Tutorials/VMs/shh/","title":"How to Generate a SSH key","text":"<p>In order to access the VM it is necessary to create Secure Shell Protocol (SSH) keys more specifically a public key (shareable part) and a private key (kept safe locally).</p>"},{"location":"Tutorials/VMs/shh/#on-windows-linux-systems","title":"On Windows &amp; Linux Systems","text":"<p>To generate your public key, find and open terminal and type: </p> <pre><code># For linux only\nsudo apt install openssh-client\n\n\n# For both Windows &amp; Linux\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in C:\\Users\\user/.ssh/id_rsa.\nYour public key has been saved in C:\\Users\\user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/VMs/shh/#manually-locate-open-and-copy-our-public-key-from-id_rsapub-file","title":"Manually locate, open and copy our public key from id_rsa.pub file.","text":""},{"location":"Tutorials/VMs/shh/#on-windows","title":"On Windows","text":"<p>the file might be here: \"C:\\Users\\write_your_user_name.ssh\"</p>"},{"location":"Tutorials/VMs/shh/#on-linux","title":"On Linux","text":"<p>The generated SSH key will be by stored under ~/.ssh/id_rsa.pub by default.</p> <p>More information can be found at https://genome.au.dk/docs/getting-started/#public-key-authentication </p>"},{"location":"UCloud_SlurmCluster/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"UCloud_SlurmCluster/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"UCloud_SlurmCluster/Ray/","title":"Ray","text":""},{"location":"UCloud_SlurmCluster/Ray/#example-using-ray","title":"Example using Ray","text":"<p>In terminal run:</p> <pre><code>python slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Output\n\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#open-extra-terminal-for-three-nodes","title":"Open extra Terminal for three Nodes","text":""},{"location":"UCloud_SlurmCluster/Ray/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/Ray/#observed-that-the-work-is-disbrubted-across-all-three-nodes","title":"Observed that the work is disbrubted across all three nodes.","text":"<p>This may look different for different backends (e.g. Dask). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"UCloud_SlurmCluster/Ray/#output-files","title":"Output files","text":""},{"location":"UCloud_SlurmCluster/Ray/#the-autogenerated-slurm-script-slurmtest_0425-1208sh","title":"The autogenerated SLURM script (SlurmTest_0425-1208.sh)","text":"<pre><code>#!/bin/bash\n# shellcheck disable=SC2206\n# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!\n# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!\n\n#SBATCH --job-name=SlurmTest_0425-1208\n#SBATCH --output=SlurmTest_0425-1208.log\n\n### This script works for any number of nodes, Ray will find and manage all resources\n#SBATCH --nodes=3\n#SBATCH --exclusive\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n##SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE} #De-activated by KGP 230317\n\n# Load modules or your own conda environment here\n# module load pytorch/v1.4.0-gpu\n# conda activate ${CONDA_ENV}\n\n\n# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====\n\necho $SLURM_JOB_NODELIST\n\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\") # Getting the node names\n\nnodes_array=($nodes)\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w \"$node_1\" hostname --ip-address) # making redis-address\n\n# if we detect a space character in the head node IP, we'll\n# convert it to an ipv4 address. This step is optional.\nif [[ \"$ip\" == *\" \"* ]]; then\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$ip\"\nif [[ ${#ADDR[0]} -gt 16 ]]; then\nip=${ADDR[1]}\nelse\nip=${ADDR[0]}\nfi\necho \"IPV6 address detected. We split the IPV4 address as $ip\"\nfi\n\nport=6379\nip_head=$ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"STARTING HEAD at $node_1\"\nsrun --nodes=1 --ntasks=1 -w \"$node_1\" ray start --head --node-ip-address=\"$ip\" --port=$port --block &amp;\nsleep 30\n\n#worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\n#export NB_WORKERS=$((${SLURM_JOB_NUM_NODES-1})) #number of nodes other than the head node\n#echo ${NB_WORKERS}\n\nexport NB_WORKERS=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\necho \"STARTING ${NB_WORKERS} WORKERS\"\nfor ((i = 1; i &lt;= NB_WORKERS; i++)); do\nnode_i=${nodes_array[$i]}\necho \"STARTING WORKER $i at $node_i\"\nsrun --nodes=1 --ntasks=1 -w \"$node_i\" ray start --address \"$ip_head\" --block &amp;\nsleep 5\ndone\n\n# ===== Call your code below =====\necho \"RUNNING CODE: python /work/data/SklearnRay.py\"\npython /work/data/SklearnRay.py\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#autogenerated-log-file-slurmtest_0425-1208log","title":"Autogenerated log file (SlurmTest_0425-1208.log)","text":"<pre><code>node[0-2]\nIPV6 address detected. We split the IPV4 address as 10.42.47.86\nIP Head: 10.42.47.86:6379\nSTARTING HEAD at node0\n2023-04-25 12:08:40,054 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\nSTARTING 2 WORKERS\nSTARTING WORKER 1 at node1\n2023-04-25 12:08:38,026 INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-04-25 12:08:38,026 INFO scripts.py:710 -- Local node IP: 10.42.47.86\n2023-04-25 12:08:41,222 SUCC scripts.py:747 -- --------------------\n2023-04-25 12:08:41,222 SUCC scripts.py:748 -- Ray runtime started.\n2023-04-25 12:08:41,223 SUCC scripts.py:749 -- --------------------\n2023-04-25 12:08:41,223 INFO scripts.py:751 -- Next steps\n2023-04-25 12:08:41,223 INFO scripts.py:752 -- To connect to this Ray runtime from another node, run\n2023-04-25 12:08:41,223 INFO scripts.py:755 --   ray start --address='10.42.47.86:6379'\n2023-04-25 12:08:41,223 INFO scripts.py:771 -- Alternatively, use the following Python code:\n2023-04-25 12:08:41,223 INFO scripts.py:773 -- import ray\n2023-04-25 12:08:41,223 INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='10.42.47.86')\n2023-04-25 12:08:41,223 INFO scripts.py:790 -- To see the status of the cluster, use\n2023-04-25 12:08:41,223 INFO scripts.py:791 --   ray status\n2023-04-25 12:08:41,223 INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.\n2023-04-25 12:08:41,224 INFO scripts.py:809 -- To terminate the Ray runtime, run\n2023-04-25 12:08:41,224 INFO scripts.py:810 --   ray stop\n2023-04-25 12:08:41,224 INFO scripts.py:891 -- --block\n2023-04-25 12:08:41,224 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:41,224 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nSTARTING WORKER 2 at node2\n2023-04-25 12:08:48,882 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:48,933 I 2244 2244] global_state_accessor.cc:356: This node has an IP address of 10.42.28.36, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:48,859 INFO scripts.py:866 -- Local node IP: 10.42.28.36\n2023-04-25 12:08:48,935 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:48,935 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:48,935 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:48,935 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:48,935 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:48,935 INFO scripts.py:891 -- --block\n2023-04-25 12:08:48,935 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:48,935 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nRUNNING CODE: python /work/data/SklearnRay.py\n2023-04-25 12:08:54,215 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:54,271 I 956 956] global_state_accessor.cc:356: This node has an IP address of 10.42.34.213, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:54,135 INFO scripts.py:866 -- Local node IP: 10.42.34.213\n2023-04-25 12:08:54,274 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:54,275 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:54,275 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:54,275 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:54,275 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:54,275 INFO scripts.py:891 -- --block\n2023-04-25 12:08:54,275 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:54,275 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n2023-04-25 12:09:24,758 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.42.47.86:6379...\n2023-04-25 12:09:24,775 INFO worker.py:1553 -- Connected to Ray cluster.\n2023-04-25 12:09:25,073 WARNING pool.py:604 -- The 'context' argument is not supported using ray. Please refer to the documentation for how to control ray initialization.\nFitting 10 folds for each of 500 candidates, totalling 5000 fits\n209.00055767036974\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nslurmstepd-node2: error: *** STEP 2.3 ON node2 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node0: error: *** STEP 2.1 ON node0 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node1: error: *** STEP 2.2 ON node1 CANCELLED AT 2023-04-25T12:12:54 ***\nsrun: error: node2: task 0: Exited with exit code 1\nsrun: error: node1: task 0: Exited with exit code 1\nsrun: error: node0: task 0: Exited with exit code 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/","title":"UCloud Tutorial: Run Multi-node SLURM Cluster on UCloud","text":"TutorialFiles"},{"location":"UCloud_SlurmCluster/SLURM/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>In addition to the normal setting fill out the following options (See figure below).</p> <p>In this example launched as cluster consisting of 3 nodes with three folder added to the launch:</p> <ul> <li>\"miniconda3\"  - contains the conda environment I want to deploy across the different nodes.</li> <li>\"SLURM_deployment\" - contains the easy-to-use deployment scripts provided in this tutorial. </li> <li>\"SLURM_scripts\" - contains the user specific script and data to run on the cluster.</li> </ul> <p>In this example Conda is used for package and evironment management. Check here for more information on Conda on UCloud.</p> <p></p>"},{"location":"UCloud_SlurmCluster/SLURM/#when-the-job-has-started-open-terminal-for-node-1","title":"When the job has started open Terminal for Node 1","text":"<p>Run following commands in the terminal: </p> <pre><code># activate SLURM Cluster if not activated in the step above\ninit_slurm_cluster\n\n# List Avaliable nodes\nsinfo -N -l\n</code></pre> <p>The controller node is always the first node. Called \"node0\" in within SLURM but called \"Node 1\" in the UCloud interface). All additional nodes are named sequentially. For example, a cluster consisting of three full u1-standard nodes is configured as follows:</p> <pre><code>NODELIST   NODES PARTITION     STATE CPUS   S:C:T MEMORY\nnode0         1     CLOUD*     idle   64   1:64:1 385024\nnode1         1     CLOUD*     idle   64   1:64:1 385024\nnode2         1     CLOUD*     idle   64   1:64:1 385024\n</code></pre> <p>But called Node 1, Node 2 and Node 3 in the UCloud interface.</p>"},{"location":"UCloud_SlurmCluster/SLURM/#acitvate-conda-environment","title":"Acitvate Conda Environment","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which environment is in path (e.g. X = python,R..)\nwhich X # (e.g. X = python,R..)\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/X # (e.g. X = python,R..)\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-deployment-scripts","title":"SLURM deployment scripts","text":"<p>The SLURM deployment script (\"slurm-launch.py\") have been adopted from  Ray documentation to support the addition of other python libraries (Dask, ipyparallel) and other languages (e.g. R).</p>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-launchpy","title":"slurm-launch.py","text":"<p>\"slurm-launch.py\" auto-generates SLURM scripts and launch. slurm-launch.py uses an underlying template (e.g. \"slurm-template_ray.sh\" or \"slurm-template_dask.sh\") and fills out placeholders given user input.</p> <pre><code># Change path:\ncd /work/SLURM_deployment\n\n# Python with Ray\npython slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Python with Dask\npython slurm-launch.py --script slurm-template_dask.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnDask.py\" --num-nodes 3 --nprocs 8 --nthreads 1\n\n# R with doParallel\npython slurm-launch.py --script slurm-template_R.sh --exp-name SlurmTest --command \"Rscript --vanilla /work/SLURM_scripts/doParallel.r\" --num-nodes 3 --nprocs 8 --nthreads 1 # Example of Output\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#addditionel-options","title":"Addditionel options","text":"<pre><code>--exp-name          # The experiment name. Will generate {exp-name}_{date}-{time}.sh and {exp-name}_{date}-{time}.log.\n--command           # The command you wish to run. For example: rllib train XXX or python XXX.py.\n--node (-w)         # The specific nodes you wish to use, in the same form as the output of sinfo. Nodes are automatically assigned if not specified.\n--num-nodes (-n)    # The number of nodes you wish to use. Default: 1.\n--partition (-p):   # The partition you wish to use. Default: \u201c\u201d, will use user\u2019s default partition.\n--load-env:         # The command to setup your environment. For example: module load cuda/10.1. Default: \u201c\u201d.\n--nprocs: --nthreads:\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#open-extra-terminal-for-the-three-nodes","title":"Open extra terminal for the three nodes","text":""},{"location":"UCloud_SlurmCluster/SLURM/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/SLURM/#observed-that-the-work-is-distibuted-across-all-three-nodes","title":"Observed that the work is distibuted across all three nodes.","text":"<p>This may look different for different frameworks (e.g. Ray, Dask, R). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"UCloud_SlurmCluster/SLURM/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/SLURM/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/SLURM/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/SLURM/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/SLURM/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"moody-s_datahub/","title":"moodys_datahub","text":"<p>moodys_datahub</p>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/","title":"How to get started","text":""},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#download","title":"Download","text":"<p>This jupyter notebook can be downloaded from this link below:</p> <p>https://github.com/CBS-HPC/moody-s_datahub/blob/main/mkdocs/how_to_get_started.ipynb</p> <p>The pip wheel can be manually downloaded using the link below:</p> <p>https://github.com/CBS-HPC/moody-s_datahub/blob/main/dist/moodys_datahub-0.0.1-py3-none-any.whl</p> <p>Or directly to the working folder by running the line below:</p> <pre><code>!curl -s -L -o moodys_datahub-0.0.1-py3-none-any.whl https://github.com/CBS-HPC/moody-s_datahub/blob/main/dist/moodys_datahub-0.0.1-py3-none-any.whl \n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#installation","title":"Installation","text":"<p>Install the package \"orbis-0.0.1-py3-none-any.whl\" or \"orbis-0.0.1.tar.gz\" using pip:</p> <pre><code>pip install moodys_datahub-0.0.1-py3-none-any.whl\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#usage","title":"Usage","text":"<pre><code>from moodys_datahub.tools import *\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#connect-to-sftp-server","title":"Connect to SFTP server","text":"<p>For CBS associates want to connect to the \"CBS server\" the user needs only to provide the \"privatkey\" (.pem) provided by CBS Staff.</p> <p>To connec to other servers the user needs to provide \"hostname\", \"username\",\"port\" and \"privatkey\".</p> <pre><code># Connects to default CBS SFTP server\nSFTP = Sftp(privatekey=\"user_provided-ssh-key.pem\")\n\n# Connects to custom SFTP server\nSFTP = Sftp(hostname = \"example.com\", username = \"username\", port = 22,privatekey=\"user_provided-ssh-key.pem\") \n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#select-data-product-and-table","title":"Select Data Product and Table","text":"<p>Run the function below to select \"Data Product\" and \"Table\" that are available on the SFTP.</p> <pre><code>SFTP.select_data()\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#overview-of-remote-files","title":"Overview of Remote Files","text":"<p>The \"Data Product\" and \"Table\" has been selected the associated files on the SFTP server are listed as shown below:</p> <pre><code>SFTP.remote_files\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#define-options","title":"Define Options","text":"<p>The function below allows the user to set the following options:</p> <p>SFTP.delete_files : Delete Files After Processing (To Prevent Large Storage Consumption - 'False' is recommeded)</p> <p>SFTP.concat_files : Concatenate Sub-Files into a Single Output File ('True' is Recommeded):</p> <p>SFTP.output_format : Select Output File Formats (More than one can be selected - '.xlsx' is not recommeded)</p> <p>SFTP.file_size_mb : File Size Cutoff (MB) Before Splitting into Multiple Output files (Only an approxiate)</p> <pre><code>SFTP.define_options()\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#column-selection","title":"Column Selection","text":"<p>Select which columns (variables) to keep </p> <pre><code>SFTP.select_columns()\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#bvd-filter","title":"BVD Filter","text":"<p>Set a \"bvd_id\" filter. This can be provided in different ways as seen below as a python list of in .txt [Link] or .xlsx[Link] format. When setting the .bvd_list the user will be prompted to select one or more \"bvd\" related columns.</p> <p>It can perform an extract search based on full bvd_id numbers or based on the country code that is the two starting letter of the bvd_id numbers.</p> <pre><code># Text file\nSFTP.bvd_list = 'bvd_numbers.txt'\n\n# Excel file - Will search through columns for relevant bvd formats\nSFTP.bvd_list = 'bvd_numbers.xlsx'\n\n# Country filter\nSFTP.bvd_list = ['US','DK','CN']\n\n# bvd number lists\nSFTP.bvd_list = ['DK28505116','SE5567031702','SE5565475489','NO934382404','SE5566674205','DK55828415']\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#time-period-filter","title":"Time Period Filter","text":"<p>A time periode filter can be set as seen below. Subsequently the user will be prompted to select a \"date\" column. </p> <p>Not all table have suitable \"date\" columns for which time periode filtration is not possible. </p> <pre><code>SFTP.time_period = [1998,2005]\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#create-filters-using-the-sftpquery-method","title":"Create Filters using the SFTP.query() method","text":"<p>With the SFTP.query() method the user can create more custom filters.The method utilises pandas.query() method. A few examples are shown below:</p> <pre><code># Example 1: \nSFTP.query =\"total_assets &gt; 1000000000\"\n\n# Example 2\nquery_args = ['CN9360885371','CN9360885372','CN9360885373']\nSFTP.query=f\"bvd_id_number in {query_args}\"\n\n# Example 3\nquery_args = 'DK'\nSFTP.query = f\"bvd_id_number.str.startswith('{query_args}', na=False)\"\n\n# Example 4\nbvd_numbers = ['CN9360885371','CN9360885372','CN9360885373']\ncountry_code = 'CN'\nSFTP.query =f\"bvd_id_number in {bvd_numbers} | (total_assets &gt; 1000000000 &amp; bvd_id_number.str.startswith('{country_code}', na=False))\"\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#create-filters-using-custom-functions","title":"Create Filters using custom functions","text":"<p>It is also possible to defined SFTP.queryprovide a custom functionWith the pandas.query() method the user can create more custom filters. Below are show four examples of how to setup a query string. </p> <pre><code>bvd_id_numbers = ['CN9360885371','CN9360885372','CN9360885373']\ncolumn_filter = ['bvd_id_number','fixed_assets','original_currency','total_assets']  # Example column filter\n\ndef bvd_filter(df,bvd_id_numbers,column_filter,specific_value,specific_col):\n\n     # Check if specific_col is a column in the DataFrame\n    if specific_col is not None and specific_col not in df.columns:\n        raise ValueError(f\"{specific_col} is not a column in the DataFrame.\")\n\n    if specific_value is not None:\n                df = df[df[specific_col] &gt; specific_value]\n\n    if bvd_id_numbers:\n        if isinstance(bvd_id_numbers, list):\n            row_filter = df['bvd_id_number'].isin(bvd_id_numbers)\n        elif isinstance(bvd_id_numbers, str):\n            row_filter  = df['bvd_id_number'].str.startswith(bvd_id_numbers)\n        else:\n            raise ValueError(\"bvd_id_numbers must be a list or a string\")\n\n        if row_filter.any():\n            df = df.loc[row_filter]\n        else: \n           df = None \n\n    if df is not None and column_filter:\n        df = df[column_filter]\n\n    return df\n\nSFTP.query = bvd_filter\nSFTP.query_args = [bvd_id_numbers,column_filter,1000000000,'total_assets']\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#test-the-selected-filters","title":"Test the selected Filters","text":"<p>Before running the selected filters on all files (SFTP.remote_files) is can be a good idea to test it on a single sub-file using the function below. </p> <p>It should be noted that the sub-file that is used below will not contain rows that a relevant for the defined filters.</p> <pre><code>df_sample = SFTP.process_one()\n\ndf_sample = SFTP.process_one(save_to = 'csv',files = SFTP.remote_files[0], n_rows = 2000)\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#download-all-files-before-batch-processing","title":"Download all files before \"Batch Processing\"","text":"<p>If working on a slower connection it may be benificial to start downloading all remote files before processing them.</p> <p>When the downloading process has been started \"SFTP._download_finished\" will change from a \"None\" to \"False and then \"True\" upon download completion.</p> <p>The function is executed asyncionsly and the user can proceed working in the jupyter notebook while it is running.</p> <pre><code>SFTP.download_all()\n\n# Define the number of workers/processors that should be utilsed. \n\nSFTP.download_all(num_workers = 12)\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#batch-process-for-on-all-files","title":"Batch Process for on all files","text":"<p>All files can be processed using the function below. </p> <ul> <li> <p>If \"SFTP._download_finished\" is \"None\" the function also download the files. </p> </li> <li> <p>If \"SFTP._download_finished\" is \"False\" it will wait upon the download process has been completed and \"SFTP._download_finished\" is set to \"True\". </p> </li> </ul> <pre><code># If no input arguments are provided it will utilise the filters that has beeen defined in the selection above.\nresults = SFTP.process_all()\n\n# Input arguments can also be set manually as shown below:  \nresults = SFTP.process_all(files = SFTP.remote_files, \n                            destination = \"file_name\",  \n                            num_workers = 12, \n                            select_cols = ['bvd_id_number','fixed_assets','original_currency','total_assets'],\n                            date_query = None,\n                            bvd_query = None,\n                            query = bvd_filter, \n                            query_args = [bvd_id_numbers,column_filter,1000000000,'total_assets']\n                            )\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#search-in-data-dictionary-for-variablescolumns","title":"Search in Data Dictionary for variables/columns","text":"<p>It is possible to search in the \"Data Dictionary\" for variables, keywords or topic. The \"Data Dictionary\" will be filtrated according to \"Data Product\" and \"Table\" selection.</p> <pre><code>df_search = SFTP.search_dictionary(save_to = 'xlsx', search_word = 'total_asset')\n\ndf_search = SFTP.search_dictionary(save_to = 'xlsx',\n                                    search_word = 'subsidiaries',\n                                    search_cols= { 'Data Product': False,'Table': False,'Column': True,'Definition': False },\n                                    letters_only = True,\n                                    extact_match = False,\n                                    data_product = None,\n                                    table = None,  \n                                    )\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#search-for-country-codes","title":"Search for country codes","text":"<p>The function below can be used to find the \"bvd_id\" country codes for specific nations.</p> <pre><code># Search for country codes by country name\nSFTP.search_country_codes(search_word='congo')\n\n# Define columns to search in:\n\nSFTP.search_country_codes(search_word='DK', search_cols = { 'Country': False,'Code': True })\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/how_to_get_started/#create-a-new-sftp-object","title":"Create a new SFTP Object","text":"<p>The \"SFTP.copy_obj()\" function can be used to create a new SFTP object in order to process another \"Data Product\"/\"Table.</p> <ul> <li>SFTP.select_data() will be prompted automatically </li> <li>Other filters will be reset.</li> </ul> <pre><code>SFTP_2 = SFTP.copy_obj()\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/moody-s%20datahub/","title":"Moody's Datahub","text":""},{"location":"moody-s_datahub/mkdocs/moody-s%20datahub/#introduction","title":"Introduction","text":"<p>This page introduces the \"moodys_datahub\" Python package, designed to facilitate access to Moody's Datahub \"Data Products\" exported via SFTP. </p> <p>The package offers a suite of functions for selecting, downloading, and curating \"Data Products\" in a highly parallelized manner, optimizing the use of compute power available on cloud or HPC systems.</p>"},{"location":"moody-s_datahub/mkdocs/moody-s%20datahub/#sftp-server-access","title":"SFTP server Access","text":"<p>To CBS associates should contact CBS staff to provide a personal \"privatkey\" (.pem) that is used to estabilish connection to the available \"CBS server\".</p> <p>To access other SFTP servers the user also needs additional information regarding \"hostname\" and \"username\". </p> <p>When connecting to an SFTP server, the package detects all export folders and attempts to match them to specific \"Data Products\" based on their \"Table\" titles. However, some \"Data Products\" have identical table names, making automatic differentiation impossible. In such cases, the user will be prompted to manually match the export folder with the correct \"Data Product.\"</p>"},{"location":"moody-s_datahub/mkdocs/moody-s%20datahub/#format-recommandation","title":"Format recommandation","text":"<p>Moody's Datahub provides several export formats (\".csv\", \".parquet\", \".orc\", and \".avro\"). The moodys_datahub Python package supports all these formats, but the preferred format is \".parquet\" due to its high compression, columnar storage format, and the fact that each table is partitioned into many small files, offering significant performance benefits.</p> <p>Using .csv is not recommended because of its lack of compression and partitioning, which results in some tables being single files exceeding 300 GB in size.</p>"},{"location":"moody-s_datahub/mkdocs/moody-s%20datahub/#system-reccomendation","title":"System Reccomendation","text":"<p>Given the large size of most data tables, it is highly recommended that users utilize more powerful machines available through cloud or HPC systems. Based on user experience, a \"rule of thumb\" has been established: each \"worker\" (which processes one \"subfile\" at a time) should have approximately 12 GB of memory available.</p> <p>For CBS Associates, it is highly recommended to use UCloud and run the application on a \"u1-standard\" machine with 64 cores, 384 GB of RAM, and a high-speed internet connection.</p>"},{"location":"moody-s_datahub/mkdocs/moody-s%20datahub/#getting-started","title":"Getting Started","text":"<p>Below you will find links to a \"How to Get Started\" tutorial (which can also be downloaded as a Jupyter notebook), the moodys_datahub Git repository, and the \"API Reference\" page.</p> <ul> <li> <p>How to get started</p> </li> <li> <p>Git Repository: moody-s_datahub</p> </li> <li> <p>API Reference</p> </li> </ul>"},{"location":"moody-s_datahub/mkdocs/reference/","title":"Reference","text":""},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp","title":"<code>Sftp</code>","text":"Source code in <code>moodys_datahub\\tools.py</code> <pre><code>class Sftp:\n    def __init__(self, hostname:str = \"s-f2112b8b980e44f9a.server.transfer.eu-west-1.amazonaws.com\", username:str = \"D2vdz8elTWKyuOcC2kMSnw\", port:int = 22, privatekey:str = None):\n\"\"\"Constructor Method\n\n         ### Sftp Class Object Variables        \n        - `connection`: None or pysftp.Connection object. Represents the current SFTP connection.\n\n        - `hostname`: str. Hostname of the SFTP server (Default connects to CBS SFTP server).\n        - `username`: str. Username for authentication (Default connects to CBS SFTP server).\n        - `privatekey`: str or None. Path to the private key file for authentication (Valid private key is needed to access SFTP server)\n        - `port`: int. Port number for the SFTP connection (default is 22).\n\n        - `output_format`: list of str. List of supported output file formats (e.g., ['.csv', '.parquet']).\n        - `file_size_mb`: int. Maximum file size in MB for before splitting output files..\n        - `delete_files`: bool. Flag indicating whether to delete processed files.\n        - `concat_files`: bool. Flag indicating whether to concatenate processed files.\n        - `select_cols`: list or None. List of columns to select during file operations.\n        - `query`: str, fnc or None. Query string or function for filtering data.\n        - `query_args`: list or None. list of arguments for the query string or function.\n        - `dfs`: None or DataFrame. Stores concatenated DataFrames if concatenation is enabled.\n\n        \"\"\"\n\n        # Set connection object to None (initial value)\n        self.connection: object = None\n        self.hostname: str = hostname\n        self.username: str = username\n        self.privatekey: str = privatekey\n        self.port: int = port\n        self._cnopts = pysftp.CnOpts()\n        self._cnopts.hostkeys = None\n\n        self.output_format: list =  ['.csv'] \n        self.file_size_mb:int = 500\n        self.delete_files: bool = False\n        self.concat_files: bool = True\n        self._select_cols: list = None \n        self.query = None\n        self.query_args: list = None\n        self._bvd_list: list = [None,None,None]\n        self._time_period: list = [None,None,None,\"remove\"]\n        self.dfs = None\n\n        self._local_path: str = None\n        self._local_files: list = []\n\n        self._remote_path: str = None\n        self._remote_files: list = []\n\n        self._tables_available = None\n        self._tables_backup = None\n        self._set_data_product:str = None\n        self._time_stamp:str = None\n        self._set_table:str = None\n        self._table_dictionary = None\n        self._table_dates = None\n        self._download_finished = None\n\n        if hasattr(os, 'fork'):\n            self._pool_method = 'fork'\n            self._max_path_length = 256\n        else:\n            self._pool_method = 'threading'\n            self._max_path_length = 256\n\n        if sys.platform.startswith('linux'):\n            self._max_path_length = 4096\n        elif sys.platform == 'darwin':\n            self._max_path_length = 1024\n        elif sys.platform == 'win32':\n            self._max_path_length = 256\n\n        self.tables_available(save_to=False)\n\n    # pool method\n    @property\n    def pool_method(self):\n       return self._pool_method\n\n    @pool_method.setter\n    def pool_method(self,method:str):\n\"\"\"\n        Get or set the worker pool method for concurrent operations.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `method` (str): Worker pool method (`'fork'`, `'threading'`, `'spawn'`).\n\n        Returns:\n        - Current worker pool method (`'fork'`, `'threading'`, `'spawn'`).\n        \"\"\"\n        if not method in ['fork','theading','spawn']:\n            print('invalid worker pool method')\n            method = 'fork'\n\n        if not hasattr(os, 'fork') and method =='fork':\n            print('fork() processes are not supported by OS')\n            method = 'spawn'\n\n        print(f'\"{method}\" is chosen as worker pool method')\n        self._pool_method == method\n\n    # Local path and files\n    @property\n    def local_path(self):\n       return self._local_path\n\n    @local_path.setter\n    def local_path(self, path):\n\"\"\"\n        Get or set the local path for operations.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `path` (str): Local path to set or retrieve.\n\n        Returns:\n        - Current local path.\n        \"\"\"\n        if path is None:\n            self._remote_files = []\n            self._remote_path  = None    \n            self._local_files  = []\n            self._local_path   = None\n        elif path is not self._local_path:\n            self._local_files, self._local_path = self._check_path(path,\"local\")\n\n    @property\n    def local_files(self):\n        self._local_files, self._local_path = self._check_path(self._local_path,\"local\")\n        return self._local_files\n\n    @local_files.setter\n    def local_files(self, value):\n        self._local_files = self._check_files(value)\n\n    @property\n    def remote_path(self):\n       return self._remote_path\n\n    @remote_path.setter\n    def remote_path(self, path):\n\"\"\"\n        Get or set the remote path for operations.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `path` (str): Remote path to set or retrieve.\n\n        Returns:\n        - Current remote path.\n        \"\"\"\n\n        if path is None:\n            self._remote_files = []\n            self._remote_path  = None    \n            self._local_files  = []\n            self._local_path   = None\n            self._set_data_product = None\n            self._set_table = None\n\n        elif path is not self.remote_path:\n            self._local_files  = []\n            self._local_path   = None\n            self._remote_files, self._remote_path = self._check_path(path,\"remote\")\n\n            if self._remote_path is not None:\n\n                df = self._tables_available.query(f\"`Base Directory` == '{self._remote_path}'\")\n\n                if df.empty:\n                    df = self._tables_available.query(f\"`Export` == '{self._remote_path}'\")\n                    self._set_table = None\n                else:                 \n                    if (self._set_table and self._set_table not in df['Table'].values) or not self._set_table:\n                        self._set_table = df['Table'].iloc[0]\n\n                if not df.empty:\n                    if (self._set_data_product and self._set_data_product not in df['Data Product'].values) or not self._set_data_product:    \n                        self._set_data_product = df['Data Product'].iloc[0]\n                        self._tables_available = df \n\n                if len(self._remote_files) &gt; 1 and any(file.endswith('.csv') for file in self._remote_files):\n                    self._remote_files, self._remote_path = self._check_path(path,\"remote\")\n\n    @property\n    def remote_files(self):\n        return self._remote_files\n\n    @remote_files.setter\n    def remote_files(self, value):\n        self._remote_files = self._check_files(value)\n\n    @property\n    def set_data_product(self):\n        return self._set_data_product\n\n    @set_data_product.setter\n    def set_data_product(self, product):\n\"\"\"\n        Set or retrieve the current data product.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `product` (str): Data product name to set or retrieve.\n\n        Returns:\n        - Current data product.\n        \"\"\"\n\n        if (product is None) or (product is not self._set_data_product):\n            self._tables_available = self._tables_backup.copy()\n\n        if product is None:\n            self._set_data_product = None\n            self._set_table = None\n            self.remote_path = None\n            self._time_stamp = None\n            self._select_cols = None \n            self.query = None\n            self.query_args = None\n\n        elif product is not self._set_data_product:\n\n            df = self._tables_available.query(f\"`Data Product` == '{product}'\")\n\n            if df.empty:\n                df = self._tables_available.query(f\"`Data Product`.str.contains('{product}', case=False, na=False,regex=False)\")\n                if df.empty:  \n                    print(\"No such Data Product was found. Please set right data product\")\n                else:\n                    matches   = df[['Data Product']].drop_duplicates()\n                    if len(matches) &gt;1:\n                        print(f\"Multiple data products partionally match '{product}' : {matches['Data Product'].tolist()}. Please set right data product\" )\n                    else:\n                        print(f\"One data product partionally match '{product}' : {matches['Data Product'].tolist()}. Please set right data product\")\n\n            elif len(df['Export'].unique()) &gt; 1:\n                matches   = df[['Data Product','Export']].drop_duplicates()\n\n                print(f\"Multiple version of '{product}' are detected: {matches['Data Product'].tolist()} with export paths ('Export') {matches['Export'].tolist()} .Please Set the '.remote_path' property with the correct 'Export' Path\")                \n            else:\n                self._tables_available = df\n                self._set_data_product = product\n                self._set_table = None\n                self._select_cols = None \n                self.query = None\n                self.query_args = None\n                self._time_stamp = df['Timestamp'].iloc[0]\n\n    @property\n    def set_table(self):\n        return self._set_table\n\n    @set_table.setter\n    def set_table(self, table):\n\"\"\"\n        Set or retrieve the current table.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `table` (str): Table name to set or retrieve.\n\n        Returns:\n        - Current table.\n        \"\"\"\n\n        if table is None:\n            self._set_table = None\n            self.remote_path = None\n            self._select_cols = None \n            self.query = None\n            self.query_args = None\n\n        elif table is not self._set_table:\n\n            if self._set_data_product is None:\n                df = self._tables_available.query(f\"`Table` == '{table}'\")\n            else:\n                df = self._tables_available.query(f\"`Table` == '{table}' &amp;  `Data Product` == '{self._set_data_product}'\")\n\n            if df.empty:\n                df = self._tables_available.query(f\"`Table`.str.contains('{table}', case=False, na=False,regex=False)\")\n                if len(df) &gt;1:\n                    matches   = df[['Data Product','Table']].drop_duplicates()\n                    print(f\"Multiple tables partionally match '{table}' : {matches['Table'].tolist()} from {matches['Data Product'].tolist()}. Please set right table\" )\n                elif df.empty:    \n                    print(\"No such Table was found. Please set right table\")\n                self._set_table = None\n            elif len(df) &gt; 1:\n                if self._set_data_product is None: \n                    matches   = df[['Data Product','Table']].drop_duplicates()\n                    print(f\"Multiple tables match '{table}' : {matches['Table'].tolist()} from {matches['Data Product'].tolist()}. Please set Data Product using the '.set_data_product' property\")\n\n                elif len(df['Export'].unique()) &gt; 1:\n                    matches   = df[['Data Product','Table','Export']].drop_duplicates()\n                    print(f\"Multiple version of '{table}' are detected: {matches['Table'].tolist()} from {matches['Data Product'].tolist()} with export paths ('Base Directory') {matches['Base Directory'].tolist()} .Please Set the '.remote_path' property with the correct 'Base Directory' Path\")\n                self._set_table = None    \n            else:\n                self._set_table = table\n                self.set_data_product = df['Data Product'].iloc[0]\n                self.remote_path = df['Base Directory'].iloc[0]\n                self._select_cols = None \n                self.query = None\n                self.query_args = None\n\n    @property\n    def bvd_list(self):\n        return self._bvd_list\n\n    @bvd_list.setter\n    def bvd_list(self, bvd_list = None):\n        def load_bvd_list(file_path, df_bvd ,delimiter='\\t'):\n            # Get the file extension\n            file_extension = file_path.split('.')[-1].lower()\n\n            # Load the file based on the extension\n            if file_extension == 'csv':\n                df = pd.read_csv(file_path)\n            elif file_extension in ['xls', 'xlsx']:\n                df = pd.read_excel(file_path)\n            elif file_extension == 'txt':\n                df = pd.read_csv(file_path, delimiter=delimiter)\n            else:\n                raise ValueError(f\"Unsupported file extension: {file_extension}\")\n\n                    # Process each column\n            for column in df.columns:\n                    # Convert the column to a list of strings\n                    bvd_list = df[column].dropna().astype(str).tolist()\n                    bvd_list = [item for item in bvd_list if item.strip()]\n\n                    # Pass through the first function\n                    bvd_list = _check_list_format(bvd_list)\n\n                    # Pass through the second function\n                    bvd_list, search_type, non_matching_items = check_bvd_format(bvd_list, df_bvd)\n\n                    # If successful, return the result\n                    if search_type is not None:\n                        return bvd_list, search_type, non_matching_items\n\n            return  bvd_list, search_type, non_matching_items  \n\n        def check_bvd_format(bvd_list, df):\n            bvd_list = list(set(bvd_list))\n            # Check against df['Code'].values\n            df_code_values = df['Code'].values\n            df_matches = [item for item in bvd_list if item in df_code_values]\n            df_match_count = len(df_matches)\n\n            # Check against the regex pattern\n            pattern = re.compile(r'^[A-Za-z]+[*]?[A-Za-z]*\\d*[-\\dA-Za-z]*$')\n            regex_matches = [item for item in bvd_list if pattern.match(item)]\n            regex_match_count = len(regex_matches)\n\n            # Determine which check has more matches\n            if df_match_count &gt;= regex_match_count:\n                non_matching_items = [item for item in bvd_list if item not in df_code_values]\n                return df_matches, True if df_match_count == len(bvd_list) else None, non_matching_items\n            else:\n                non_matching_items = [item for item in bvd_list if not pattern.match(item)]\n                return regex_matches, False if regex_match_count == len(bvd_list) else None, non_matching_items\n\n        if bvd_list is not None:\n            df =self.search_country_codes()\n\n            if (self._bvd_list[1] is not None and self._select_cols is not None) and self._bvd_list[1] in self._select_cols:\n                    self._select_cols.remove(self._bvd_list[1])\n\n            self._bvd_list = [None,None,None]\n            search_word = None\n            if (isinstance(bvd_list,str)) and os.path.isfile(bvd_list):\n                bvd_list, search_type,non_matching_items = load_bvd_list(bvd_list,df)\n            elif (isinstance(bvd_list,list) and len(bvd_list)==2) and (isinstance(bvd_list[0],(list, pd.Series, np.ndarray)) and isinstance(bvd_list[1],str)):\n                search_word =  bvd_list[1]\n                if isinstance(bvd_list[0],(pd.Series, np.ndarray)):\n                    bvd_list = bvd_list[0].tolist()\n                else:\n                    bvd_list = bvd_list[0]\n            else:\n                if isinstance(bvd_list,(pd.Series, np.ndarray)):\n                    bvd_list = bvd_list.tolist()\n\n            bvd_list = _check_list_format(bvd_list)\n            bvd_list, search_type,non_matching_items = check_bvd_format(bvd_list,df)\n\n            if search_type is None:\n                raise ValueError(f\"The following elements does not seem to match bvd format:{non_matching_items}\")\n\n            if self._set_data_product is None or self._set_table is None:\n                self.select_data()\n\n            if search_word is None:\n                bvd_col = self.search_dictionary()\n            else:\n                bvd_col = self.search_dictionary(search_word = search_word,search_cols={'Data Product':False,'Table':False,'Column':True,'Definition':False})\n\n            if bvd_col.empty:\n                raise ValueError(\"No 'bvd' columns were found for this table\")\n\n            bvd_col = bvd_col['Column'].unique().tolist()\n\n            self._bvd_list[0]  = bvd_list\n\n            if len(bvd_col) &gt; 1:\n                if isinstance(search_word, str) and search_word in bvd_col:\n                    self._bvd_list[1] = search_word\n                else:\n                    _select_list('_SelectMultiple',bvd_col,'Columns:','Select \"bvd\" Columns to filtrate',_select_bvd,[self._bvd_list,self._select_cols, search_type])\n                    return\n            else:    \n                self._bvd_list[1]  = bvd_col[0]\n\n            self._bvd_list[2] = _construct_query(self._bvd_list[1],self._bvd_list[0],search_type)\n        else:\n            self._bvd_list = [None,None,None]\n\n        if self._select_cols  is not None:\n            self._select_cols = _check_list_format(self._select_cols,self._bvd_list[1],self._time_period[2])\n\n    @property\n    def time_period(self):\n        return self._time_period\n\n    @time_period.setter\n    def time_period(self,years: list = None):\n        def check_year(years):\n            # Get the current year\n            current_year = datetime.now().year\n\n            # Check if the list has exactly two elements\n            if len(years) &lt;2:\n                raise ValueError(\"The list must contain at least a start and end year e.g [1998,2005]. It can also contain a column name as a third element [1998,2005,'closing_date']\")\n\n            # Initialize start and end year with default values\n            start_year = years[0] if years[0] is not None else 1900\n            end_year = years[1] if years[1] is not None else current_year\n\n            # Check if years are integers\n            if not isinstance(start_year, int) or not isinstance(end_year, int):\n                raise ValueError(\"Both start year and end year must be integers\")\n\n            # Check if years are within a valid range\n            if start_year &lt; 1900 or start_year &gt; current_year:\n                raise ValueError(f\"Start year must be between 1900 and {current_year}\")\n            if end_year &lt; 1900 or end_year &gt; current_year:\n                raise ValueError(f\"End year must be between 1900  and {current_year}\")\n\n            # Check if start year is less than or equal to end year\n            if start_year &gt; end_year:\n                raise ValueError(\"Start year must be less than or equal to end year\")\n\n            if len(years) == 3:\n                return [start_year, end_year, years[2]] \n            else:\n                return [start_year, end_year, None] \n\n        if years is not None:\n            if (self._time_period[2] is not None and self._select_cols is not None) and self._time_period[2] in self._select_cols:\n                self._select_cols.remove(self._time_period[2])\n\n            self._time_period = check_year(years)\n            self._time_period.append(\"remove\")\n\n            if self._set_data_product is None or self._set_table is None:\n                self.select_data()\n\n            date_col = self.table_dates(data_product=self.set_data_product,table = self._set_table,save_to=False)\n\n            if date_col.empty:\n                raise ValueError(\"No data columns were found for this table\")\n\n            date_col = date_col['Column'].unique().tolist()\n\n            if self._time_period[2] is not None and self._time_period[2] not in date_col:\n                raise ValueError(f\"{self._time_period[2]} was not found as date related column: {date_col}. Set ._time_period[2] with the correct one\") \n\n            elif self._time_period[2] is None and len(date_col) &gt; 1:\n                _select_list('_SelectList',date_col,'Columns:','Select \"date\" Column to filtrate',_select_date,[self._time_period,self._select_cols])\n                return          \n\n            if self._time_period[2] is None:\n                self._time_period[2] = date_col[0]\n        else:\n            self._time_period =[None,None,None,\"remove\"]\n        if self._select_cols  is not None:\n            self._select_cols = _check_list_format(self._select_cols,self._bvd_list[1],self._time_period[2])\n\n    @property\n    def select_cols(self):\n        return self._select_cols\n\n    @select_cols.setter\n    def select_cols(self,select_cols = None):\n\n        if select_cols is not None:\n            if self._set_data_product is None or self._set_table is None:\n                self.select_data()\n\n            select_cols = _check_list_format(select_cols,self._bvd_list[1],self._time_period[2])\n\n            table_cols = self.search_dictionary(data_product=self.set_data_product,table = self._set_table,save_to=False)\n\n            if table_cols.empty:\n                self._select_cols = None\n                raise ValueError(\"No columns were found for this table\")\n\n            table_cols = table_cols['Column'].unique().tolist()\n\n            if not all(element in table_cols for element in select_cols):\n                not_found = [element for element in table_cols if element not in select_cols]\n                print(\"The following selected columns cannot be found in the table columns\", not_found)\n                self._select_cols = None\n            else:\n                self._select_cols = select_cols\n        else:\n            self._select_cols = None\n\n    def connect(self):\n\"\"\"\n        Establish an SFTP connection.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n\n        Returns:\n        - SFTP connection object.\n        \"\"\"\n        sftp = pysftp.Connection(host=self.hostname , username=self.username ,port = self.port ,private_key=self.privatekey, cnopts=self._cnopts)\n        return sftp\n\n    def select_data(self):\n\"\"\"\n        Asynchronously select and set the data product and table using interactive widgets.\n\n        This method initializes an instance of `_SelectData` with `_tables_backup`, displays interactive widgets\n        to allow the user to select a data product and table, and sets these selections to `self.set_data_product`\n        and `self.set_table`, respectively. It also prints the selected data product and table.\n\n        The method ensures that `_tables_available` or `_tables_backup` is populated by calling `tables_available()`\n        if they are not already set.\n\n        Notes:\n        - This method uses `asyncio.ensure_future` to run the asynchronous function `f` which handles the widget interaction.\n\n        Example:\n            self.select_data()\n        \"\"\"\n\n        async def f(self):\n            Select_obj = _SelectData(self._tables_backup,'Select Data Product and Table')\n            selected_product, selected_table = await Select_obj.display_widgets()\n\n            df = self._tables_backup.copy()\n            df = df[['Data Product','Table','Base Directory','Top-level Directory']].query(f\"`Data Product` == '{selected_product}' &amp; `Table` == '{selected_table}'\").drop_duplicates()\n\n            if len(df) &gt; 1:\n                options = df['Top-level Directory'].tolist()\n                product = df['Data Product'].drop_duplicates().tolist()\n                msg = f\"Multiple data products match '{product[0]}'. Please set right data product:\" \n                self._set_table = selected_table\n                self._set_data_product = selected_product\n                _select_list('_SelectList',options,f\"'{product[0]}':\",msg,_select_product,[df,self])\n            elif len(df) == 1:\n                self.set_data_product = selected_product\n                self.set_table = selected_table\n                print(f\"{self.set_data_product} was set as Data Product\")\n                print(f\"{self.set_table} was set as Table\")\n\n        self._download_finished = None \n\n        asyncio.ensure_future(f(self))\n\n    def define_options(self):\n        async def f(self):\n\n            config = {\n            'delete_files': self.delete_files,\n            'concat_files': self.concat_files,\n            'output_format': self.output_format,\n            'file_size_mb': self.file_size_mb}\n\n            Options_obj = _SelectOptions(config)\n\n            config = await Options_obj.display_widgets()\n\n            if config:\n                self.delete_files = config['delete_files']\n                self.concat_files = config['concat_files']\n                self.output_format = config['output_format']\n                self.file_size_mb = config['file_size_mb']\n\n                print(\"The following options were selected:\")\n                print(f\"Delete Files: {self.delete_files}\")\n                print(f\"Concatenate Files: {self.output_format}\")\n                print(f\"Output File Size: {self.file_size_mb } MB\")\n\n        asyncio.ensure_future(f(self))\n\n    def select_columns(self):\n\"\"\"\n        Asynchronously select and set columns for a specified data product and table using interactive widgets.\n\n        This method performs the following steps:\n        1. Checks if the data product and table are set. If not, it calls `select_data()` to set them.\n        2. Searches the dictionary for columns corresponding to the set data product and table.\n        3. Displays an interactive widget for the user to select columns based on their names and definitions.\n        4. Sets the selected columns to `self._select_cols` and prints the selected columns.\n\n        If no columns are found for the specified table, a `ValueError` is raised.\n\n        Args:\n        - `self`: Implicit reference to the instance.\n\n        Notes:\n        - This method uses `asyncio.ensure_future` to run the asynchronous function `f` which handles the widget interaction.\n        - The function `f` combines column names and definitions for display, maps selected items to their indices,\n        and then extracts the selected columns based on these indices.\n\n        Raises:\n        - `ValueError`: If no columns are found for the specified table.\n\n        Example:\n            self.select_columns()\n        \"\"\"\n        async def f(self,column, definition):\n\n            combined = [f\"{col}  -----  {defn}\" for col, defn in zip(column, definition)]\n\n            Select_obj = _SelectMultiple(combined,'Columns:',\"Select Table Columns\")\n            selected_list = await Select_obj.display_widgets()\n            if selected_list is not None:\n\n                # Create a dictionary to map selected strings to their indices in the combined list\n                indices = {item: combined.index(item) for item in selected_list if item in combined}\n\n                # Extract selected columns based on indices\n                selected_list = [column[indices[item]] for item in selected_list if item in indices]\n                self._select_cols = selected_list\n                self._select_cols = _check_list_format(self._select_cols,self._bvd_list[1],self._time_period[2])\n                print(f\"The following columns have been selected: {self._select_cols}\")\n\n        if self._set_data_product is None or self._set_table is None:\n            self.select_data()\n\n        table_cols = self.search_dictionary(data_product=self.set_data_product,table = self._set_table,save_to=False)\n\n        if table_cols.empty:\n            self._select_cols = None\n            raise ValueError(\"No columns were found for this table\")\n\n        column = table_cols['Column'].tolist()\n        definition = table_cols['Definition'].tolist()\n\n        asyncio.ensure_future(f(self, column, definition)) \n\n    def copy_obj(self):\n\"\"\"\n        Create a deep copy of the current Sftp instance with optional updates.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n\n        Returns:\n        - Deep copy of the current Sftp instance with optional updates.\n        \"\"\"\n\n        new_obj= Sftp(hostname = self.hostname, username = self.username, port = 22, privatekey= self.privatekey) \n\n        #new_obj = copy.deepcopy(self)\n\n        new_obj.select_data()\n        #new_obj.bvd_list = None\n        #new_obj.time_period = None\n        #new_obj.select_cols = None\n\n        return new_obj\n\n    def table_dates(self,save_to:str=False, data_product = None,table = None):\n\"\"\"\n        Retrieve and save the available date columns for a specified data product and table.\n\n        This method performs the following steps:\n        1. Ensures that the available tables and table dates are loaded.\n        2. Filters the dates data by the specified data product and table, if provided.\n        3. Optionally saves the filtered results to a specified format.\n\n        Args:\n        - `self`: Implicit reference to the instance.\n        - `save_to` (str, optional): Format to save results. If False, results are not saved (default is False).\n        - `data_product` (str, optional): Specific data product to filter results by. If None, defaults to `self.set_data_product`.\n        - `table` (str, optional): Specific table to filter results by. If None, defaults to `self.set_table`.\n\n        Returns:\n        - pandas.DataFrame: A DataFrame containing the filtered dates for the specified data product and table. If no results are found, an empty DataFrame is returned.\n\n        Notes:\n        - If `data_product` is provided and does not match any records, a message is printed and an empty DataFrame is returned.\n        - If `table` is provided and does not match any records, it attempts to perform a case-insensitive partial match search.\n        - If `save_to` is specified, the query results are saved in the format specified.\n\n        Example:\n            df = self.table_dates(save_to='csv', data_product='Product1', table='TableA')\n        \"\"\"    \n\n        if data_product is None and self.set_data_product is not None:\n            data_product = self.set_data_product\n\n            if table is None and self.set_table is not None:\n                table = self.set_table\n\n        if self._table_dates is None:\n            self._table_dates = _table_dates()        \n        df = self._table_dates\n        df = df[df['Data Product'].isin(self._tables_backup['Data Product'].drop_duplicates())]\n\n        if data_product is not None:\n            df_product = df.query(f\"`Data Product` == '{data_product}'\")\n            if df_product.empty:\n                print(\"No such Data Product was found. Please set right data product\")\n                return df_product\n            else:\n                df = df_product\n        if table is not None:\n            df_table = df.query(f\"`Table` == '{table}'\")\n            if df_table.empty:\n                df_table = df.query(f\"`Table`.str.contains('{table}', case=False, na=False,regex=False)\")\n                if df_table.empty:\n                    print(\"No such Table was found. Please set right table\")\n                    return df_table\n            df = df_table\n\n        _save_to(df,'date_cols_search',save_to)\n\n        return df    \n\n    # Under development\n    def _search_dictionary_list(self, save_to:str=False, search_word=None, search_cols={'Data Product':True, 'Table':True, 'Column':True, 'Definition':True}, letters_only:bool=False, exact_match:bool=False, data_product = None, table = None):\n\"\"\"\n        Search for a term in a column/variable dictionary and save results to a file.\n\n        Args:\n        - `self`: Implicit reference to the instance.\n        - `save_to` (str, optional): Format to save results. If False, results are not saved (default is False).\n        - `search_word` (str or list of str, optional): Search term(s). If None, no term is searched.\n        - `search_cols` (dict, optional): Dictionary indicating which columns to search. Columns are 'Data Product', 'Table', 'Column', and 'Definition' with default value as True for each.\n        - `letters_only` (bool, optional): If True, search only for alphabetic characters in the search term (default is False).\n        - `exact_match` (bool, optional): If True, search for an exact match of the search term. Otherwise, search for partial matches (default is False).\n        - `data_product` (str, optional): Specific data product to filter results by. If None, no filtering by data product (default is None).\n        - `table` (str, optional): Specific table to filter results by. If None, no filtering by table (default is None).\n\n        Returns:\n        - pandas.DataFrame: A DataFrame containing the search results. If no results are found, an empty DataFrame is returned.\n\n        Notes:\n        - If `data_product` is provided and does not match any records, a message is printed and an empty DataFrame is returned.\n        - If `table` is provided and does not match any records, it attempts to perform a case-insensitive partial match search.\n        - If `search_word` is provided and no matches are found, a message is printed indicating no results were found.\n        - If `letters_only` is True, the search term is processed to include only alphabetic characters before searching.\n        - If `save_to` is specified, the query results are saved in the format specified.\n        \"\"\"\n\n        if data_product is None and self.set_data_product is not None:\n            data_product = self.set_data_product\n\n        if table is None and self.set_table is not None:\n            table = self.set_table\n\n        if self._table_dictionary is None:\n            self._table_dictionary = _table_dictionary()        \n\n        df = self._table_dictionary\n        df = df[df['Data Product'].isin(self._tables_backup['Data Product'].drop_duplicates())]\n\n        if data_product is not None:\n            df_product = df.query(f\"`Data Product` == '{data_product}'\")\n            if df_product.empty:\n                print(\"No such Data Product was found. Please set right data product\")\n                return df_product\n            else:\n                df = df_product\n            search_cols['Data Product'] = False\n\n        if table is not None:\n            df_table = df.query(f\"`Table` == '{table}'\")\n            if df_table.empty:   \n                df_table = df.query(f\"`Table`.str.contains('{table}', case=False, na=False, regex=False)\")\n                if df_table.empty:\n                    print(\"No such Table was found. Please set right table\")\n                    return df_table\n            search_cols['Table'] = False\n            df = df_table \n\n        if search_word is not None:\n\n            if letters_only:\n                df_backup = df.copy()\n                df = df.map(_letters_only_regex)\n\n\n            if not isinstance(search_word, list):\n                search_word = [search_word]\n\n            results = []\n\n            for word in search_word:\n                if letters_only:\n                    word = _letters_only_regex(word)\n\n                if exact_match:\n                    base_string = \"`{col}` == '{{word}}'\"\n                else:\n                    base_string = \"`{col}`.str.contains('{{word}}', case=False, na=False, regex=False)\"\n\n                search_conditions = \" | \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n                final_string = search_conditions.format(word=word)\n\n                result_df = df.query(final_string)\n\n                if result_df.empty:\n                    base_string = \"'{col}'\"\n                    search_conditions = \" , \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n                    print(f\"No such '{word}' was detected across columns: \" + search_conditions)\n                else:\n                    if letters_only:\n                      result_df = df_backup.loc[result_df.index]  \n                    result_df['search_word'] = word\n                    results.append(result_df)\n\n            if results:\n                df = pd.concat(results, ignore_index=True)\n            else:\n                df = pd.DataFrame()\n\n            #if letters_only:\n            #    df = df_backup.loc[df.index]\n\n            if save_to:\n                print(f\"The following query was executed for each word in search_word: {search_word} : \")\n\n        _save_to(df, 'dict_search', save_to)\n\n        return df\n\n    def search_dictionary(self,save_to:str=False, search_word = None,search_cols={'Data Product':True,'Table':True,'Column':True,'Definition':True}, letters_only:bool=False,extact_match:bool=False, data_product = None, table = None):\n\n\"\"\"\n        Search for a term in a column/variable dictionary and save results to a file.\n\n        Args:\n        - `self`: Implicit reference to the instance.\n        - `save_to` (str, optional): Format to save results. If False, results are not saved (default is False).\n        - `search_word` (str, optional): Search term. If None, no term is searched.\n        - `search_cols` (dict, optional): Dictionary indicating which columns to search. Columns are 'Data Product', 'Table', 'Column', and 'Definition' with default value as True for each.\n        - `letters_only` (bool, optional): If True, search only for alphabetic characters in the search term (default is False).\n        - `exact_match` (bool, optional): If True, search for an exact match of the search term. Otherwise, search for partial matches (default is False).\n        - `data_product` (str, optional): Specific data product to filter results by. If None, no filtering by data product (default is None).\n        - `table` (str, optional): Specific table to filter results by. If None, no filtering by table (default is None).\n\n        Returns:\n        - pandas.DataFrame: A DataFrame containing the search results. If no results are found, an empty DataFrame is returned.\n\n        Notes:\n        - If `data_product` is provided and does not match any records, a message is printed and an empty DataFrame is returned.\n        - If `table` is provided and does not match any records, it attempts to perform a case-insensitive partial match search.\n        - If `search_word` is provided and no matches are found, a message is printed indicating no results were found.\n        - If `letters_only` is True, the search term is processed to include only alphabetic characters before searching.\n        - If `save_to` is specified, the query results are saved in the format specified.\n        \"\"\"\n\n\n        if data_product is None and self.set_data_product is not None:\n            data_product = self.set_data_product\n\n            if table is None and self.set_table is not None:\n                table = self.set_table\n\n        if self._table_dictionary is None:\n            self._table_dictionary = _table_dictionary()        \n        df = self._table_dictionary\n        df = df[df['Data Product'].isin(self._tables_backup['Data Product'].drop_duplicates())]\n\n        if data_product is not None:\n            df_product = df.query(f\"`Data Product` == '{data_product}'\")\n            if df_product.empty:\n                print(\"No such Data Product was found. Please set right data product\")\n                return df_product\n            else:\n                df = df_product\n            search_cols['Data Product'] = False\n        if table is not None:\n            df_table = df.query(f\"`Table` == '{table}'\")\n            if df_table.empty:   \n                df_table = df.query(f\"`Table`.str.contains('{table}', case=False, na=False,regex=False)\")\n                if df_table.empty:\n                    print(\"No such Table was found. Please set right table\")\n                    return df_table\n            search_cols['Table'] = False\n            df = df_table \n\n        if search_word is not None:\n            if letters_only:\n                df_backup = df.copy()\n                search_word = _letters_only_regex(search_word)\n                df = df.map(_letters_only_regex)\n\n            if extact_match:\n                base_string = \"`{col}` ==  '{{search_word}}'\"\n            else:\n                base_string = \"`{col}`.str.contains('{{search_word}}', case=False, na=False,regex=False)\"\n\n            search_conditions = \" | \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n            final_string = search_conditions.format(search_word=search_word)\n\n            df = df.query(final_string)\n\n            if df.empty:\n                base_string = \"'{col}'\"\n                search_conditions = \" , \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n                print(\"No such 'search word' was detected across columns: \" + search_conditions)\n                return df\n\n            if letters_only:\n                df = df_backup.loc[df.index]\n\n            if save_to:\n                print(f\"The folloiwng query was executed:\" + final_string)\n\n        _save_to(df,'dict_search',save_to)\n\n        return df    \n\n    def orbis_to_moodys(self,file):\n\n        def _load_orbis_file(file):\n            df = pd.read_excel(file, sheet_name='Results')\n\n            # Get the headings (column names) from the DataFrame\n            headings = df.columns.tolist()\n\n            # Process headings to keep only the first line if they contain multiple lines\n            processed_headings = [heading.split('\\n')[0] for heading in headings]\n\n            # Keep only unique headings\n            unique_headings = list(set(processed_headings)) \n            unique_headings.remove('Unnamed: 0')\n            return unique_headings\n\n        def sort_by(df):\n            # Sort by 'Data Product'\n            df_sorted = df.sort_values(by='Data Product')\n\n            # Count unique headings for each 'Data Product'\n            grouped = df_sorted.groupby('Data Product')['heading'].nunique().reset_index()\n            grouped.columns = ['Data Product', 'unique_headings']\n\n            # Sort 'Data Product' based on the number of unique headings in descending order\n            sorted_products = grouped.sort_values(by='unique_headings', ascending=False)['Data Product']\n\n            # Reorder the original DataFrame based on the sorted 'Data Product'\n            df_reordered = pd.concat(\n                [df_sorted[df_sorted['Data Product'] == product] for product in sorted_products],\n                ignore_index=True\n            )\n            return df_reordered\n\n        headings = _load_orbis_file(file)\n        headings_processed = [_letters_only_regex(heading) for heading in headings]\n\n        df = _table_dictionary()\n        df['letters_only'] = df['Column'].apply(_letters_only_regex)\n\n        found = []\n        not_found  = []\n        for heading, heading_processed in zip(headings,headings_processed):\n            df_sel = df.query(f\"`letters_only` == '{heading_processed}'\")\n\n            if df_sel.empty:\n                not_found.append(heading)\n            else:\n                df_sel = df_sel.copy()  # Avoid SettingWithCopyWarning\n                df_sel['heading'] = heading \n                found.append(df_sel)\n\n        # Concatenate all found DataFrames if needed\n        if found:\n            found = pd.concat(found, ignore_index=True)\n            found = sort_by(found)\n        else:\n            found = pd.DataFrame()\n\n        return found, not_found\n\n    def tables_available(self,save_to:str=False,reset:bool=False):\n\"\"\"\n        Retrieve available SFTP data products and tables and save them to a file.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `save_to` (str, optional): Format to save results (default is CSV).\n        - `reset` (bool, optional): Reset flag to force refreshing data products and tables.\n\n        Returns:\n        - Pandas DataFrame with the available SFTP data products and tables.\n        \"\"\"\n\n        if self._tables_available is None and self._tables_backup is None:\n            self._tables_available,to_delete = self._table_overview()\n            self._tables_backup = self._tables_available.copy()\n\n            if self.hostname == \"s-f2112b8b980e44f9a.server.transfer.eu-west-1.amazonaws.com\" and len(to_delete) &gt; 0:\n                print(\"------------------  DELETING OLD EXPORTS FROM SFTP\")\n                self._remove_exports(to_delete)\n\n        elif reset:\n            self._tables_available = self._tables_backup.copy()\n\n        # Specify unknown data product exports\n        self._specify_data_products()\n\n        _save_to(self._tables_available,'tables_available',save_to)\n\n        return self._tables_available.copy()\n\n    def search_country_codes(self,search_word = None,search_cols={'Country':True,'Code':True}):        \n\"\"\"\n        Search for country codes matching a search term.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `search_word` (str, optional): Term to search for country codes.\n        - `search_cols` (dict, optional): Dictionary indicating columns to search (default is {'Country':True,'Code':True}).\n\n        Returns:\n        - Pandas Dataframe of country codes matching the search term\n        \"\"\"\n\n        df = _country_codes()\n        if search_word is not None:\n\n            base_string = \"`{col}`.str.contains('{{search_word}}', case=False, na=False,regex=False)\"\n            search_conditions = \" | \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n            final_string = search_conditions.format(search_word=search_word)\n\n            df = df.query(final_string)\n\n            if df.empty:\n                base_string = \"'{col}'\"\n                search_conditions = \" , \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n                print(\"No such 'search word' was detected across columns: \" + search_conditions)\n                return df\n            else:\n                print(f\"The folloiwng query was executed:\" + final_string)\n\n        return df    \n\n    def process_one(self,save_to=False,files = None,n_rows:int=1000):\n\"\"\"\n        Retrieve a sample of data from a table and save it to a file.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `files` (list, optional): List of files to process. Defaults to `self.remote_files`.\n        - `save_to` (str, optional): Format to save sample data (default is CSV).\n        - `n_rows` (int, optional): Number of rows to retrieve (default is 1000).\n\n        Returns:\n        - Pandas Dateframe with output\n        \"\"\"\n\n        if files is None:\n            if self._set_data_product is None or self._set_table is None:\n                self.select_data()\n            files = [self.remote_files[0]]\n        elif isinstance(files,int):\n            files = [files]    \n\n        df, files = self.process_all(files = files,num_workers=len(files))\n\n        if df is None and files is not None:\n            dfs = []\n            for file in files:\n                df  = _load_table(file)\n                dfs.append(df)\n            df = pd.concat(dfs, ignore_index=True)\n            df = df.head(n_rows)\n            _save_to(df,'process_one',save_to) \n        elif not df.empty and files is not None:\n            df = df.head(n_rows)\n            print(f\"Results have been saved to '{files}'\")\n        elif df.empty:  \n            print(\"No rows were retained\")  \n        return df\n\n    def process_all(self, files:list = None,destination:str = None, num_workers:int = -1, select_cols: list = None , date_query = None, bvd_query = None, query = None, query_args:list = None,pool_method = None):\n\"\"\"\n        Read and process files into a DataFrame with optional filtering and parallel processing.\n\n        This method reads multiple files into Pandas DataFrames, optionally selecting specific columns and\n        applying filters, either sequentially or in parallel.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `files` (list, optional): List of files to process. Defaults to `self.remote_files`.\n        - `destination` (str, optional): Destination path for processed files.\n        - `num_workers` (int, optional): Number of workers for parallel processing. Default is -1 (auto-determined).\n        - `select_cols` (list, optional): Columns to select from files. Default is `self._select_cols`.\n        - `date_query`: (optional): Date query for filtering data. Default is `self.time_period`.\n        - `bvd_query`: (optional): BVD query for filtering data. Default is `self._bvd_list[2]`.\n        - `query` (str, optional): Query for additional filtering of data.\n        - `query_args` (list, optional): Arguments for the query.\n\n        Returns:\n        - `dfs`: List of Pandas DataFrames with selected columns and filtered data.\n        - `file_names`: List of file names processed.\n\n        Notes:\n        - If `select_cols` is provided, it is validated against expected formats.\n        - Uses parallel processing if `num_workers` is greater than 1.\n        - Handles file concatenation and deletion based on instance attributes (`concat_files`, `delete_files`).\n        - Prints current working directory if `self.delete_files` is `True`.\n\n        Raises:\n        - `ValueError`: If validation of arguments (`files`, `destination`, `flag`) fails.\n        \"\"\"\n\n        files = files or self.remote_files\n        date_query = date_query or self.time_period\n        bvd_query = bvd_query or self._bvd_list[2]\n        query = query or self.query\n        query_args = query_args or self.query_args\n        select_cols = select_cols or self._select_cols\n\n        # To handle executing when download_all() have not finished!\n        if self._download_finished is False and all(file in self._remote_files for file in files): \n            start_time = time.time()\n            timeout = 5\n            files_not_ready =  not all(file in self.local_files for file in files) \n            while files_not_ready:\n                time.sleep(0.1)\n                files_not_ready =  not all(file in self.local_files for file in files)\n                if time.time() - start_time &gt;= timeout:\n                    print(f\"Files have not finished downloading within the timeout period of {timeout} seconds.\")\n                    return None, None\n\n            self._download_finished =True \n\n        if select_cols is not None:\n            select_cols = _check_list_format(select_cols,self._bvd_list[1],self._time_period[2])\n\n        try:\n            flag =  any([select_cols, query, all(date_query),bvd_query]) \n            files, destination = self._check_args(files,destination,flag)\n        except ValueError as e:\n            print(e)\n            return None\n\n        if isinstance(num_workers, (int, float, complex)):\n            num_workers = int(num_workers) \n        else: \n            num_workers = -1\n\n        if num_workers &lt; 1:\n            num_workers =int(psutil.virtual_memory().total/ (1024 ** 3)/12)\n\n        # Read multithreaded\n        if num_workers != 1 and len(files) &gt; 1:\n            def batch_processing():\n                def batch_list(input_list, batch_size):\n\"\"\"Splits the input list into batches of a given size.\"\"\"\n                    batches = []\n                    for i in range(0, len(input_list), batch_size):\n                        batches.append(input_list[i:i + batch_size])\n                    return batches\n\n                batches = batch_list(files,num_workers)\n\n                lists = []\n\n                print(f'Processing {len(files)} files in Parallel')\n\n                for index, batch in enumerate(batches,start=1):\n                    print(f\"Processing Batch {index} of {len(batches)}\")\n                    print(f\"------ First file: '{batch[0]}'\")  \n                    print(f\"------ Last file : '{batch[-1]}'\")               \n                    params_list = [(file, destination, select_cols, date_query, bvd_query, query, query_args) for file in batch]\n                    list_batch = _run_parallel(fnc=self._process_parallel,params_list=params_list,n_total=len(batch),num_workers=num_workers,pool_method=pool_method ,msg='Processing')\n                    lists.extend(list_batch)\n\n\n                file_names = [elem[1] for elem in lists]\n                file_names = [file_name[0] for file_name in file_names if file_name is not None]\n\n                dfs =  [elem[0] for elem in lists]\n                dfs = [df for df in dfs if df is not None]\n\n                flags =  [elem[2] for elem in lists]\n\n                return dfs, file_names, flags\n\n            dfs, file_names, flags = batch_processing()\n\n        else: # Read Sequential\n            print(f'Processing  {len(files)} files in sequence')\n            dfs, file_names, flags = self._process_sequential(files, destination, select_cols, date_query, bvd_query, query, query_args,num_workers)\n\n        flag =  all(flags) \n\n        if (not self.concat_files and not flag) or len(dfs) == 0:\n                self.dfs = None\n        elif self.concat_files and not flag:\n\n            # Concatenate and save\n            self.dfs, file_names = _save_chunks(dfs=dfs,file_name=destination,output_format=self.output_format,file_size=self.file_size_mb, num_workers=num_workers)\n\n        return self.dfs, file_names\n\n    def download_all(self,num_workers = None):\n\n        if hasattr(os, 'fork'):\n            pool_method = 'fork'\n        else:\n            print(\"Function only works on Unix systems right now\")\n            return\n\n        if self._set_data_product is None or self._set_table is None:\n            self.select_data()\n\n        if num_workers is None:\n            num_workers= int(cpu_count() - 2)\n\n        if isinstance(num_workers, (int, float, complex))and num_workers != 1:\n            num_workers= int(num_workers)\n\n        _, _ = self._check_args(self._remote_files)\n\n        self._download_finished = None \n        self.delete_files = False\n\n        print(\"Downloading all files\")\n        process = Process(target=self.process_all, kwargs={'num_workers': num_workers, 'pool_method': pool_method})\n        process.start()\n\n        self._download_finished = False \n\n    def get_column_names(self,save_to:str=False, files = None):\n\"\"\"\n        Retrieve column names from a DataFrame or dictionary and save them to a file.\n\n        Input Variables:\n        - `self`: Implicit reference to the instance.\n        - `save_to` (str, optional): Format to save results (default is CSV).\n        - `files` (list, optional): List of files to retrieve column names from.\n\n        Returns:\n        - List of column names or None if no valid source is provided.\n        \"\"\"\n\n        def from_dictionary(self):\n            if self.set_table is not None: \n                df = self.search_dictionary(save_to=False)\n                column_names = df['Column'].to_list()\n                return column_names\n            else: \n                return None\n        def from_files(self,files):\n            if files is None and self.remote_files is None:   \n                raise ValueError(\"No files were added\")\n            elif files is None and self.remote_files is not None:\n                files = self.remote_files   \n\n            try:\n                file,_ = self._check_args([files[0]])\n                file, _ = self._get_file(file[0])\n                parquet_file = pq.ParquetFile(file)\n                # Get the column names\n                column_names = parquet_file.schema.names\n                return column_names\n            except ValueError as e:\n                print(e)\n                return None\n\n        if files is not None:\n            column_names = from_files(self,files)\n        else:       \n            column_names = from_dictionary(self)    \n\n        if column_names is not None:\n            df = pd.DataFrame({'Column_Names': column_names})\n            _save_to(df,'column_names',save_to)\n\n        return column_names\n\n    def _table_overview(self,product_overview = None):\n\n        print('Retrieving Data Product overview from SFTP..wait a moment')\n\n        if product_overview is None:\n            product_overview =_table_names()\n\n        with self.connect() as sftp:\n            product_paths = sftp.listdir()\n            newest_exports = []\n            time_stamp = []\n            repeating = []\n            data_products = []\n            to_delete = []\n\n            for product_path in product_paths:\n                sel_product = product_overview.loc[product_overview['Top-level Directory'] == product_path, 'Data Product']\n                if len(sel_product) == 0:\n                    data_products.append(None)\n                else:    \n                    data_products.append(sel_product.values[0])\n\n                tnfs_folder = product_path + '/tnfs'\n                export_paths =  sftp.listdir(product_path)\n\n                if not sftp.exists(tnfs_folder):\n                    path = product_path + '/' + export_paths[0]\n                    newest_exports.append(path)\n                    time_stamp.append(None)\n                    repeating.append('One-off')\n                    continue\n\n                # Get all .tnfs files in the 'tnfs' folder\n                tnfs_files = [f for f in sftp.listdir(tnfs_folder) if f.endswith('.tnf')]\n                if not tnfs_files:\n                    print(tnfs_files)\n                    print('Error')\n\n                # Initialize variables to keep track of the newest .tnfs file\n                newest_tnfs_file = None\n                newest_mtime = float('-inf')\n\n                # Determine the newest .tnfs file\n                for tnfs_file in tnfs_files:\n                    tnfs_file_path  = tnfs_folder + '/' + tnfs_file\n                    file_attributes = sftp.stat(tnfs_file_path)\n                    mtime = file_attributes.st_mtime\n                    if mtime &gt; newest_mtime:\n                        newest_mtime = mtime\n                        if newest_tnfs_file is not None:\n                            sftp.remove(newest_tnfs_file)\n                        newest_tnfs_file = tnfs_file_path\n                    else:\n                        sftp.remove(tnfs_file_path)\n\n                if newest_tnfs_file:\n\n                    time_stamp.append(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(newest_mtime)))\n                    if self._local_path is not None:\n                        local_file = self._local_path + '/' +  \"temp.tnf\"\n                    else: \n                        local_file = \"temp.tnf\"\n                    sftp.get(newest_tnfs_file,  local_file)\n\n                    # Read the contents of the newest .tnfs file\n                    with open( local_file , 'r') as f:\n                        tnfs_data = json.load(f)\n                        newest_export = product_path + '/' + tnfs_data.get('DataFolder')\n                        newest_exports.append(newest_export)\n                        repeating.append('Repeating')\n\n                    os.remove(local_file)\n\n                    for export_path in export_paths:\n                        export_path = product_path + '/' +  export_path \n                        if export_path != newest_export and export_path != tnfs_folder:\n                            to_delete.append(export_path)\n\n            # Create a DataFrame from the lists\n            df = pd.DataFrame({'Data Product': data_products,'Top-level Directory': product_paths,'Newest Export': newest_exports,'Timestamp': time_stamp,'Repeating': repeating})\n\n            data = []\n\n            for _ , row  in df.iterrows():                \n                data_product = row['Data Product']\n                timestamp = row['Timestamp']\n                main_directory = row['Top-level Directory']\n                export = row['Newest Export']\n                repeating = row['Repeating']\n                tables = sftp.listdir(export)\n\n                full_paths  = [export + '/' + table for table in tables]\n                full_paths = [os.path.dirname(full_path) if full_path.endswith('.csv') else full_path for full_path in full_paths]\n\n                if pd.isna(data_product):\n                    data_product, tables = _table_match(tables)\n\n                # Append a dictionary with export, timestamp, and modified_list to the list\n                for full_path, table in zip(full_paths,tables):\n\n                    data.append({'Data Product':data_product,\n                                 #'Table': os.path.basename(full_path),\n                                 'Table': table,\n                                 'Base Directory': full_path,\n                                 'Timestamp': timestamp,\n                                 'Repeating':repeating,\n                                 'Export': export,\n                                 'Top-level Directory':main_directory})\n\n            # Create a DataFrame from the list of dictionaries\n            df = pd.DataFrame(data)\n\n            return  df, to_delete\n\n    def _remove_exports(self,to_delete = None,num_workers = None):\n\n        if to_delete is None:\n            _, to_delete = self._table_overview()\n\n        if len(to_delete) == 0:\n            return\n\n        def batch_list(input_list, num_batches):\n\"\"\"Splits the input list into a specified number of batches.\"\"\"\n            # Calculate the batch size based on the total number of elements and the number of batches\n\n            if len(input_list) &lt;= num_batches:\n                num_batches = len(input_list)\n            batch_size = len(input_list) // num_batches\n\n            # If there is a remainder, some batches will have one extra element\n            remainder = len(input_list) % num_batches\n\n            batches = []\n            start = 0\n            for i in range(num_batches):\n                # Calculate the end index for each batch\n                end = start + batch_size + (1 if i &lt; remainder else 0)\n                batches.append(input_list[start:end])\n                start = end\n\n            return batches\n\n        # Detecting files to delete\n        lists = _run_parallel(fnc=self._recursive_collect,params_list=to_delete,n_total=len(to_delete),msg = 'Collecting files to delete')\n        file_paths = [item for sublist in lists for item in sublist]\n\n        # Define worker\n        if num_workers is None:\n            num_workers= int(cpu_count() - 2)\n\n        if isinstance(num_workers, (int, float, complex))and num_workers != 1:\n            num_workers = int(num_workers)\n\n        # Batch files to delete\n        batches = batch_list(file_paths,num_workers)\n\n        # Deleting files\n        _run_parallel(fnc=self._delete_files,params_list=batches,n_total=len(batches),msg = 'Deleting files')\n\n        # Deleting empty folders\n        _run_parallel(fnc=self._delete_folders,params_list=to_delete,n_total=len(to_delete),msg='Deleting folders')\n\n    def _delete_folder(self,folder_path:str=None):\n        def recursive_delete(sftp, path,extensions: tuple = (\".parquet\", \".csv\",\".orc\",\".avro\")):\n            for file_attr in sftp.listdir_attr(path):\n                full_path = path + '/' + file_attr.filename\n\n                # Check if the file ends with any of the specified extensions\n                if full_path.endswith(extensions):\n                    sftp.remove(full_path)\n                elif sftp.isdir(full_path):\n                    recursive_delete(sftp, full_path)\n                else:\n                    sftp.remove(full_path)\n\n        with self.connect() as sftp:\n            try:\n                recursive_delete(sftp, folder_path)\n                print(f\"Folder {folder_path} deleted successfully\")\n            except FileNotFoundError:\n                print(f\"Folder {folder_path} not found\")\n            except Exception as e:\n                print(f\"Failed to delete folder {folder_path}: {e}\")\n\n    def _recursive_collect(self, path,extensions: tuple = (\".parquet\", \".csv\",\".orc\",\".avro\")):\n            file_paths = []\n            with self.connect() as sftp:\n                for file_attr in sftp.listdir_attr(path):\n                    full_path = path + '/' + file_attr.filename\n\n                    # Check if the file ends with any of the specified extensions\n                    if full_path.endswith(extensions):\n                        file_paths.append(full_path)\n                    elif sftp.isdir(full_path):\n                        subfolder_paths = self._recursive_collect(full_path,extensions)\n                        file_paths.extend(subfolder_paths)\n                    else:\n                        file_paths.append(full_path)\n\n            return file_paths\n\n    def _delete_files(self,files):\n            with self.connect() as sftp:\n                for file in files:\n                   sftp.remove(file)  \n\n    def _delete_folders(self,folder_path:str=None):\n\n        def recursive_delete(sftp, path):\n            for file_attr in sftp.listdir_attr(path):\n                full_path = path + '/' + file_attr.filename\n                sftp.remove(full_path)   \n\n        with self.connect() as sftp:\n            try:\n                recursive_delete(sftp, folder_path)\n                print(f\"Folder {folder_path} deleted successfully\")\n            except FileNotFoundError:\n                print(f\"Folder {folder_path} not found\")\n            except Exception as e:\n                print(f\"Failed to delete folder {folder_path}: {e}\")\n\n    def _get_attr(self, inputs_args):\n        file, path, mode = inputs_args\n        data = {'file': [],\n                'size': [],\n                'time_stamp': []}\n        if mode == \"local\":\n            data['file'].append(file)\n            data['size'].append(os.path.getsize(path + \"/\" + file))\n        else:\n            with self.connect() as sftp:\n                file_attributes = sftp.stat(path + \"/\" + file)\n                data['file'].append(file)\n                data['size'].append(file_attributes.st_size)\n                data['time_stamp'].append(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_attributes.st_mtime)))\n\n        return pd.DataFrame(data)\n\n    def _file_attr(self,mode:str=None,num_workers:int = -1):\n\n        file_attributes = pd.DataFrame()\n        if mode == \"local\":\n            path = self._local_path\n            files = self._local_files\n        else:\n            path = self._remote_path\n            files = self._remote_files\n\n        if path is not None:\n            print(f\"----------getting {mode} file attributes\")\n\n            # Read multithreaded\n            if isinstance(num_workers, (int, float, complex))and num_workers != 1:\n                num_workers = int(num_workers) \n                params_list =  [(file, path, mode) for file in files]\n                dfs = _run_parallel(fnc=self._get_attr,params_list=params_list,n_total=len(files),num_workers=num_workers,msg='Processing')\n                file_attributes = pd.concat(dfs, ignore_index=True)\n            else:\n                print('------ Processing files in sequence')\n                for file in files:\n                    file_attributes =  pd.concat([file_attributes, self._get_attr((file, path, mode))])\n        else:\n            print(f\"{mode} path is not defined.\")\n\n        file_attributes.reset_index(drop=True, inplace=True)\n        file_attributes.to_csv(self._local_path + \"/file_attributes.csv\")\n        return file_attributes\n\n    def _check_path(self,path,mode=None):\n        files = []\n        if path is not None:\n            if mode == \"local\" or mode is None:\n                if os.path.exists(path):\n                    if os.path.isdir(path):\n                        files = os.listdir(path)\n                    elif os.path.isfile(path):\n                        files = [os.path.basename(path)]\n                        path  = os.path.dirname(path)\n                else:\n                    if mode is None:\n                        mode = \"remote\"\n                    else:\n                        os.makedirs(path)\n                        print(f\"Folder '{path}' created.\")\n\n            if mode==\"remote\":\n                sftp = self.connect()\n                if sftp.exists(path):\n                    files = sftp.listdir(path)\n                    if not files:\n                        files = [os.path.basename(path)]\n                        path  = os.path.dirname(path)\n                else:\n                    print(f\"Remote path is invalid:'{path}'\")\n                    path = None\n\n            if len(files) &gt; 1 and any(file.endswith('.csv') for file in files):\n                if self._set_table is not None:\n                    # Find the file that matches the match_string without the .csv suffix\n                    files = [next((file for file in files if os.path.splitext(file)[0] == self._set_table), None)]\n        else:\n            files = []\n        return files,path\n\n    def _check_files(self,value):\n        if isinstance(value, list) and all(isinstance(item, str) for item in value):\n            return value\n        else:\n            raise ValueError(\"file list must be a list of strings\")\n\n    def _get_file(self,file:str):\n\n        def _file_exist(file:str):\n            base_path = os.getcwd()\n            base_path = base_path.replace(\"\\\\\", \"/\")\n\n            if not file.startswith(base_path):\n                file = os.path.join(base_path,file)\n\n            if len(file) &gt; self._max_path_length:\n                raise ValueError(f\"file path is longer ({len(file)}) than the max path length of the OS {self._max_path_length}. Set '.local_path' closer to root. The file is : '{file}'\") \n\n            if os.path.exists(file):\n                self.delete_files = False\n                flag = True\n            else:\n                file = self._local_path + \"/\" + os.path.basename(file)\n                flag = False\n                if not file.startswith(base_path):\n                    file = base_path + \"/\" + file\n\n            if len(file) &gt; self._max_path_length:\n                raise ValueError(f\"file path is longer ({len(file)}) than the max path length of the OS {self._max_path_length}. Set '.local_path' closer to root. The file is : '{file}'\")     \n\n            return file, flag\n\n        local_file,flag = _file_exist(file) \n\n        if not os.path.exists(local_file):\n            try:\n                with self.connect() as sftp: \n                    remote_file =  self.remote_path + \"/\" + os.path.basename(file)\n                    sftp.get(remote_file, local_file)\n                    file_attributes = sftp.stat(remote_file)\n                    time_stamp = file_attributes.st_mtime\n                    os.utime(local_file, (time_stamp, time_stamp))\n            except Exception as e:\n                raise ValueError(f\"Error reading remote file: {e}\")\n\n        return local_file, flag\n\n    def _curate_file(self,flag:bool,file:str,destination:str,local_file:str,select_cols:list, date_query:list=[None,None,None,\"remove\"], bvd_query:str = None, query = None, query_args:list = None,num_workers:int = -1):\n        df = None \n        file_name = None\n        if any([select_cols, query, all(date_query),bvd_query]) or flag: \n\n            file_extension = file.lower().split('.')[-1]\n\n            if file_extension in ['csv']:\n                df = _load_csv_table(file = local_file, \n                                    select_cols = select_cols, \n                                    date_query = date_query, \n                                    bvd_query = bvd_query, \n                                    query = query, \n                                    query_args = query_args,\n                                    num_workers = num_workers\n                                    )\n            else:\n                df = _load_table(file = local_file, \n                                    select_cols = select_cols, \n                                    date_query = date_query, \n                                    bvd_query = bvd_query, \n                                    query = query, \n                                    query_args = query_args\n                                    )\n\n            if (df is not None and self.concat_files is False and self.output_format is not None) and not flag:\n                file_name, _ = os.path.splitext(destination + \"/\" + file)\n                file_name = _save_files(df,file_name,self.output_format)\n                df = None\n\n            if self.delete_files and not flag:\n                try: \n                    os.remove(local_file)\n                except:\n                    raise ValueError(f\"Error deleting local file: {local_file}\")\n        else:\n            file_name = local_file  \n\n        return df, file_name\n\n    def _process_sequential(self, files:list, destination:str=None, select_cols:list = None, date_query:list=[None,None,None,\"remove\"], bvd_query:str = None, query = None, query_args:list = None,num_workers:int = -1):\n        dfs = []\n        file_names = []\n        flags   = []\n        total_files = len(files)\n        for i, file in enumerate(files, start=1):\n            if total_files &gt; 1:\n                print(f\"{i} of {total_files} files\")   \n            try:\n                local_file, flag = self._get_file(file)\n                df , file_name = self._curate_file(flag = flag,\n                                                    file = file,\n                                                    destination = destination,\n                                                    local_file = local_file,\n                                                    select_cols = select_cols,\n                                                    date_query = date_query,\n                                                    bvd_query = bvd_query,\n                                                    query = query,\n                                                    query_args = query_args,\n                                                    num_workers = num_workers\n                                                    )\n                flags.append(flag)\n\n                if df is not None:\n                    dfs.append(df)\n                else:\n                    file_names.append(file_name)\n            except ValueError as e:\n                print(e)\n\n        return dfs,file_names,flags\n\n    def _process_parallel(self, inputs_args:list):      \n        file, destination, select_cols, date_query, bvd_query, query, query_args = inputs_args\n        local_file, flag = self._get_file(file)\n        df, file_name = self._curate_file(flag = flag,\n                                                file = file,\n                                                destination = destination,\n                                                local_file = local_file,\n                                                select_cols = select_cols,\n                                                date_query = date_query,\n                                                bvd_query = bvd_query,\n                                                query = query,\n                                                query_args = query_args\n                                                )\n\n        return [df, file_name,flag]\n\n    def _check_args(self,files:list,destination = None,flag:bool = False):\n\n        def _detect_files(files):\n            def format_timestamp(timestamp: str) -&gt; str:\n                formatted_timestamp = timestamp.replace(' ', '_').replace(':', '-')\n                return formatted_timestamp\n\n            if isinstance(files,str):\n                files = [files]\n            elif isinstance(files,list) and len(files) == 0:\n                raise ValueError(\"'files' is a empty list\") \n            elif not isinstance(files,list):\n                raise ValueError(\"'files' should be str or list formats\") \n\n            existing_files = [file for file in files if os.path.exists(file)]\n            missing_files = [file for file in files if not os.path.exists(file)]\n\n            if not existing_files:\n                if not self.local_files and not self.remote_files:\n                    raise ValueError(\"No local or remote files detected\") \n\n                if self._local_path is None and self._remote_path is not None:\n\n                    if self._time_stamp:\n                        self.local_path = \"Data Products/\" + self.set_data_product +'_exported '+ format_timestamp(self._time_stamp) + '/' + self.set_table\n                    else:\n                        self.local_path = \"Data Products/\" + self.set_data_product +'_one-off'+ '/' + self.set_table\n\n                missing_files = [file for file in files if file not in self._remote_files and file not in self.local_files]\n                existing_files = [file for file in files if file in self._remote_files or file in self.local_files] \n\n            if not existing_files:\n                raise ValueError('Requested files cannot be found locally or remotely') \n\n            return existing_files, missing_files\n\n        files,missing_files = _detect_files(files)\n\n        if missing_files:\n            print(\"Missing files:\")\n            for file in missing_files:\n                print(file)\n\n        if destination is None and flag:    \n            current_time = datetime.now()\n            timestamp_str = current_time.strftime(\"%y%m%d%H%M\")\n\n            if self._remote_path is not None:\n                suffix= os.path.basename(self._remote_path)\n            else: \n                suffix= os.path.basename(self._local_path)\n\n            destination = f\"{timestamp_str}_{suffix}\"\n\n            base_path = os.getcwd()\n            base_path = base_path.replace(\"\\\\\", \"/\")\n\n            destination = base_path + \"/\" + destination\n\n        if self.concat_files is False and destination is not None:\n            if not os.path.exists(destination):\n                os.makedirs(destination)\n        elif self.concat_files is True and destination is not None:\n            parent_directory = os.path.dirname(destination)\n\n            if parent_directory and not os.path.exists(parent_directory):\n                os.makedirs(parent_directory)\n\n        return files, destination   \n\n    def _table_search(self, search_word):\n\n        filtered_df = self._tables_available.query(f\"`Data Product`.str.contains('{search_word}', case=False, na=False,regex=False) | `Table`.str.contains('{search_word}', case=False, na=False,regex=False)\")\n        return filtered_df\n\n    def _specify_data_products(self):\n\n        def extract_options(row):\n            if \"Mutliple_Options: \" in row:\n                # Extract the substring starting after \"Mutliple_Options: \"\n                list_str = row.split(\"Mutliple_Options: \")[1]\n                # Convert the extracted substring to a Python list using ast.literal_eval\n                try:\n                    options_list = ast.literal_eval(list_str)\n                    return options_list\n                except (SyntaxError, ValueError):\n                    print(f\"Error parsing list from row: {row}\")\n                    return None\n            else:\n                return None\n\n        # Function to check if a row contains \"Mutliple_Options: \"  \n        def contains_multiple_options(row):\n            return \"Mutliple_Options: \" in row\n\n        async def f(self,df,df_multiple):    \n                    selected_values = None\n\n                    # Keep only columns '1' and '2'\n                    df_multiple = df_multiple[['Data Product','Top-level Directory']]\n\n                    # Remove duplicate rows based on columns '1' and '2'\n                    df_multiple = df_multiple.drop_duplicates()\n\n                    # Apply the function to the column '1' and store the results in a new column 'Options_List'\n                    df_multiple['Options_List'] = df_multiple['Data Product'].apply(extract_options)\n\n                    Select_obj = _Multi_dropdown(df_multiple['Options_List'].to_list(), df_multiple['Top-level Directory'].to_list(), \"Specify 'Data Product' for unknown exports:\")\n\n                    selected_values = await Select_obj.display_widgets()\n\n\n                    if selected_values:\n\n                        df_multiple['Data Product'] = selected_values\n\n                        # Create a mapping from df1\n                        mapping = pd.Series(df_multiple['Data Product'].values, index=df_multiple['Top-level Directory']).to_dict()\n\n                        # Update df2['2'] based on the mapping\n                        df['Data Product'] = df['Top-level Directory'].map(mapping).combine_first(df['Data Product'])\n\n                        self._tables_available = df \n                        self._tables_backup = df \n\n        df = self._tables_available.copy()\n\n        # Filter rows where column '1' contains \"Mutliple_Options: \"\n        df_multiple = df[df['Data Product'].apply(contains_multiple_options)].copy()\n\n        if not df_multiple.empty:\n            asyncio.ensure_future(f(self,df,df_multiple))\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.__init__","title":"<code>__init__(hostname='s-f2112b8b980e44f9a.server.transfer.eu-west-1.amazonaws.com', username='D2vdz8elTWKyuOcC2kMSnw', port=22, privatekey=None)</code>","text":"<p>Constructor Method</p> <p>### Sftp Class Object Variables       - <code>connection</code>: None or pysftp.Connection object. Represents the current SFTP connection.</p> <ul> <li><code>hostname</code>: str. Hostname of the SFTP server (Default connects to CBS SFTP server).</li> <li><code>username</code>: str. Username for authentication (Default connects to CBS SFTP server).</li> <li><code>privatekey</code>: str or None. Path to the private key file for authentication (Valid private key is needed to access SFTP server)</li> <li> <p><code>port</code>: int. Port number for the SFTP connection (default is 22).</p> </li> <li> <p><code>output_format</code>: list of str. List of supported output file formats (e.g., ['.csv', '.parquet']).</p> </li> <li><code>file_size_mb</code>: int. Maximum file size in MB for before splitting output files..</li> <li><code>delete_files</code>: bool. Flag indicating whether to delete processed files.</li> <li><code>concat_files</code>: bool. Flag indicating whether to concatenate processed files.</li> <li><code>select_cols</code>: list or None. List of columns to select during file operations.</li> <li><code>query</code>: str, fnc or None. Query string or function for filtering data.</li> <li><code>query_args</code>: list or None. list of arguments for the query string or function.</li> <li><code>dfs</code>: None or DataFrame. Stores concatenated DataFrames if concatenation is enabled.</li> </ul> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def __init__(self, hostname:str = \"s-f2112b8b980e44f9a.server.transfer.eu-west-1.amazonaws.com\", username:str = \"D2vdz8elTWKyuOcC2kMSnw\", port:int = 22, privatekey:str = None):\n\"\"\"Constructor Method\n\n     ### Sftp Class Object Variables        \n    - `connection`: None or pysftp.Connection object. Represents the current SFTP connection.\n\n    - `hostname`: str. Hostname of the SFTP server (Default connects to CBS SFTP server).\n    - `username`: str. Username for authentication (Default connects to CBS SFTP server).\n    - `privatekey`: str or None. Path to the private key file for authentication (Valid private key is needed to access SFTP server)\n    - `port`: int. Port number for the SFTP connection (default is 22).\n\n    - `output_format`: list of str. List of supported output file formats (e.g., ['.csv', '.parquet']).\n    - `file_size_mb`: int. Maximum file size in MB for before splitting output files..\n    - `delete_files`: bool. Flag indicating whether to delete processed files.\n    - `concat_files`: bool. Flag indicating whether to concatenate processed files.\n    - `select_cols`: list or None. List of columns to select during file operations.\n    - `query`: str, fnc or None. Query string or function for filtering data.\n    - `query_args`: list or None. list of arguments for the query string or function.\n    - `dfs`: None or DataFrame. Stores concatenated DataFrames if concatenation is enabled.\n\n    \"\"\"\n\n    # Set connection object to None (initial value)\n    self.connection: object = None\n    self.hostname: str = hostname\n    self.username: str = username\n    self.privatekey: str = privatekey\n    self.port: int = port\n    self._cnopts = pysftp.CnOpts()\n    self._cnopts.hostkeys = None\n\n    self.output_format: list =  ['.csv'] \n    self.file_size_mb:int = 500\n    self.delete_files: bool = False\n    self.concat_files: bool = True\n    self._select_cols: list = None \n    self.query = None\n    self.query_args: list = None\n    self._bvd_list: list = [None,None,None]\n    self._time_period: list = [None,None,None,\"remove\"]\n    self.dfs = None\n\n    self._local_path: str = None\n    self._local_files: list = []\n\n    self._remote_path: str = None\n    self._remote_files: list = []\n\n    self._tables_available = None\n    self._tables_backup = None\n    self._set_data_product:str = None\n    self._time_stamp:str = None\n    self._set_table:str = None\n    self._table_dictionary = None\n    self._table_dates = None\n    self._download_finished = None\n\n    if hasattr(os, 'fork'):\n        self._pool_method = 'fork'\n        self._max_path_length = 256\n    else:\n        self._pool_method = 'threading'\n        self._max_path_length = 256\n\n    if sys.platform.startswith('linux'):\n        self._max_path_length = 4096\n    elif sys.platform == 'darwin':\n        self._max_path_length = 1024\n    elif sys.platform == 'win32':\n        self._max_path_length = 256\n\n    self.tables_available(save_to=False)\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.connect","title":"<code>connect()</code>","text":"<p>Establish an SFTP connection.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance.</p> <p>Returns: - SFTP connection object.</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def connect(self):\n\"\"\"\n    Establish an SFTP connection.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n\n    Returns:\n    - SFTP connection object.\n    \"\"\"\n    sftp = pysftp.Connection(host=self.hostname , username=self.username ,port = self.port ,private_key=self.privatekey, cnopts=self._cnopts)\n    return sftp\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.copy_obj","title":"<code>copy_obj()</code>","text":"<p>Create a deep copy of the current Sftp instance with optional updates.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance.</p> <p>Returns: - Deep copy of the current Sftp instance with optional updates.</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def copy_obj(self):\n\"\"\"\n    Create a deep copy of the current Sftp instance with optional updates.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n\n    Returns:\n    - Deep copy of the current Sftp instance with optional updates.\n    \"\"\"\n\n    new_obj= Sftp(hostname = self.hostname, username = self.username, port = 22, privatekey= self.privatekey) \n\n    #new_obj = copy.deepcopy(self)\n\n    new_obj.select_data()\n    #new_obj.bvd_list = None\n    #new_obj.time_period = None\n    #new_obj.select_cols = None\n\n    return new_obj\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.get_column_names","title":"<code>get_column_names(save_to=False, files=None)</code>","text":"<p>Retrieve column names from a DataFrame or dictionary and save them to a file.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance. - <code>save_to</code> (str, optional): Format to save results (default is CSV). - <code>files</code> (list, optional): List of files to retrieve column names from.</p> <p>Returns: - List of column names or None if no valid source is provided.</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def get_column_names(self,save_to:str=False, files = None):\n\"\"\"\n    Retrieve column names from a DataFrame or dictionary and save them to a file.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n    - `save_to` (str, optional): Format to save results (default is CSV).\n    - `files` (list, optional): List of files to retrieve column names from.\n\n    Returns:\n    - List of column names or None if no valid source is provided.\n    \"\"\"\n\n    def from_dictionary(self):\n        if self.set_table is not None: \n            df = self.search_dictionary(save_to=False)\n            column_names = df['Column'].to_list()\n            return column_names\n        else: \n            return None\n    def from_files(self,files):\n        if files is None and self.remote_files is None:   \n            raise ValueError(\"No files were added\")\n        elif files is None and self.remote_files is not None:\n            files = self.remote_files   \n\n        try:\n            file,_ = self._check_args([files[0]])\n            file, _ = self._get_file(file[0])\n            parquet_file = pq.ParquetFile(file)\n            # Get the column names\n            column_names = parquet_file.schema.names\n            return column_names\n        except ValueError as e:\n            print(e)\n            return None\n\n    if files is not None:\n        column_names = from_files(self,files)\n    else:       \n        column_names = from_dictionary(self)    \n\n    if column_names is not None:\n        df = pd.DataFrame({'Column_Names': column_names})\n        _save_to(df,'column_names',save_to)\n\n    return column_names\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.process_all","title":"<code>process_all(files=None, destination=None, num_workers=-1, select_cols=None, date_query=None, bvd_query=None, query=None, query_args=None, pool_method=None)</code>","text":"<p>Read and process files into a DataFrame with optional filtering and parallel processing.</p> <p>This method reads multiple files into Pandas DataFrames, optionally selecting specific columns and applying filters, either sequentially or in parallel.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance. - <code>files</code> (list, optional): List of files to process. Defaults to <code>self.remote_files</code>. - <code>destination</code> (str, optional): Destination path for processed files. - <code>num_workers</code> (int, optional): Number of workers for parallel processing. Default is -1 (auto-determined). - <code>select_cols</code> (list, optional): Columns to select from files. Default is <code>self._select_cols</code>. - <code>date_query</code>: (optional): Date query for filtering data. Default is <code>self.time_period</code>. - <code>bvd_query</code>: (optional): BVD query for filtering data. Default is <code>self._bvd_list[2]</code>. - <code>query</code> (str, optional): Query for additional filtering of data. - <code>query_args</code> (list, optional): Arguments for the query.</p> <p>Returns: - <code>dfs</code>: List of Pandas DataFrames with selected columns and filtered data. - <code>file_names</code>: List of file names processed.</p> <p>Notes: - If <code>select_cols</code> is provided, it is validated against expected formats. - Uses parallel processing if <code>num_workers</code> is greater than 1. - Handles file concatenation and deletion based on instance attributes (<code>concat_files</code>, <code>delete_files</code>). - Prints current working directory if <code>self.delete_files</code> is <code>True</code>.</p> <p>Raises: - <code>ValueError</code>: If validation of arguments (<code>files</code>, <code>destination</code>, <code>flag</code>) fails.</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def process_all(self, files:list = None,destination:str = None, num_workers:int = -1, select_cols: list = None , date_query = None, bvd_query = None, query = None, query_args:list = None,pool_method = None):\n\"\"\"\n    Read and process files into a DataFrame with optional filtering and parallel processing.\n\n    This method reads multiple files into Pandas DataFrames, optionally selecting specific columns and\n    applying filters, either sequentially or in parallel.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n    - `files` (list, optional): List of files to process. Defaults to `self.remote_files`.\n    - `destination` (str, optional): Destination path for processed files.\n    - `num_workers` (int, optional): Number of workers for parallel processing. Default is -1 (auto-determined).\n    - `select_cols` (list, optional): Columns to select from files. Default is `self._select_cols`.\n    - `date_query`: (optional): Date query for filtering data. Default is `self.time_period`.\n    - `bvd_query`: (optional): BVD query for filtering data. Default is `self._bvd_list[2]`.\n    - `query` (str, optional): Query for additional filtering of data.\n    - `query_args` (list, optional): Arguments for the query.\n\n    Returns:\n    - `dfs`: List of Pandas DataFrames with selected columns and filtered data.\n    - `file_names`: List of file names processed.\n\n    Notes:\n    - If `select_cols` is provided, it is validated against expected formats.\n    - Uses parallel processing if `num_workers` is greater than 1.\n    - Handles file concatenation and deletion based on instance attributes (`concat_files`, `delete_files`).\n    - Prints current working directory if `self.delete_files` is `True`.\n\n    Raises:\n    - `ValueError`: If validation of arguments (`files`, `destination`, `flag`) fails.\n    \"\"\"\n\n    files = files or self.remote_files\n    date_query = date_query or self.time_period\n    bvd_query = bvd_query or self._bvd_list[2]\n    query = query or self.query\n    query_args = query_args or self.query_args\n    select_cols = select_cols or self._select_cols\n\n    # To handle executing when download_all() have not finished!\n    if self._download_finished is False and all(file in self._remote_files for file in files): \n        start_time = time.time()\n        timeout = 5\n        files_not_ready =  not all(file in self.local_files for file in files) \n        while files_not_ready:\n            time.sleep(0.1)\n            files_not_ready =  not all(file in self.local_files for file in files)\n            if time.time() - start_time &gt;= timeout:\n                print(f\"Files have not finished downloading within the timeout period of {timeout} seconds.\")\n                return None, None\n\n        self._download_finished =True \n\n    if select_cols is not None:\n        select_cols = _check_list_format(select_cols,self._bvd_list[1],self._time_period[2])\n\n    try:\n        flag =  any([select_cols, query, all(date_query),bvd_query]) \n        files, destination = self._check_args(files,destination,flag)\n    except ValueError as e:\n        print(e)\n        return None\n\n    if isinstance(num_workers, (int, float, complex)):\n        num_workers = int(num_workers) \n    else: \n        num_workers = -1\n\n    if num_workers &lt; 1:\n        num_workers =int(psutil.virtual_memory().total/ (1024 ** 3)/12)\n\n    # Read multithreaded\n    if num_workers != 1 and len(files) &gt; 1:\n        def batch_processing():\n            def batch_list(input_list, batch_size):\n\"\"\"Splits the input list into batches of a given size.\"\"\"\n                batches = []\n                for i in range(0, len(input_list), batch_size):\n                    batches.append(input_list[i:i + batch_size])\n                return batches\n\n            batches = batch_list(files,num_workers)\n\n            lists = []\n\n            print(f'Processing {len(files)} files in Parallel')\n\n            for index, batch in enumerate(batches,start=1):\n                print(f\"Processing Batch {index} of {len(batches)}\")\n                print(f\"------ First file: '{batch[0]}'\")  \n                print(f\"------ Last file : '{batch[-1]}'\")               \n                params_list = [(file, destination, select_cols, date_query, bvd_query, query, query_args) for file in batch]\n                list_batch = _run_parallel(fnc=self._process_parallel,params_list=params_list,n_total=len(batch),num_workers=num_workers,pool_method=pool_method ,msg='Processing')\n                lists.extend(list_batch)\n\n\n            file_names = [elem[1] for elem in lists]\n            file_names = [file_name[0] for file_name in file_names if file_name is not None]\n\n            dfs =  [elem[0] for elem in lists]\n            dfs = [df for df in dfs if df is not None]\n\n            flags =  [elem[2] for elem in lists]\n\n            return dfs, file_names, flags\n\n        dfs, file_names, flags = batch_processing()\n\n    else: # Read Sequential\n        print(f'Processing  {len(files)} files in sequence')\n        dfs, file_names, flags = self._process_sequential(files, destination, select_cols, date_query, bvd_query, query, query_args,num_workers)\n\n    flag =  all(flags) \n\n    if (not self.concat_files and not flag) or len(dfs) == 0:\n            self.dfs = None\n    elif self.concat_files and not flag:\n\n        # Concatenate and save\n        self.dfs, file_names = _save_chunks(dfs=dfs,file_name=destination,output_format=self.output_format,file_size=self.file_size_mb, num_workers=num_workers)\n\n    return self.dfs, file_names\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.process_one","title":"<code>process_one(save_to=False, files=None, n_rows=1000)</code>","text":"<p>Retrieve a sample of data from a table and save it to a file.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance. - <code>files</code> (list, optional): List of files to process. Defaults to <code>self.remote_files</code>. - <code>save_to</code> (str, optional): Format to save sample data (default is CSV). - <code>n_rows</code> (int, optional): Number of rows to retrieve (default is 1000).</p> <p>Returns: - Pandas Dateframe with output</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def process_one(self,save_to=False,files = None,n_rows:int=1000):\n\"\"\"\n    Retrieve a sample of data from a table and save it to a file.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n    - `files` (list, optional): List of files to process. Defaults to `self.remote_files`.\n    - `save_to` (str, optional): Format to save sample data (default is CSV).\n    - `n_rows` (int, optional): Number of rows to retrieve (default is 1000).\n\n    Returns:\n    - Pandas Dateframe with output\n    \"\"\"\n\n    if files is None:\n        if self._set_data_product is None or self._set_table is None:\n            self.select_data()\n        files = [self.remote_files[0]]\n    elif isinstance(files,int):\n        files = [files]    \n\n    df, files = self.process_all(files = files,num_workers=len(files))\n\n    if df is None and files is not None:\n        dfs = []\n        for file in files:\n            df  = _load_table(file)\n            dfs.append(df)\n        df = pd.concat(dfs, ignore_index=True)\n        df = df.head(n_rows)\n        _save_to(df,'process_one',save_to) \n    elif not df.empty and files is not None:\n        df = df.head(n_rows)\n        print(f\"Results have been saved to '{files}'\")\n    elif df.empty:  \n        print(\"No rows were retained\")  \n    return df\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.search_country_codes","title":"<code>search_country_codes(search_word=None, search_cols={'Country': True, 'Code': True})</code>","text":"<p>Search for country codes matching a search term.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance. - <code>search_word</code> (str, optional): Term to search for country codes. - <code>search_cols</code> (dict, optional): Dictionary indicating columns to search (default is {'Country':True,'Code':True}).</p> <p>Returns: - Pandas Dataframe of country codes matching the search term</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def search_country_codes(self,search_word = None,search_cols={'Country':True,'Code':True}):        \n\"\"\"\n    Search for country codes matching a search term.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n    - `search_word` (str, optional): Term to search for country codes.\n    - `search_cols` (dict, optional): Dictionary indicating columns to search (default is {'Country':True,'Code':True}).\n\n    Returns:\n    - Pandas Dataframe of country codes matching the search term\n    \"\"\"\n\n    df = _country_codes()\n    if search_word is not None:\n\n        base_string = \"`{col}`.str.contains('{{search_word}}', case=False, na=False,regex=False)\"\n        search_conditions = \" | \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n        final_string = search_conditions.format(search_word=search_word)\n\n        df = df.query(final_string)\n\n        if df.empty:\n            base_string = \"'{col}'\"\n            search_conditions = \" , \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n            print(\"No such 'search word' was detected across columns: \" + search_conditions)\n            return df\n        else:\n            print(f\"The folloiwng query was executed:\" + final_string)\n\n    return df    \n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.search_dictionary","title":"<code>search_dictionary(save_to=False, search_word=None, search_cols={'Data Product': True, 'Table': True, 'Column': True, 'Definition': True}, letters_only=False, extact_match=False, data_product=None, table=None)</code>","text":"<p>Search for a term in a column/variable dictionary and save results to a file.</p> <p>Args: - <code>self</code>: Implicit reference to the instance. - <code>save_to</code> (str, optional): Format to save results. If False, results are not saved (default is False). - <code>search_word</code> (str, optional): Search term. If None, no term is searched. - <code>search_cols</code> (dict, optional): Dictionary indicating which columns to search. Columns are 'Data Product', 'Table', 'Column', and 'Definition' with default value as True for each. - <code>letters_only</code> (bool, optional): If True, search only for alphabetic characters in the search term (default is False). - <code>exact_match</code> (bool, optional): If True, search for an exact match of the search term. Otherwise, search for partial matches (default is False). - <code>data_product</code> (str, optional): Specific data product to filter results by. If None, no filtering by data product (default is None). - <code>table</code> (str, optional): Specific table to filter results by. If None, no filtering by table (default is None).</p> <p>Returns: - pandas.DataFrame: A DataFrame containing the search results. If no results are found, an empty DataFrame is returned.</p> <p>Notes: - If <code>data_product</code> is provided and does not match any records, a message is printed and an empty DataFrame is returned. - If <code>table</code> is provided and does not match any records, it attempts to perform a case-insensitive partial match search. - If <code>search_word</code> is provided and no matches are found, a message is printed indicating no results were found. - If <code>letters_only</code> is True, the search term is processed to include only alphabetic characters before searching. - If <code>save_to</code> is specified, the query results are saved in the format specified.</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def search_dictionary(self,save_to:str=False, search_word = None,search_cols={'Data Product':True,'Table':True,'Column':True,'Definition':True}, letters_only:bool=False,extact_match:bool=False, data_product = None, table = None):\n\n\"\"\"\n    Search for a term in a column/variable dictionary and save results to a file.\n\n    Args:\n    - `self`: Implicit reference to the instance.\n    - `save_to` (str, optional): Format to save results. If False, results are not saved (default is False).\n    - `search_word` (str, optional): Search term. If None, no term is searched.\n    - `search_cols` (dict, optional): Dictionary indicating which columns to search. Columns are 'Data Product', 'Table', 'Column', and 'Definition' with default value as True for each.\n    - `letters_only` (bool, optional): If True, search only for alphabetic characters in the search term (default is False).\n    - `exact_match` (bool, optional): If True, search for an exact match of the search term. Otherwise, search for partial matches (default is False).\n    - `data_product` (str, optional): Specific data product to filter results by. If None, no filtering by data product (default is None).\n    - `table` (str, optional): Specific table to filter results by. If None, no filtering by table (default is None).\n\n    Returns:\n    - pandas.DataFrame: A DataFrame containing the search results. If no results are found, an empty DataFrame is returned.\n\n    Notes:\n    - If `data_product` is provided and does not match any records, a message is printed and an empty DataFrame is returned.\n    - If `table` is provided and does not match any records, it attempts to perform a case-insensitive partial match search.\n    - If `search_word` is provided and no matches are found, a message is printed indicating no results were found.\n    - If `letters_only` is True, the search term is processed to include only alphabetic characters before searching.\n    - If `save_to` is specified, the query results are saved in the format specified.\n    \"\"\"\n\n\n    if data_product is None and self.set_data_product is not None:\n        data_product = self.set_data_product\n\n        if table is None and self.set_table is not None:\n            table = self.set_table\n\n    if self._table_dictionary is None:\n        self._table_dictionary = _table_dictionary()        \n    df = self._table_dictionary\n    df = df[df['Data Product'].isin(self._tables_backup['Data Product'].drop_duplicates())]\n\n    if data_product is not None:\n        df_product = df.query(f\"`Data Product` == '{data_product}'\")\n        if df_product.empty:\n            print(\"No such Data Product was found. Please set right data product\")\n            return df_product\n        else:\n            df = df_product\n        search_cols['Data Product'] = False\n    if table is not None:\n        df_table = df.query(f\"`Table` == '{table}'\")\n        if df_table.empty:   \n            df_table = df.query(f\"`Table`.str.contains('{table}', case=False, na=False,regex=False)\")\n            if df_table.empty:\n                print(\"No such Table was found. Please set right table\")\n                return df_table\n        search_cols['Table'] = False\n        df = df_table \n\n    if search_word is not None:\n        if letters_only:\n            df_backup = df.copy()\n            search_word = _letters_only_regex(search_word)\n            df = df.map(_letters_only_regex)\n\n        if extact_match:\n            base_string = \"`{col}` ==  '{{search_word}}'\"\n        else:\n            base_string = \"`{col}`.str.contains('{{search_word}}', case=False, na=False,regex=False)\"\n\n        search_conditions = \" | \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n        final_string = search_conditions.format(search_word=search_word)\n\n        df = df.query(final_string)\n\n        if df.empty:\n            base_string = \"'{col}'\"\n            search_conditions = \" , \".join(base_string.format(col=col) for col, include in search_cols.items() if include)\n            print(\"No such 'search word' was detected across columns: \" + search_conditions)\n            return df\n\n        if letters_only:\n            df = df_backup.loc[df.index]\n\n        if save_to:\n            print(f\"The folloiwng query was executed:\" + final_string)\n\n    _save_to(df,'dict_search',save_to)\n\n    return df    \n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.select_columns","title":"<code>select_columns()</code>","text":"<p>Asynchronously select and set columns for a specified data product and table using interactive widgets.</p> <p>This method performs the following steps: 1. Checks if the data product and table are set. If not, it calls <code>select_data()</code> to set them. 2. Searches the dictionary for columns corresponding to the set data product and table. 3. Displays an interactive widget for the user to select columns based on their names and definitions. 4. Sets the selected columns to <code>self._select_cols</code> and prints the selected columns.</p> <p>If no columns are found for the specified table, a <code>ValueError</code> is raised.</p> <p>Args: - <code>self</code>: Implicit reference to the instance.</p> <p>Notes: - This method uses <code>asyncio.ensure_future</code> to run the asynchronous function <code>f</code> which handles the widget interaction. - The function <code>f</code> combines column names and definitions for display, maps selected items to their indices, and then extracts the selected columns based on these indices.</p> <p>Raises: - <code>ValueError</code>: If no columns are found for the specified table.</p> Example <p>self.select_columns()</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def select_columns(self):\n\"\"\"\n    Asynchronously select and set columns for a specified data product and table using interactive widgets.\n\n    This method performs the following steps:\n    1. Checks if the data product and table are set. If not, it calls `select_data()` to set them.\n    2. Searches the dictionary for columns corresponding to the set data product and table.\n    3. Displays an interactive widget for the user to select columns based on their names and definitions.\n    4. Sets the selected columns to `self._select_cols` and prints the selected columns.\n\n    If no columns are found for the specified table, a `ValueError` is raised.\n\n    Args:\n    - `self`: Implicit reference to the instance.\n\n    Notes:\n    - This method uses `asyncio.ensure_future` to run the asynchronous function `f` which handles the widget interaction.\n    - The function `f` combines column names and definitions for display, maps selected items to their indices,\n    and then extracts the selected columns based on these indices.\n\n    Raises:\n    - `ValueError`: If no columns are found for the specified table.\n\n    Example:\n        self.select_columns()\n    \"\"\"\n    async def f(self,column, definition):\n\n        combined = [f\"{col}  -----  {defn}\" for col, defn in zip(column, definition)]\n\n        Select_obj = _SelectMultiple(combined,'Columns:',\"Select Table Columns\")\n        selected_list = await Select_obj.display_widgets()\n        if selected_list is not None:\n\n            # Create a dictionary to map selected strings to their indices in the combined list\n            indices = {item: combined.index(item) for item in selected_list if item in combined}\n\n            # Extract selected columns based on indices\n            selected_list = [column[indices[item]] for item in selected_list if item in indices]\n            self._select_cols = selected_list\n            self._select_cols = _check_list_format(self._select_cols,self._bvd_list[1],self._time_period[2])\n            print(f\"The following columns have been selected: {self._select_cols}\")\n\n    if self._set_data_product is None or self._set_table is None:\n        self.select_data()\n\n    table_cols = self.search_dictionary(data_product=self.set_data_product,table = self._set_table,save_to=False)\n\n    if table_cols.empty:\n        self._select_cols = None\n        raise ValueError(\"No columns were found for this table\")\n\n    column = table_cols['Column'].tolist()\n    definition = table_cols['Definition'].tolist()\n\n    asyncio.ensure_future(f(self, column, definition)) \n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.select_data","title":"<code>select_data()</code>","text":"<p>Asynchronously select and set the data product and table using interactive widgets.</p> <p>This method initializes an instance of <code>_SelectData</code> with <code>_tables_backup</code>, displays interactive widgets to allow the user to select a data product and table, and sets these selections to <code>self.set_data_product</code> and <code>self.set_table</code>, respectively. It also prints the selected data product and table.</p> <p>The method ensures that <code>_tables_available</code> or <code>_tables_backup</code> is populated by calling <code>tables_available()</code> if they are not already set.</p> <p>Notes: - This method uses <code>asyncio.ensure_future</code> to run the asynchronous function <code>f</code> which handles the widget interaction.</p> Example <p>self.select_data()</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def select_data(self):\n\"\"\"\n    Asynchronously select and set the data product and table using interactive widgets.\n\n    This method initializes an instance of `_SelectData` with `_tables_backup`, displays interactive widgets\n    to allow the user to select a data product and table, and sets these selections to `self.set_data_product`\n    and `self.set_table`, respectively. It also prints the selected data product and table.\n\n    The method ensures that `_tables_available` or `_tables_backup` is populated by calling `tables_available()`\n    if they are not already set.\n\n    Notes:\n    - This method uses `asyncio.ensure_future` to run the asynchronous function `f` which handles the widget interaction.\n\n    Example:\n        self.select_data()\n    \"\"\"\n\n    async def f(self):\n        Select_obj = _SelectData(self._tables_backup,'Select Data Product and Table')\n        selected_product, selected_table = await Select_obj.display_widgets()\n\n        df = self._tables_backup.copy()\n        df = df[['Data Product','Table','Base Directory','Top-level Directory']].query(f\"`Data Product` == '{selected_product}' &amp; `Table` == '{selected_table}'\").drop_duplicates()\n\n        if len(df) &gt; 1:\n            options = df['Top-level Directory'].tolist()\n            product = df['Data Product'].drop_duplicates().tolist()\n            msg = f\"Multiple data products match '{product[0]}'. Please set right data product:\" \n            self._set_table = selected_table\n            self._set_data_product = selected_product\n            _select_list('_SelectList',options,f\"'{product[0]}':\",msg,_select_product,[df,self])\n        elif len(df) == 1:\n            self.set_data_product = selected_product\n            self.set_table = selected_table\n            print(f\"{self.set_data_product} was set as Data Product\")\n            print(f\"{self.set_table} was set as Table\")\n\n    self._download_finished = None \n\n    asyncio.ensure_future(f(self))\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.table_dates","title":"<code>table_dates(save_to=False, data_product=None, table=None)</code>","text":"<p>Retrieve and save the available date columns for a specified data product and table.</p> <p>This method performs the following steps: 1. Ensures that the available tables and table dates are loaded. 2. Filters the dates data by the specified data product and table, if provided. 3. Optionally saves the filtered results to a specified format.</p> <p>Args: - <code>self</code>: Implicit reference to the instance. - <code>save_to</code> (str, optional): Format to save results. If False, results are not saved (default is False). - <code>data_product</code> (str, optional): Specific data product to filter results by. If None, defaults to <code>self.set_data_product</code>. - <code>table</code> (str, optional): Specific table to filter results by. If None, defaults to <code>self.set_table</code>.</p> <p>Returns: - pandas.DataFrame: A DataFrame containing the filtered dates for the specified data product and table. If no results are found, an empty DataFrame is returned.</p> <p>Notes: - If <code>data_product</code> is provided and does not match any records, a message is printed and an empty DataFrame is returned. - If <code>table</code> is provided and does not match any records, it attempts to perform a case-insensitive partial match search. - If <code>save_to</code> is specified, the query results are saved in the format specified.</p> Example <p>df = self.table_dates(save_to='csv', data_product='Product1', table='TableA')</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def table_dates(self,save_to:str=False, data_product = None,table = None):\n\"\"\"\n    Retrieve and save the available date columns for a specified data product and table.\n\n    This method performs the following steps:\n    1. Ensures that the available tables and table dates are loaded.\n    2. Filters the dates data by the specified data product and table, if provided.\n    3. Optionally saves the filtered results to a specified format.\n\n    Args:\n    - `self`: Implicit reference to the instance.\n    - `save_to` (str, optional): Format to save results. If False, results are not saved (default is False).\n    - `data_product` (str, optional): Specific data product to filter results by. If None, defaults to `self.set_data_product`.\n    - `table` (str, optional): Specific table to filter results by. If None, defaults to `self.set_table`.\n\n    Returns:\n    - pandas.DataFrame: A DataFrame containing the filtered dates for the specified data product and table. If no results are found, an empty DataFrame is returned.\n\n    Notes:\n    - If `data_product` is provided and does not match any records, a message is printed and an empty DataFrame is returned.\n    - If `table` is provided and does not match any records, it attempts to perform a case-insensitive partial match search.\n    - If `save_to` is specified, the query results are saved in the format specified.\n\n    Example:\n        df = self.table_dates(save_to='csv', data_product='Product1', table='TableA')\n    \"\"\"    \n\n    if data_product is None and self.set_data_product is not None:\n        data_product = self.set_data_product\n\n        if table is None and self.set_table is not None:\n            table = self.set_table\n\n    if self._table_dates is None:\n        self._table_dates = _table_dates()        \n    df = self._table_dates\n    df = df[df['Data Product'].isin(self._tables_backup['Data Product'].drop_duplicates())]\n\n    if data_product is not None:\n        df_product = df.query(f\"`Data Product` == '{data_product}'\")\n        if df_product.empty:\n            print(\"No such Data Product was found. Please set right data product\")\n            return df_product\n        else:\n            df = df_product\n    if table is not None:\n        df_table = df.query(f\"`Table` == '{table}'\")\n        if df_table.empty:\n            df_table = df.query(f\"`Table`.str.contains('{table}', case=False, na=False,regex=False)\")\n            if df_table.empty:\n                print(\"No such Table was found. Please set right table\")\n                return df_table\n        df = df_table\n\n    _save_to(df,'date_cols_search',save_to)\n\n    return df    \n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.tools.Sftp.tables_available","title":"<code>tables_available(save_to=False, reset=False)</code>","text":"<p>Retrieve available SFTP data products and tables and save them to a file.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance. - <code>save_to</code> (str, optional): Format to save results (default is CSV). - <code>reset</code> (bool, optional): Reset flag to force refreshing data products and tables.</p> <p>Returns: - Pandas DataFrame with the available SFTP data products and tables.</p> Source code in <code>moodys_datahub\\tools.py</code> <pre><code>def tables_available(self,save_to:str=False,reset:bool=False):\n\"\"\"\n    Retrieve available SFTP data products and tables and save them to a file.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n    - `save_to` (str, optional): Format to save results (default is CSV).\n    - `reset` (bool, optional): Reset flag to force refreshing data products and tables.\n\n    Returns:\n    - Pandas DataFrame with the available SFTP data products and tables.\n    \"\"\"\n\n    if self._tables_available is None and self._tables_backup is None:\n        self._tables_available,to_delete = self._table_overview()\n        self._tables_backup = self._tables_available.copy()\n\n        if self.hostname == \"s-f2112b8b980e44f9a.server.transfer.eu-west-1.amazonaws.com\" and len(to_delete) &gt; 0:\n            print(\"------------------  DELETING OLD EXPORTS FROM SFTP\")\n            self._remove_exports(to_delete)\n\n    elif reset:\n        self._tables_available = self._tables_backup.copy()\n\n    # Specify unknown data product exports\n    self._specify_data_products()\n\n    _save_to(self._tables_available,'tables_available',save_to)\n\n    return self._tables_available.copy()\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.utils.fuzzy_match","title":"<code>fuzzy_match(input_strings, df, num_workers=1, match_column=None, return_column=None, cut_off=50, remove_str=None)</code>","text":"<p>Perform fuzzy string matching with a list of input strings against a specific column in a DataFrame.</p> <p>Parameters: - input_strings (list): A list of strings for fuzzy matching. - df (pandas.DataFrame): The DataFrame containing the target columns. - match_column (str): The name of the column to match against. - return_column (str): The name of the column from which to return the matching value. - remove_str (list): A list of substrings to remove from the choices.</p> <p>Returns: - pandas.DataFrame: A DataFrame containing the original data along with the best match, its score, and the matching value from another column.</p> Source code in <code>moodys_datahub\\utils.py</code> <pre><code>def fuzzy_match(input_strings:list, df: pd.DataFrame, num_workers:int = 1, match_column:str =None , return_column:str = None, cut_off:int = 50 , remove_str:list = None):\n\"\"\"\n    Perform fuzzy string matching with a list of input strings against a specific column in a DataFrame.\n\n    Parameters:\n    - input_strings (list): A list of strings for fuzzy matching.\n    - df (pandas.DataFrame): The DataFrame containing the target columns.\n    - match_column (str): The name of the column to match against.\n    - return_column (str): The name of the column from which to return the matching value.\n    - remove_str (list): A list of substrings to remove from the choices.\n\n    Returns:\n    - pandas.DataFrame: A DataFrame containing the original data along with the best match, its score, and the matching value from another column.\n    \"\"\"\n\n    def remove_substrings(choices, substrings):\n        for substring in substrings:\n            choices = [choice.replace(substring.lower(), '') for choice in choices]\n        return choices\n\n    matches = []\n\n    choices = [choice.lower() for choice in df[match_column].tolist()]\n    input_strings = [input.lower() for input in input_strings]\n\n    if remove_str is not None:\n        choices = remove_substrings(choices, remove_str)\n\n    if isinstance(num_workers, (int, float, complex))and num_workers != 1:\n        num_workers = int(num_workers) \n        if num_workers &lt; 0:\n            num_workers = int(cpu_count() - 2)\n        args_list = [(input_string, choices, cut_off, df, match_column, return_column) for input_string in input_strings]\n        pool = Pool(processes=num_workers)\n        matches = pool.map(_fuzzy_worker, args_list)\n        pool.close()\n        pool.join()\n    else:    \n        for input_string in input_strings:\n            args_list = (input_string, choices, cut_off, df, match_column, return_column)\n            input_string, match, score, match_value, return_value = _fuzzy_worker(args_list)\n            matches.append((input_string, match, score, match_value, return_value))\n\n    result_df = pd.DataFrame(matches, columns=['Search_string', 'BestMatch', 'Score', match_column, return_column])\n    return result_df\n\n\n    def replace_columns_with_na(df, replacement_value:str ='N/A'):\n        for column_name in df.columns:\n            if df[column_name].isna().all():\n                df[column_name] = replacement_value\n        return df\n\n    file_names = []\n    try:\n        for extension in output_format:\n            if extension == '.csv':\n                current_file = file_name + '.csv'\n                df.to_csv(current_file,index=False)\n            elif extension == '.xlsx':\n                current_file = file_name + '.xlsx'\n                df.to_excel(current_file,index=False)\n            elif extension == '.parquet':\n                current_file = file_name + '.parquet'\n                df.to_parquet(current_file)\n            elif extension == '.pickle':\n                current_file = file_name + '.pickle'\n                df.to_pickle(current_file) \n            elif extension == '.dta':\n                current_file = file_name + '.dta'              \n                df = replace_columns_with_na(df, replacement_value='N/A') # .dta format does not like empty columns so these are removed\n                df.to_stata(current_file)\n            file_names.append(current_file)\n\n    except PermissionError as e:\n        print(f\"PermissionError: {e}. Check if you have the necessary permissions to write to the specified location.\")\n\n        if not file_name.endswith('_copy'):\n            print(f'Saving \"{file_name}\" as \"{file_name}_copy\" instead')\n            current_file = _save_files(df,file_name + \"_copy\",output_format)\n            file_names.append(current_file)\n        else: \n            print(f'\"{file_name}\" was not saved')\n\n    return file_names\n\n\n\n\n\"\"\"Converts the title to lowercase and removes non-alphanumeric characters.\"\"\"\n    if isinstance(text,str):\n        return re.sub(r'[^a-zA-Z0-9]', '', text.lower())\n    else:\n        return text\n</code></pre>"},{"location":"moody-s_datahub/mkdocs/reference/#moodys_datahub.utils.year_distribution","title":"<code>year_distribution(df=None)</code>","text":"<p>Display the distribution of years in the data.</p> <p>Input Variables: - <code>self</code>: Implicit reference to the instance.</p> <p>Returns: - None</p> Source code in <code>moodys_datahub\\utils.py</code> <pre><code>def year_distribution(df=None):\n\"\"\"\n    Display the distribution of years in the data.\n\n    Input Variables:\n    - `self`: Implicit reference to the instance.\n\n    Returns:\n    - None\n    \"\"\"\n\n    if df is None:\n        print('No Dataframe (df) was detected')\n        return\n\n    columns_to_check = ['closing_date', 'information_date']\n    date_col = next((col for col in columns_to_check if col in df.columns), None)\n\n    if not date_col:\n        print('No valid date columns found')\n        return\n\n    # Convert the date column to datetime\n    df[date_col] = pd.to_datetime(df[date_col], format='%d-%m-%Y')\n\n    # Create a new column extracting the year\n    df['year'] = df[date_col].dt.year\n\n    year_counts = df['year'].value_counts().reset_index()\n    year_counts.columns = ['Year', 'Frequency']\n\n    # Sort by year\n    year_counts = year_counts.sort_values(by='Year')\n\n    # Calculate percentage\n    year_counts['Percentage'] = (year_counts['Frequency'] / year_counts['Frequency'].sum()) * 100\n\n    # Calculate total row\n    total_row = pd.DataFrame({'Year': ['Total'], 'Frequency': [year_counts['Frequency'].sum()],'Percentage':[year_counts['Percentage'].sum()]})\n\n    # Concatenate total row to the DataFrame\n    year_counts = pd.concat([year_counts, total_row])\n\n    # Display the table\n    print(year_counts)\n</code></pre>"}]}