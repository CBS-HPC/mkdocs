{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC &amp; Data Science Support at CBS","text":"<p>This is the GitHub repository for HPC &amp; Data Science Support at CBS. The team is dedicated to providing assistance and support to CBS researchers and students in their research utilizing the different HPC systems available at CBS. The repository contains various resources and information related to our services and activities.</p>"},{"location":"#activities","title":"Activities","text":"Tutorials &amp; User UtilitiesTeaching ActivitiesDevelopment of HPCDaily User SupportResearch Consultancy <p> The HPC &amp; Da Science Support team provides tutorials and user utilities to assist and support CBS researchers and students in their research utilizing the different HPC systems available at CBS. The tutorials and utilities covering various topics such as:</p> <ul> <li>Use cases for different HPC systems</li> <li>Efficient and secure data tranfer</li> <li>parallel computing</li> <li>environment management (e.g Conda)</li> </ul> <p>These tutorials offer step-by-step guidance, empowering users to effectively utilize HPC resources for their research. </p> <p> We conducts teaching activities through researcher and student webinars. Titles include \"High Performance Computing\", \"HPC &amp; Parallel Programming in R,\" and \"HPC &amp; Parallel Programming in Python\" with more in the pipeline.</p> <p>See \"Events\" section for more information.</p> <p>Upon receiving requests from course coordinators, we are also available to participate in teaching activities for courses at CBS.</p> <p> We are committed to the continuous development of HPC resources at CBS. This is both by ensuring that researchers have access to the right facilities, both short- and long-term, but also by providing a clear learning strategy for research to develop their HPC &amp; data science skillset.</p> <p> As Deic Front Office at CBS are we in charge off all communications with HPC system adminstrators (Back Office) and DeiC.</p> <p>Ideally, all user requests and troubleshooting should be send to the CBS Front Office(rdm@cbs.dk) as a Single Point of Contact (SPOC) where resulting tickets will be directed accordingly. </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p> <p> We provide consulting services to researchers and research projects, assisting them with their HPC requirements. Our support includes, but is not limited to, the following examples:</p> <ul> <li>HPC grant application guidance</li> <li>Assessing user needs for HPC resources</li> <li>Workflow and code optimization assistance</li> </ul> <p>By offering expert consultation, we help researchers identify and address their specific HPC needs, ensuring they can effectively utilize the available resources and optimize their workflows and code for maximum performance and efficiency.</p>"},{"location":"#most-relevant-links","title":"Most Relevant Links","text":"Online LearningUCloudRPythonSTATA / SAS / MatlabGPUsLarge Memory HPC <ul> <li>The Turing Way - Guide for Reproducible Research in Data Science </li> <li>Coderefinery - Introduction to version control with Git</li> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> <li>Kaggle.com: Online interactive data science courses</li> <li>Kaggle.com: Python</li> <li>Kaggle.com: Getting staRted in R: First Steps</li> <li>Datacarpentry - Introduction to Stata for Economics</li> </ul> <ul> <li>Getting Started with HPC (UCloud)</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>Synchronization to UCloud (Hosted by UCloud) </li> </ul> <ul> <li>Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library</li> <li>Use Conda on UCloud to manage R-packages</li> <li>SLURM Clusters on UCloud</li> </ul> <ul> <li>Use Conda on UCloud to manage Python-libraries</li> <li>SLURM Clusters on UCloud</li> </ul> <p>STATA</p> <ul> <li>Run Stata on UCloud</li> <li>Install Stata on UCloud</li> <li>Run Stata in jupyter-notebooks</li> <li>Run Stata on Type 3 </li> </ul> <p>SAS</p> <ul> <li>Run SAS on UCloud</li> </ul> <p>Matlab</p> <ul> <li>Run Matlab on UCloud</li> </ul> <ul> <li>Which GPU to Choose?</li> <li>Access GPUs on UCloud</li> <li>GPU Libraries for Python and R</li> <li>Conda: for easy workflow deployment on AAU GPU VMs</li> <li>Run Python and R jupyter notebooks on AAU VMs</li> <li>Setting up jupyter-notebook with GPUs on AAU using Docker images (Hosted by RUC)</li> <li>Pytorch: Train your deep-learning models on UCloud GPUs</li> <li>Tensorflow: Train your deep-learning models on UCloud GPUs</li> <li>RAPIDS-cuML: Train your Scikit-learn models on UCloud GPUs</li> </ul> <ul> <li>Getting Started with large memory HPC (UCloud)</li> <li>Type 3 user guide (from SDU)</li> <li>Run Stata on Type 3 </li> <li>Use Conda to manage Jupyterlab environments on Type 3</li> </ul>"},{"location":"#hpc-operational-status","title":"HPC Operational Status","text":"<ul> <li>TYPE 1 (UCloud)</li> <li>TYPE 3 (Hippo)</li> <li>TYPE 5 (LUMI)</li> </ul>"},{"location":"contact/","title":"Contact","text":"<ul> <li>Research &amp; Data Management @ CBS (rdm@cbs.dk)</li> <li>Kristoffer Gulmark Poulsen (kgp.lib@cbs.dk)</li> <li>Lars Nondal (ln.lib@cbs.dk)</li> </ul>"},{"location":"events/","title":"Teaching Events","text":"For ResearchersFor StudentsFor All <p>\u00a0\u00a023-11-14 @ 14.00-15.00: - Train your ML/AI Model on GPUs - [Download Slides]</p> <p>\u00a0\u00a023-11-06 @ 13.00-14.00: - HPC &amp; Parallel Programming in R - [Download Slides]</p> <p>\u00a0\u00a023-11-02 @ 13.00-14.00: - HPC &amp; Parallel Programming in Python - [Download Slides]</p> <p>\u00a0\u00a023-10-31 @ 14.00-15.00: - High Performance Computing - [Download Slides]</p> <p></p> <p>\u00a0\u00a023-10-27 @ 13.00-14.00: - HPC &amp; Parallel Programming in Python</p> <p>\u00a0\u00a023-10-26 @ 13.00-14.00: - HPC &amp; Parallel Programming in R - [Download Slides]</p> <p>\u00a0\u00a023-10-23 @ 13.00-14.00: - High Performance Computing - [Download Slides]</p> <p></p> <ul> <li>23-09-11 -Nordic HPC Workshops - Parallel Computing and AI with MATLAB:  11, 18, 25 &amp; 26 September  </li> </ul>"},{"location":"getresources/","title":"Get Resources","text":""},{"location":"getresources/#local-resources","title":"Local Resources","text":"<p>Twice a year CBS is awarded Local HPC ressources that can be freely distributed to our researchers and students. </p> <p>Currently, CBS primarily have Local Type 1 resources as the reflects our current user needs.</p> TYPE 1 (UCloud)TYPE 3 (Hippo)Other <p>Students</p> <ul> <li>When you need more/other resources you should contact RDM Support to discuss further. </li> </ul> <p>Researchers &amp; Staff</p> <ul> <li> <p>You apply from UCloud by sending a UCloud grant application. </p> </li> <li> <p>Information on machine type selection be found here. </p> </li> <li> <p>Otherwise please contact RDM Support.</p> </li> </ul> <p>Students</p> <ul> <li> <p>CBS students can only under very specific situation get access to Type 3 HPC. If this is of interrest you are welcome to contact RDM Support to discuss further.</p> </li> <li> <p>Please contact RDM Support if you would like to CBS to request Local resources to Type 3.</p> </li> </ul> <p>Researchers &amp; Staff</p> <ul> <li> <p>You apply from UCloud by sending a UCloud grant application. </p> </li> <li> <p>In 2023 CBS has a very limited Type 3 resource pool. Type 3 grant applications are therefore only granted after thorough evulation by the RDM Support.</p> </li> <li> <p>For more information please contact RDM Support.</p> </li> </ul> <p>Please contact RDM Support if you would like to CBS to request Local resources to Type 2 and 5.</p>"},{"location":"getresources/#sandbox-resources","title":"Sandbox Resources","text":"<p>CBS researchers wanting to test out HPC systems Type 2 to 5 can gain acess to sandbox ressources by contacting RDM Support. Find more information here.</p>"},{"location":"getresources/#grant-applications","title":"Grant Applications","text":"<p>Find an overview of currently open application rounds below. Please contact RDM Support as soon as possible if you consider applying as we can aid in the application process.</p> <ul> <li> <p>TYPE 1 to 3: Researcher can apply for the bi-annual application round for the national HPC resources. </p> </li> <li> <p>TYPE 5 &amp; other international HPC systems: Researcher can apply for resources at LUMI and other international HPC facilities. </p> </li> </ul>"},{"location":"getresources/#current-calls","title":"Current Calls","text":"<ul> <li>DeiC - Call H1-2024 Call for applications for access to the e- resources - (Deadline: September 5th, 2023)</li> </ul>"},{"location":"getresources/#external-links","title":"External Links","text":"<ul> <li>Apply for national HPC resources</li> <li>Acknowledge the use of national HPC </li> <li>The EuroCC Knowledge Pool (Hosted by DeiC)</li> </ul>"},{"location":"hpc/","title":"HPC Facilites","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>Type 3 \u2013 Large Memory HPC (Hippo)</li> <li>National HPC Facilities (DeiC)</li> <li>WRDS - Wharton Research Data Services</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> </ul>"},{"location":"news/","title":"News","text":"<ul> <li>23-10-25 - Whisper large language model from OpenAI for Transcription of audio files (UCloud)</li> <li>23-08-24 - Larger GPU machine available for UCloud</li> <li>23-07-24 - CodeRefinery workshop September 19-21 and 26-28, 2023</li> <li>23-07-14 - DeiC - Call H1-2024 Call for applications for access to the e- resources</li> <li>23-06-27 - Interactive HPC lives up to highest international standards with ISO 27001</li> <li>23-06-12 - New way to use SSH for accessing apps on Interactive HPC</li> <li>23-05-31 - UCloud Maintenance notice - 27/06/2023</li> <li>23-04-26 - UCloud scheduled maintenance between 8:00-10:00 on Wednesday 26/04/2023.</li> <li>23-04-12 - New milestone as DeiC Interactive HPC reaches 6,000 users</li> <li>23-03-17 - The cost of success \u2013 user overload on DeiC Interactive HPC</li> <li>23-03-15 - Launch of the DeiC Integration Portal</li> </ul>"},{"location":"tut_docs/","title":"Tutorials &amp; Documentation","text":""},{"location":"tut_docs/#cbs-tutorials","title":"CBS Tutorials","text":"<p>Type 1: CPU</p> <ul> <li>Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library</li> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul> <p>Type 1: GPU</p> <ul> <li>Access GPUs on UCloud</li> <li>Conda: for easy workflow deployment on AAU GPU VMs</li> <li>Run Python and R jupyter notebooks on AAU VMs</li> <li>Pytorch: Train your deep-learning models on UCloud GPUs</li> <li>Tensorflow: Train your deep-learning models on UCloud GPUs</li> <li>RAPIDS-cuML: Train your Scikit-learn models on UCloud GPUs</li> </ul>"},{"location":"tut_docs/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>SDU</p> <ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul> <p>AAU</p> <ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"tut_docs/#type-2-throughput","title":"TYPE 2 (Throughput)","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"tut_docs/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"tut_docs/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"tut_docs/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"HPC_Facilities/DeiC/","title":"National HPC Facilities","text":"<p>This page provides an overview of the national HPC facilities (Content is provided by DeiC). </p>"},{"location":"HPC_Facilities/DeiC/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>The type 1 system is mainly focused on interactive computing and easy access for users. The system is made of the YouGene cluster hosted at SDU. CBS staff and students can access the cluster resources via UCloud. </p> <p>Get for Type 1 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/DeiC/#type-2-throughput-hpc","title":"TYPE 2 (Throughput HPC)","text":"<p>Three Type 2 HPC systems are available (Computerome 2.0,GenomeDK and Sophia). This type of HPC system typically has a large number of cores which can be a mix between cost-effective and calculation-efficient units. Type 2 also has the ability to handle large amounts of data and its main focus is on high-throughput performance. </p> <p>Get for Type 2 resources here.</p> <p>More information is found here:</p> <p>Computerome 2.0  \u00a0\u00a0 | \u00a0\u00a0 GenomeDK \u00a0\u00a0 | \u00a0\u00a0 Sophia</p>"},{"location":"HPC_Facilities/DeiC/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<p>This is a large memory HPC. This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area. </p> <p>Type 3 is hosted and maintained at SDU. For the cluster specs check here. </p> <p>User guide can be found here. </p> <p>Get for Type 3 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/DeiC/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<p>Type 5 is the European pre-exascale supercomputer LUMI. LUMI is an abbreviation for \"Large Unified Modern Infrastructure\", and will be located in CSC's data center in Kajaani, Finland. LUMI is one of three pre-exascale supercomputers to be build as part of the European EuroHPC project.</p> <p>LUMI Capability HPC provides a similar setup to DeiC Throughput HPC but with increased possibilities by virtue of state-of-the-art hardware. Specifically the interconnections between compute nodes is designed to minimize latency thereby addressing the issue of communication induced latency in distributed-memory programs running on separate nodes. Additionally the user can obtain access to large amounts of disk space also with low-latency interconnects. In this way Capability HPC enables computations that are prohibitive with DeiC Throughput HPC due to communication latency. </p> <p>Get for Type 5 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/GrantApp/","title":"Applying for ressources in 6 Simple Steps","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/GrantApp/#step-1-select-apply-for-resources-on-the-ucloud-frontpage","title":"Step 1: Select \"Apply for resources\" on the UCloud frontpage","text":""},{"location":"HPC_Facilities/GrantApp/#step-2-select-apply-for-new-project-instead","title":"Step 2: Select \"Apply for new project instead\"","text":""},{"location":"HPC_Facilities/GrantApp/#step-3-provide-a-project-title-and-choose-hpc-type-1-or-3","title":"Step 3: Provide a project title and choose HPC type (1 or 3)","text":""},{"location":"HPC_Facilities/GrantApp/#step-4-select-storage-amount-and-machine-type","title":"Step 4: Select storage amount and machine type.","text":"<p>The requested resources should either be given in DKK or Core Hours</p>"},{"location":"HPC_Facilities/GrantApp/#cpu-machines-deic-interactive-hpc-sdu-red-circle","title":"CPU Machines / DeiC Interactive HPC (SDU)  (Red Circle)","text":"<ul> <li>u1-standard in DKK</li> </ul>"},{"location":"HPC_Facilities/GrantApp/#gpu-machines-deic-interactive-hpc-aau-blue-circles","title":"GPU Machines / DeiC Interactive HPC (AAU) (Blue Circles)","text":"<ul> <li> <p>uc-t4\" (GPU) in DKK</p> </li> <li> <p>uc-t4-h\" (GPU) in Core Hours</p> </li> <li> <p>uc-A100-h\" (GPU) in Core Hours</p> </li> <li> <p>uc-A40-h\" (GPU) in Core Hours</p> </li> <li> <p>uc-A10-h\" (GPU) in Core Hours</p> </li> </ul> <p></p>"},{"location":"HPC_Facilities/GrantApp/#step-5-project-description-software-license","title":"Step 5: project description &amp; software license","text":"<p>Provide a meaningfull project description &amp; select SAS or STATA License (if needed). </p>"},{"location":"HPC_Facilities/GrantApp/#step-6-press-submit-application","title":"Step 6: Press \"Submit application\"","text":"<p>Now the application will be evaluated by the CBS front office at first given opportunity. The application will either be accepted otherwise you will be contacted (CBS mail).</p> <p></p>"},{"location":"HPC_Facilities/Hippo/","title":"Type 3 \u2013 Large Memory HPC (Hippo)","text":"<p>This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area. Type 3 is hosted and maintained at SDU. </p>"},{"location":"HPC_Facilities/Hippo/#facility-overview","title":"Facility Overview","text":"<p>The DeiC Large Memory HPC system is a system consisting of large memory nodes (between 1 and 4 TB RAM per node).</p>"},{"location":"HPC_Facilities/Hippo/#system-specifications","title":"System Specifications","text":"<p>System specifications can be found here.</p>"},{"location":"HPC_Facilities/Hippo/#get-started","title":"Get started","text":"<p>Large Memory HPC (Hippo) is integrated with UCloud which providing an easy-to-use interface.</p> <p>The UCloud integration provides three base applications RStudio, JupyterLab and Slurm. See apps.</p> <ul> <li> <p>RStudio and JupyterLab facilites interactive jobs on a single Type3 node for a range of popular programming languages.</p> </li> <li> <p>Slurm provides batch job processing across multiple Type 3 nodes.</p> </li> </ul> <p>To get resources read here.</p> <p>Start by reading the following Type 3 tutorials:</p> <ul> <li>Type 3 user guide (from SDU)</li> <li>Run Stata on Type 3 </li> <li>Use Conda to manage Jupyterlab environments on Type 3</li> </ul>"},{"location":"HPC_Facilities/Hippo/#user-support","title":"User Support","text":"<p>All UCloud support should go through the RDM Support. If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/License/","title":"Add License to STATA and SAS application","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/License/#add-local-license","title":"Add Local License","text":""},{"location":"HPC_Facilities/License/#step-1-upload-local-stata-lic-file-or-sas-txt-file-license-to-ucloud","title":"Step 1: Upload local STATA (.lic file) or SAS (.txt file) license to UCloud","text":""},{"location":"HPC_Facilities/License/#step-2-select-the-license-file-lic-or-txt-while-setting-up-ucloud-job","title":"Step 2: Select the license file (.lic or .txt) while setting up UCloud Job","text":""},{"location":"HPC_Facilities/License/#add-server-license","title":"Add Server License","text":""},{"location":"HPC_Facilities/License/#step-1-apply-for-server-license-through-ucloud-grant-application","title":"Step 1: Apply for Server License through UCloud Grant Application","text":""},{"location":"HPC_Facilities/License/#step-2-activate-license-sas-94-license-shown-as-example","title":"Step 2: Activate License (SAS 9.4 license shown as example)","text":""},{"location":"HPC_Facilities/License/#step-3-select-server-license-while-setting-up-ucloud-job","title":"Step 3: Select server license while setting up UCloud Job","text":""},{"location":"HPC_Facilities/MachineType/","title":"Type 1 - DeiC Interactive HPC","text":""},{"location":"HPC_Facilities/MachineType/#deic-interactive-hpc-sdu","title":"DeiC Interactive HPC (SDU)","text":"<p>SDU provides CPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as you would on your laptop. See all apps. </p> <p>u1-standard</p> <p>The following machines are available:</p> <p></p> <p>The specification of the largest node (u1-standard-64) are summarized below:</p> <p>Dell PowerEdge C6420</p> <p>CPU:    64 (32 virtual cores) 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz</p> <p>RAM: 384 GB  DDR 4-2666</p> <p>Price: 5,49 DKK/hour</p> <p>Description: The full node consists of 2x Intel(R) Xeon(R) Gold 6130 CPU@2.10 GHz, 32 virtual cores/CPU, and 384 GB of memory.</p>"},{"location":"HPC_Facilities/MachineType/#deic-interactive-hpc-aau","title":"DeiC Interactive HPC (AAU)","text":"<p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab.</p> <p>Four different machine types based on different Nvidia GPUs (T4, A10 , A40 and A100) with different application purposes. </p> Nvidia T4Nvidia A10Nvidia A40Nvidia A100Specification Comparisons <ul> <li>AI Inference: The T4 is optimized for AI inference workloads, making it suitable for applications like image and speech recognition, natural language processing, and recommendation systems in data centers.</li> </ul> <p>Nvidia T4 is avaliable on the following machines:</p> <p></p> <p></p> <ul> <li>Data Center Workloads: The A10 offers a balance of compute power and memory, making it versatile for various data center tasks, including virtualization, cloud computing, and database acceleration.</li> </ul> <p>Nvidia A10 is avaliable on the following machines:</p> <p></p> <ul> <li>High-Performance Computing (HPC): The A40 is designed for HPC applications, such as scientific simulations, climate modeling, and molecular dynamics, where high computational power is essential.</li> </ul> <p>Nvidia A40 is avaliable on the following machines:</p> <p></p> <ul> <li>Deep Learning and AI Research: The A100 excels in deep learning training and AI research, enabling faster model training for tasks like image recognition, natural language understanding, and autonomous driving.</li> <li>High-Performance Computing (HPC): It's also used in HPC environments for tasks like molecular dynamics simulations, quantum chemistry, and climate modeling, thanks to its exceptional computational capabilities.</li> </ul> <p>Nvidia A100 is avaliable on the following machines:</p> <p></p> <p>Their specifications are summarized in a table below.</p> GPU Architecture CUDA Cores Tensor Cores Memory FP16 (Half) TFLOPS FP32 (Float) TFLOPS FP64 (Double) GFLOPS Data Sheet Nvidia T4 Turing 2,560 320 16 GB GDDR6 65.1 8.1 254.4 T4 Nvidia A10 Ampere 6,144 384 24 GB GDDR6 31.2 31.2 976.3 A10 Nvidia A40 Ampere 10,240 320 48 GB GDDR6 37.4 37.4 0.59 A40 Nvidia A100 Ampere 6,912 432 80 GB HBM2 78.0 19.5 9700 A100 <ul> <li> <p>CUDA cores are the general-purpose processing units in a GPU that can perform computations with standard floating-point precision, such as single-precision (32-bit) or double-precision (64-bit).</p> </li> <li> <p>Tensor Cores are optimized to trade off precision for speed and can significantly accelerate deep learning training and inference.</p> </li> <li> <p>A more detailed description can be found here.</p> </li> </ul>"},{"location":"HPC_Facilities/NGC/","title":"Nationalt Genom Center HPC (Danish Statistics Data)","text":"<p>It is possible to access and process data from Danish Statistics through the Nationalt Genom Center HPC. This service is not funded by CBS. See pricing list.</p> <p>A techinal guide is found here.</p> <p>The linked information is only avaliable in danish.</p> <p>For further information or support please contact RDM Support.</p>"},{"location":"HPC_Facilities/Overview/","title":"Overview","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>National HPC Facilities (DeiC)</li> <li>WRDS - Wharton Research Data Services</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> <li>HPC Operational Status</li> </ul>"},{"location":"HPC_Facilities/UCloud/","title":"Type 1 \u2013 Interactive HPC (UCloud)","text":"<p>The easiest-to-use HPC service is DeiC Interactive HPC (Type 1) also known as UCloud. This service is provided by the Danish universities SDU and AAU.</p>"},{"location":"HPC_Facilities/UCloud/#facility-overview","title":"Facility Overview","text":"<p>SDU provides CPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as they would on your laptop. See all apps. </p> <p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab. </p>"},{"location":"HPC_Facilities/UCloud/#system-specifications","title":"System Specifications","text":"<p>System specifications can be found here.</p>"},{"location":"HPC_Facilities/UCloud/#login-on-ucloud","title":"Login on UCloud","text":"<p>You can login on to UCloud using WAYF (Where Are You From). Press here to login.</p> <ul> <li>Select Copenhagen Business School as your affiliate institution on the login page. </li> <li>Sign in using your CBS mail account</li> </ul> <p>Upon the first login it is necessary to approve the SDU eScience terms of service. Afterwards, the user is redirected to the UCloud user interface.</p> <p>Note: After login the user can activate two factor authentication by clicking on the avatar icon in the top-right corner of the home screen.</p>"},{"location":"HPC_Facilities/UCloud/#geting-started","title":"Geting started","text":"<p>All new users in UCloud are awarded a \"My Workspace\" with 1000 DKK of computing (CPU only) resources to the \"DeiC Interactive HPC (SDU)\", as well as 50 GB remote storage. You can use these resources to get acquainted with the system, run test jobs, etc. </p> <p>\"DeiC Interactive HPC (SDU)\" provides broadest ranges of containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI). See all apps.</p> <p>The largest machine (64 cores &amp; 384 GB memory) cost 5.49kr/Hour. So the free 1000 DKK will give you access to approx. 182 hours of inital run time.</p> <p>For additional resources see here.</p> <p>Start by watching the following UCloud tutorials:</p> <ul> <li>Manage Files and Folders</li> <li>Manage Applications</li> <li>Manage Workspaces</li> <li>Use Cases</li> <li>Webinars</li> <li>UCloud Documentation</li> </ul> <p>More Tutorials and Documentation can be found here</p>"},{"location":"HPC_Facilities/UCloud/#user-support","title":"User Support","text":"<p>All UCloud support should go through the RDM Support. If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/UCloud/#collaboration","title":"Collaboration","text":"<p> International Collaborators</p> <p>International researchers need a \"visiting researcher premission\"(g\u00e6steforskeradgang) to CBS to gain access to UCloud. One can be obtained by contacting CBS HR (hr@cbs.dk).</p> <p>Once this is in place CBS HPC support will contact the UCloud Research Support Team and provide the below shown information. </p> <ul> <li> <p>Full name:</p> </li> <li> <p>Occupation:</p> </li> <li> <p>Organisation (University):</p> </li> <li> <p>Email (University):</p> </li> </ul> <p>Subsequently, the UCloud Research Support Team will contact the researcher to verify their identity through a video meeting. A valid ID is needed. </p>"},{"location":"HPC_Facilities/UCloud/#license-software","title":"License Software","text":"<p>There are several types of licensed software that can be run on UCloud. </p> MATLABSTATASAS &amp; SAS Studio <p>  A Matlab server license is needed in order to run the application on UCloud. Once can be acquired through CBS IT help desk at own expense.</p> <ul> <li> <p>Matlab UCloud Application</p> </li> <li> <p>UCloud Matlab Documentation</p> </li> <li> <p>UCloud video tutorial </p> <ul> <li>Matlab walkthrough starts at 16:00 minutes into the video. </li> <li>Shows how activate Matlab with a personal license.</li> </ul> </li> </ul> <p> Users can either upload their own personal STATA license (.lic file) to UCloud or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>STATA UCloud Application</p> </li> <li> <p>UCloud STATA Documentation</p> </li> </ul> <p> Users can either upload their own personal SAS license (.txt file) or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>SAS UCloud Application</p> </li> <li> <p>UCloud SAS Documentation</p> </li> </ul>"},{"location":"HPC_Facilities/WRDS/","title":"WRDS - Wharton Research Data Services","text":"<p>WRDS- Wharton Research Data Services provides access to financial data, accounting figures as well as banking and management information. CBS students and staff must register to get access. </p> <p>More information can be found here.</p> <p>For further support lease contact RDM Support.</p>"},{"location":"HPC_Facilities/WRDS/#accessing-wrds-databases","title":"Accessing WRDS databases:","text":""},{"location":"HPC_Facilities/WRDS/#local-pc-ucloud","title":"Local PC &amp; UCloud","text":"<ul> <li>Python</li> <li>R</li> <li>Matlab</li> <li>SAS</li> <li>STATA</li> <li>UCloud Templates/Scripts </li> </ul>"},{"location":"HPC_Facilities/WRDS/#wrds-cloud","title":"WRDS Cloud","text":"<p>WRDS Cloud is a HPC service with the possibility to process the data avaliable on WRDS. WRDS Cloud is only acessable for CBS staff and researchers.</p>"},{"location":"HPC_Facilities/WRDS/#available-software","title":"Available Software","text":"<p>The WRDS Cloud provides the following software:</p> <ul> <li>SAS 9.4</li> <li>R 3.5</li> <li>Python 3.6 and 2.7</li> <li>Stata 15 (requires special subscription agreement with StataCorp)</li> </ul> <p>In addition, it further supports remote access from:</p> <ul> <li>MATLAB 2016a +</li> <li>Native PostgreSQL clients</li> <li>ODBC- or JDBC-compliant clients</li> </ul>"},{"location":"HPC_Facilities/WRDS/#wrds-cloud-documentation","title":"WRDS Cloud Documentation","text":"<ul> <li>Introduction to the WRDS Cloud</li> <li>Using SSH to Connect to the WRDS Cloud </li> </ul>"},{"location":"HPC_Facilities/status/","title":"HPC Operational Status","text":"<ul> <li>Type 1 (UCloud)</li> <li>Type 3 (Hippo)</li> <li>Type 5 (LUMI)</li> </ul>"},{"location":"Tutorial_Docs/BLAS/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library","text":"<p>When it comes to numerical computations and linear algebra in the R programming language, the BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries play crucial roles. These libraries provide a collection of efficient and optimized routines for various linear algebra operations, such as matrix multiplication, solving linear systems, eigenvalue computations, and more.</p> <p>BLAS, the Basic Linear Algebra Subprograms, is a standard interface specification for low-level linear algebra operations. It defines a set of routines that perform basic vector and matrix operations efficiently. The BLAS routines are highly optimized and implemented in highly efficient machine code to take advantage of the specific hardware architecture. R relies on the BLAS library for fundamental linear algebra operations, providing performance improvements for numerical computations.</p> <p>LAPACK, the Linear Algebra Package, builds upon the BLAS library and provides higher-level routines for solving more complex linear algebra problems. LAPACK offers a comprehensive set of algorithms for solving systems of linear equations, eigenvalue problems, least-squares problems, and singular value decompositions. These routines are widely used in various scientific and engineering applications.</p>"},{"location":"Tutorial_Docs/BLAS/#r","title":"R","text":"<ul> <li>Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.</li> </ul>"},{"location":"Tutorial_Docs/BatchMode/","title":"UCloud Tutorial: Batch Processing on UCloud","text":"<p>Running non-interactive script in \"batch mode\" can help overcome the UCloud capacity issues as jobs can be started any time and then executed whenever the systems resources permits.  </p> <p>Many applications already have a \"Batch Processing\" UCloud functionality:</p> <ul> <li>Batch Processing</li> <li>RStudio</li> <li>JupyterLab</li> <li>Matlab</li> <li>PyTorch</li> <li>Tensorflow</li> <li>Spark Cluster</li> <li>Terminal/SLURM Cluster</li> </ul> <p>Application that currently does not have this UCloud functionality (e.g. Stata) can instead run batch mode computations using the terminal app. See the following tutorials:</p> <ul> <li>Stata</li> </ul>"},{"location":"Tutorial_Docs/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <ul> <li> <p>R</p> </li> <li> <p>Python</p> </li> </ul> <p>Further documentation can be found on UCloud:</p> <ul> <li>Conda on UCloud </li> </ul>"},{"location":"Tutorial_Docs/VMs/","title":"Access GPUs on UCloud","text":"<ul> <li> <p>GPUs are accessible on UCloud through virtual machines (VMs) hosted by AAU. </p> </li> <li> <p>GPU resources can  be obtained through a UCloud grant application.</p> </li> <li> <p>Information of the different GPU systems can be found here.</p> </li> </ul>"},{"location":"Tutorial_Docs/VMs/#tutorials","title":"Tutorials","text":"<ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access at data transfer to AAU VMs using SSH</p> </li> </ul>"},{"location":"Tutorials/BLAS/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library","text":"<p>When it comes to numerical computations and linear algebra in the R programming language, the BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries play crucial roles. These libraries provide a collection of efficient and optimized routines for various linear algebra operations, such as matrix multiplication, solving linear systems, eigenvalue computations, and more.</p> <p>BLAS, the Basic Linear Algebra Subprograms, is a standard interface specification for low-level linear algebra operations. It defines a set of routines that perform basic vector and matrix operations efficiently. The BLAS routines are highly optimized and implemented in highly efficient machine code to take advantage of the specific hardware architecture. R relies on the BLAS library for fundamental linear algebra operations, providing performance improvements for numerical computations.</p> <p>LAPACK, the Linear Algebra Package, builds upon the BLAS library and provides higher-level routines for solving more complex linear algebra problems. LAPACK offers a comprehensive set of algorithms for solving systems of linear equations, eigenvalue problems, least-squares problems, and singular value decompositions. These routines are widely used in various scientific and engineering applications.</p>"},{"location":"Tutorials/BLAS/#r","title":"R","text":"<ul> <li>Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.</li> </ul>"},{"location":"Tutorials/BLAS/#python","title":"Python","text":"<ul> <li>Speed up your Numpy calculations</li> </ul>"},{"location":"Tutorials/BLAS/BLAS_Python/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.","text":"<p>In the context of R, there are several libraries available that interface with the BLAS and LAPACK libraries to enhance their functionality or provide alternative implementations. Here are a few notable libraries:</p> <ul> <li> <p>base R: The base R installation includes a reference BLAS implementation that provides basic linear algebra functionality. While this implementation is not as optimized as other libraries, it serves as a fallback option when more optimized libraries are not available.</p> </li> <li> <p>OpenBLAS: OpenBLAS is an optimized BLAS and LAPACK library that offers high-performance linear algebra routines. It is widely used and provides significant speed improvements over the reference BLAS implementation in base R.</p> </li> <li> <p>Intel MKL: The Intel Math Kernel Library (MKL) is a highly optimized set of mathematical functions for various platforms, including CPUs from Intel. It includes efficient implementations of BLAS and LAPACK routines and is known for its excellent performance. MKL is often favored for its outstanding performance on Intel architectures.</p> </li> <li> <p>ACML: The AMD Core Math Library (ACML) is an optimized library for AMD processors. It provides optimized BLAS and LAPACK routines tailored for AMD architectures.</p> </li> <li> <p>vecLib: vecLib is the BLAS and LAPACK library included with macOS. It provides optimized routines for Apple hardware.</p> </li> <li> <p>ATLAS: ATLAS (Automatically Tuned Linear Algebra Software) is another popular BLAS implementation that can be used as an alternative to the reference BLAS library. ATLAS utilizes an automated tuning process to generate highly optimized code specifically tailored to the host system's architecture. It offers improved performance for various linear algebra operations.</p> </li> </ul> <p>These libraries can be linked with R during installation or dynamically loaded during runtime, allowing users to choose the most suitable implementation for their specific hardware and performance requirements.</p> <p>In summary, the BLAS and LAPACK libraries are essential components for numerical computations and linear algebra in R. They provide efficient and optimized routines for fundamental linear algebra operations, and various libraries such as OpenBLAS, Intel MKL, ACML, vecLib, and ATLAS enhance their functionality or provide alternative implementations for improved performance.</p>"},{"location":"Tutorials/BLAS/BLAS_Python/#rstudio-on-ucloud","title":"RStudio on UCloud","text":"<p>When looking at the different R/RStudio version available on UCloud (see below) it can be observed that some have the suffix \"_MKL\". These have \"Intel MKL\" as their default BLAS/LAPACK library, while versions with out the suffix have the \"LibBLAST\" library whihc is the base R library on linux systems.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_Python/#use-the-sessioninfo-function-to-check-which-blaslapack-library-is-deployed","title":"Use the \"sessionInfo()\" function to check which BLAS/LAPACK library is deployed:","text":"<p>MKL</p> <p></p> <p>LibBLAS</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_Python/#benchmarking-the-two-versions","title":"Benchmarking the two versions","text":"<p>Benchmarking a simple matrix multiplication shows that the \"MKL\" is close 78 times faster than the \"LibBLAS\"!!</p> <p>MKL mean = 7.63 milliseconds</p> <p></p> <p>LiBLAS mean = 592.666 milliseconds</p> <p></p> <pre><code>library(microbenchmark)\n\nn &lt;- 1000\nmat1 &lt;- matrix(rnorm(n*n), ncol=n)\nmat2 &lt;- matrix(rnorm(n*n), ncol=n)\n\ntic()\nf &lt;- function() mat1 %*% mat2\nres &lt;- microbenchmark(f(), times=100L)\nres\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#installing-and-changing-blas-lapack-library","title":"Installing and Changing BLAS &amp; LAPACK Library","text":"<p>Select between the available BLAS/LAPACK libraies by posting the command below i the job terminal. </p> <pre><code># BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n# LAPACK Selection\nsudo update-alternatives --config liblapack.so.3-x86_64-linux-gnu\n\n\n\n# BLAS Selection Output: \nThere are 2 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#install-other-libraries","title":"Install other libraries.","text":"<p>Changing the BLAS/LAPACK libraries is not possible for the \"MKL\" versions of the RStudio applications on UCloud. </p> <pre><code># install OpenBLAS\nsudo apt-get install libopenblas-base\n# install ATLAS\nsudo apt-get install libatlas3-base liblapack3\n\n# BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n\n# BLAS Selection Output: \nThere are 4 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3               35        manual mode\n  3            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n  4            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number: ^C\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#select-an-alternative-library-atlas-libraries-in-this-case","title":"Select an alternative library (Atlas libraries in this case)","text":"<p>Open R and confirm the change of BLAS/LAPACK library using \"sessionInfo()\" function.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.","text":"<p>In the context of R, there are several libraries available that interface with the BLAS and LAPACK libraries to enhance their functionality or provide alternative implementations. Here are a few notable libraries:</p> <ul> <li> <p>base R: The base R installation includes a reference BLAS implementation that provides basic linear algebra functionality. While this implementation is not as optimized as other libraries, it serves as a fallback option when more optimized libraries are not available.</p> </li> <li> <p>OpenBLAS: OpenBLAS is an optimized BLAS and LAPACK library that offers high-performance linear algebra routines. It is widely used and provides significant speed improvements over the reference BLAS implementation in base R.</p> </li> <li> <p>Intel MKL: The Intel Math Kernel Library (MKL) is a highly optimized set of mathematical functions for various platforms, including CPUs from Intel. It includes efficient implementations of BLAS and LAPACK routines and is known for its excellent performance. MKL is often favored for its outstanding performance on Intel architectures.</p> </li> <li> <p>ACML: The AMD Core Math Library (ACML) is an optimized library for AMD processors. It provides optimized BLAS and LAPACK routines tailored for AMD architectures.</p> </li> <li> <p>vecLib: vecLib is the BLAS and LAPACK library included with macOS. It provides optimized routines for Apple hardware.</p> </li> <li> <p>ATLAS: ATLAS (Automatically Tuned Linear Algebra Software) is another popular BLAS implementation that can be used as an alternative to the reference BLAS library. ATLAS utilizes an automated tuning process to generate highly optimized code specifically tailored to the host system's architecture. It offers improved performance for various linear algebra operations.</p> </li> </ul> <p>These libraries can be linked with R during installation or dynamically loaded during runtime, allowing users to choose the most suitable implementation for their specific hardware and performance requirements.</p> <p>In summary, the BLAS and LAPACK libraries are essential components for numerical computations and linear algebra in R. They provide efficient and optimized routines for fundamental linear algebra operations, and various libraries such as OpenBLAS, Intel MKL, ACML, vecLib, and ATLAS enhance their functionality or provide alternative implementations for improved performance.</p>"},{"location":"Tutorials/BLAS/BLAS_R/#rstudio-on-ucloud","title":"RStudio on UCloud","text":"<p>When looking at the different R/RStudio version available on UCloud (see below) it can be observed that some have the suffix \"_MKL\". These have \"Intel MKL\" as their default BLAS/LAPACK library, while versions with out the suffix have the \"LibBLAST\" library whihc is the base R library on linux systems.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/#use-the-sessioninfo-function-to-check-which-blaslapack-library-is-deployed","title":"Use the \"sessionInfo()\" function to check which BLAS/LAPACK library is deployed:","text":"<p>MKL</p> <p></p> <p>LibBLAS</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/#benchmarking-the-two-versions","title":"Benchmarking the two versions","text":"<p>Benchmarking a simple matrix multiplication shows that the \"MKL\" is close 78 times faster than the \"LibBLAS\"!!</p> <p>MKL mean = 7.63 milliseconds</p> <p></p> <p>LiBLAS mean = 592.666 milliseconds</p> <p></p> <pre><code>library(microbenchmark)\n\nn &lt;- 1000\nmat1 &lt;- matrix(rnorm(n*n), ncol=n)\nmat2 &lt;- matrix(rnorm(n*n), ncol=n)\n\ntic()\nf &lt;- function() mat1 %*% mat2\nres &lt;- microbenchmark(f(), times=100L)\nres\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#installing-and-changing-blas-lapack-library","title":"Installing and Changing BLAS &amp; LAPACK Library","text":"<p>Select between the available BLAS/LAPACK libraies by posting the command below i the job terminal. </p> <pre><code># BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n# LAPACK Selection\nsudo update-alternatives --config liblapack.so.3-x86_64-linux-gnu\n\n\n\n# BLAS Selection Output: \nThere are 2 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#install-other-libraries","title":"Install other libraries.","text":"<p>Changing the BLAS/LAPACK libraries is not possible for the \"MKL\" versions of the RStudio applications on UCloud. </p> <pre><code># Update\nsudo apt-get update\n# install OpenBLAS\nsudo apt-get install libopenblas-base\n# install ATLAS\nsudo apt-get install libatlas3-base liblapack3\n\n# BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n\n# BLAS Selection Output: \nThere are 4 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3               35        manual mode\n  3            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n  4            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number: ^C\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#select-an-alternative-library-atlas-libraries-in-this-case","title":"Select an alternative library (Atlas libraries in this case)","text":"<p>Open R and confirm the change of BLAS/LAPACK library using \"sessionInfo()\" function.</p> <p></p>"},{"location":"Tutorials/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <p>R</p> <p>Python</p> <p>Further documentation can be found on UCloud:</p> <p>Conda on UCloud </p>"},{"location":"Tutorials/Conda/Conda_Python/","title":"UCloud Tutorial: Using Conda for easy management of Python environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_Python/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_Python/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#installing-and-activate-python-environments","title":"Installing and activate Python environments","text":"<p>https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html </p> <pre><code># Showing available python versions\nconda search python\n\n# Installing a Python environment (Python 3.9 in this example) \nconda create -n myenv python=3.9\n\n# Or install packages during installation.\nconda create -n myenv python=3.9 numpy=1.16\n\n# Shows already installed environments (R-4.2.3 show be displayed)\nconda env list\n\n# Activate environment\nconda activate myenv\n\n#Check which Python is in path\nwhich python\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#vscode-on-ucloud","title":"VScode on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-coder-python-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Coder python UCloud job.","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-coder.html?highlight=coder</p> <p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-vscode-interface-and-open-file-and-activate-myenv-as-python-interpreter","title":"Now you can launch VSCode interface and open file and activate \u201cmyenv\u201d as python interpreter:","text":"<p>Select the menu View -&gt; Command Palette:</p> <p></p> <p>Execute the command &gt; Python: Select Intepreter:</p> <p></p>"},{"location":"Tutorials/Conda/Conda_Python/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info --envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install ipykernel:\n\nconda install ipykernel\n\n# \npython -m ipykernel install --user --name myenv --display-name \"myenv\"\n\n# De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_Python/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python_1","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#installing-and-activate-r-environments","title":"Installing and activate R environments","text":"<p>https://docs.anaconda.com/free/anaconda/packages/using-r-language/</p> <pre><code># Installing a R environment\nconda create -n  myenv r-essentials r-base\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#install-packages-through-conda","title":"install packages through Conda","text":"<p>When using conda to install R packages, you will need to add r- before the regular package name:</p> <pre><code># For instance, if you want to install rbokeh:\nconda install r-rbokeh\n\n# or for rJava:\nconda install r-rjava\n\n# Update packages:\n\nconda update r-caret\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Conda_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Mamba_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Mamba_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#installing-r-environment-using-conda","title":"Installing R environment using Conda","text":""},{"location":"Tutorials/Conda/Mamba_R/#installing-mamba-add-in","title":"Installing Mamba add-in","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p>"},{"location":"Tutorials/Conda/Mamba_R/#installing-and-activate-r-environment-with-mamba","title":"Installing and activate R environment with mamba","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p> <p>https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda</p> <pre><code>#Installing a R environment (R-4.2.3 in this example) \nconda create --solver=libmamba -n myenv -y -c conda-forge r-base=4.2.3\n\n#Or install packages during installation.\nconda create --solver=libmamba -n myenv -y -c conda-forge r-base=4.2.3 r-tidyverse\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Start R:\nR\n\n# Install a package\ninstall.packages(\u201ctidymodels\u201d)\n\n# Close R:\nquit()\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Mamba_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Install iRkernel:\nconda install -c conda-forge r-irkernel\n\n# Activate R Kernel in Jupter\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n\n# De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Mamba_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-r-environment-using-conda","title":"Installing R environment using Conda","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-mamba-add-in","title":"Installing Mamba add-in","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p> <pre><code># Installing mamba add-in:\nconda install -n base -c conda-forge mamba\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#installing-and-activate-r-environment-with-mamba","title":"Installing and activate R environment with mamba","text":"<p>https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda</p> <pre><code>#Showing available R versions\nmamba search -c conda-forge r-base\n\n#Installing a R environment (R-4.2.3 in this example) \nmamba create -n myenv -y -c conda-forge r-base=4.2.3\n\n#Or install packages during installation.\nmamba create -n myenv -y -c conda-forge r-base=4.2.3 r-tidyverse\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Old%20Version/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/GPU/gpu_libraries/","title":"GPU libraries","text":"<p>The list of GPU libraries and packages provided below are by no means exhaustive, and the world of GPU-accelerated computing is continually evolving. Users are encouraged to conduct their own due diligence, explore additional packages, and stay updated with the latest developments in the GPU computing ecosystem to best suit their specific requirements and project needs.</p> PythonR <p></p> <ol> <li> <p>CUDA Toolkit - The official parallel computing platform and API developed by NVIDIA for GPU acceleration.</p> </li> <li> <p>CuPy - An open-source GPU-accelerated array library compatible with NumPy.</p> </li> <li> <p>PyTorch - A popular deep learning framework with GPU support for defining and training neural networks.</p> </li> <li> <p>TensorFlow - A widely-used deep learning framework for defining and training machine learning models on GPUs.</p> </li> <li> <p>scikit-cuda - A library providing Python interfaces to various CUDA libraries and functions for GPU acceleration.</p> </li> <li> <p>RAPIDS - A suite of GPU-accelerated data science libraries, including cuDF, cuML, and cuGraph, developed by NVIDIA.</p> </li> <li> <p>Numba - A Just-In-Time (JIT) compiler for Python that can generate optimized machine code for CPU and GPU execution.</p> </li> <li> <p>Theano: (No longer actively developed) - An early deep learning framework that supported GPU acceleration. Many of its concepts have influenced other frameworks.</p> </li> <li> <p>MXNet - A deep learning framework known for its flexibility and efficiency in training neural networks on GPUs.</p> </li> <li> <p>Cupy-cuBLAS - An extension of CuPy that provides GPU-accelerated linear algebra operations using the cuBLAS library.</p> </li> </ol> <p>Note: Make sure to check the official websites or repositories of these libraries for the latest installation instructions and documentation.</p> <p></p> <ol> <li> <p>gputools - Provides GPU-based tools for data manipulation, including matrix operations and basic statistics.</p> </li> <li> <p>gpuR - Offers GPU support for matrix operations and linear algebra in R.</p> </li> <li> <p>RapidsR - An R interface to the RAPIDS suite of GPU-accelerated data science libraries developed by NVIDIA, including cuDF and cuML.</p> </li> <li> <p>mxnet - The R interface to the MXNet deep learning framework, which allows you to train and deploy neural networks on GPUs.</p> </li> <li> <p>tensorflow - The R interface to TensorFlow, a popular deep learning framework with GPU support.</p> </li> <li> <p>cudaBayesreg - A package for Bayesian regression analysis with GPU support.</p> </li> <li> <p>gpuMagic - A package for general-purpose GPU computing in R.</p> </li> <li> <p>H2O4GPU - A GPU-accelerated machine learning library for R, offering a wide range of machine learning algorithms optimized for GPUs.</p> </li> <li> <p>Rtorch - An R interface to PyTorch, a popular deep learning framework, allowing you to create and train neural networks on GPUs.</p> </li> <li> <p>gmatrix - Offers GPU support for matrix operations and linear algebra in R.</p> </li> </ol> <p>Note: Some of these packages may require specific GPU hardware and dependencies. Be sure to check the official documentation and package repositories for installation instructions and system requirements.</p> <p>You can click on the provided links or search for the package names on CRAN or GitHub to find more information about each package and how to install and use them in your R projects.</p>"},{"location":"Tutorials/GPU/pytorch_ddp/","title":"Pytorch: Train your deep-learning models on AAU GPUs","text":"<p>This tutorial show how to deploy \"Distributed Data Parallel (DDP) in PyTorch\" to efficiently train your deep-learning models on the AAU GPUs avalaible through UCloud.</p> <p>See here for a more detailed tutorial on DDP using Pytorch.</p> <p>This tutorial specifically focuses on Part 4: Multi-GPU DDP Training with fault tolerance using Torchrun. </p> <p>The following python scripts are needed to replicate this tutorial: </p> <ul> <li>multigpu_torchrun.py (Can be used as template)</li> <li>datautils.py (Used to load dummy data)</li> </ul> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/GPU/pytorch_ddp/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#activate-conda","title":"Activate Conda","text":"<p>This can be done by either installing a conda from scratch or by deploying er prior installation. Please see  \"Using Conda for easy workflow deployment on AAU GPU VMs\" for information.</p> <pre><code># Download and install miniconda (If needed)\ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#create-or-re-load-a-pytorch-conda-environment","title":"Create or re-load a Pytorch-conda environment","text":"<p>look for latest pytorch installation at https://pytorch.org/get-started/locally/</p> <pre><code># Create pytorch conda environment if non-exist on the Conda installation\nconda deactivate\nconda create --name pytorch\nconda activate pytorch\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#transfer-files-and-folders-ssh-copy-to-vm","title":"Transfer Files and Folders (SSH-Copy) to VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path\\pytorch_folder\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#run-pytorch-training-in-ddp-mode","title":"Run Pytorch training in DDP mode:","text":"<p>In this example a model is trained for 50 epocs with a model snapshot (\"snapshot.pt\") being save every 10 epocs and the final model being saved as \"finalmodel.pt\". </p> <p>Line 78 and 79 needs to be changed to adjust training data and model.   78   train_set = MyTrainDataset(2048)  # load your dataset  79   model = torch.nn.Linear(20, 1)  # load your model</p> <p>\"--nproc_per_node=\" determines the number of GPUs to use. \"gpu\" pytorch will utilise all avaliable GPUs.</p> <pre><code>torchrun --standalone --nproc_per_node=gpu multigpu_torchrun.py 50 10\n</code></pre> <pre><code>$ torchrun --standalone --nproc_per_node=2 multigpu_torchrun.py 50 10\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nWARNING:torch.distributed.run:\n*****************************************\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n*****************************************\n[GPU0] Epoch 0 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 0 | Batchsize: 32 | Steps: 32\nEpoch 0 | Training snapshot saved at snapshot.pt\n[GPU1] Epoch 1 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 1 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 2 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 2 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 3 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 3 | Batchsize: 32 | Steps: 32\n...\n[GPU0] Epoch 7 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 7 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 8 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 8 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 9 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 9 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 10 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 10 | Batchsize: 32 | Steps: 32\nEpoch 10 | Training snapshot saved at snapshot.pt\n[GPU0] Epoch 11 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 11 | Batchsize: 32 | Steps: 32\n...\n[GPU0] Epoch 47 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 47 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 48 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 48 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 49 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 49 | Batchsize: 32 | Steps: 32\nFinal model saved at finalmodel.pt\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#track-the-gpu-usage","title":"Track the GPU usage","text":"<pre><code>nvidia-smi -l 5 # Will update every 5 seconds\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A      2312      C   python                           1324MiB |\n|    0   N/A  N/A      2381      C   ...a3/envs/rapids/bin/python     1042MiB |\n|    1   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    1   N/A  N/A      2383      C   ...a3/envs/rapids/bin/python     1042MiB |\n+-----------------------------------------------------------------------------+\nTue Aug 29 11:04:31 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P0    49W /  70W |   2389MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   36C    P0    53W /  70W |   1067MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Tutorials/GPU/pytorch_ddp/#transfer-results-and-conda-enviroment-local-machine-ssh-copy","title":"Transfer Results and Conda enviroment local machine (SSH-Copy)","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/","title":"RAPIDS-cuML: Train your Scikit-learn models on AAU GPUs","text":"<p>This tutorial show how to deploy RAPIDS-cuML-GPU Machine Learning Algorithms to efficiently train your scikit-learn models on the AAU GPUs avalaible through UCloud. </p> <p>\"cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions. cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents. For details on performance, see the cuML Benchmarks Notebook.\"</p> <p>In most cases, cuML's Python API matches the API from scikit-learn. which will make it easy to navigate from scikit-learn to RAPIDS-cuML</p> <p>A table of the supported algoritmns can be found here. </p> <p>This tutorial will use a random forrest which can be found on the cuML notebook examples</p> <p>The following python script is needed to replicate this tutorial: </p> <ul> <li>multigpu_rapids_cuml.py (Can be used as template)</li> </ul> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/GPU/rapids_cuml/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#activate-conda","title":"Activate Conda","text":"<p>This can be done by either installing a conda from scratch or by deploying er prior installation. Please see  \"Using Conda for easy workflow deployment on AAU GPU VMs\" for information.</p> <pre><code># Download and install miniconda (If needed)\ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#create-or-re-load-a-rapids-conda-environment","title":"Create or re-load a RAPIDS Conda environment","text":"<p>look for latest RAPIDS installation at https://docs.rapids.ai/install</p> <pre><code># Create pytorch conda environment if non-exist on the Conda installation\nconda deactivate\nconda create --solver=libmamba -n rapids -c rapidsai -c conda-forge -c nvidia  \\\n    rapids=23.08 python=3.10 cuda-version=11.8\n\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#transfer-files-and-folders-ssh-copy-to-vm","title":"Transfer Files and Folders (SSH-Copy) to VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path\\pytorch_folder\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#run-a-random-forrest-training-on-multiple-gpus","title":"Run a Random Forrest training on multiple GPUs:","text":"<pre><code>python multigpu_rapids_cuml.py\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#track-the-gpu-usage","title":"Track the GPU usage","text":"<pre><code>nvidia-smi -l 5 # Will update every 5 seconds\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A      2312      C   python                           1324MiB |\n|    0   N/A  N/A      2381      C   ...a3/envs/rapids/bin/python     1042MiB |\n|    1   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    1   N/A  N/A      2383      C   ...a3/envs/rapids/bin/python     1042MiB |\n+-----------------------------------------------------------------------------+\nTue Aug 29 11:04:31 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P0    49W /  70W |   2389MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   36C    P0    53W /  70W |   1067MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Tutorials/GPU/rapids_cuml/#transfer-results-and-conda-enviroment-local-machine-ssh-copy","title":"Transfer Results and Conda enviroment local machine (SSH-Copy)","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/","title":"Tensorflow: Train your deep-learning models on AAU GPUs","text":"<p>This tutorial show how to deploy \"Distributed Data Parallel (DDP) using Tensorflow/Keras\" to efficiently train your deep-learning models on the AAU GPUs avalaible through UCloud.</p> <p>See here for a more detailed tutorial on DDP using Tensorflow.</p> <p>This tutorial specifically focuses on Multi-GPU DDP Training with fault tolerance</p> <p>The following python script is needed to replicate this tutorial: </p> <ul> <li>multigpu_torchrun.py (Can be used as template)</li> </ul> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/GPU/tf_ddp/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#activate-conda","title":"Activate Conda","text":"<p>This can be done by either installing a conda from scratch or by deploying er prior installation. Please see  \"Using Conda for easy workflow deployment on AAU GPU VMs\" for information.</p> <pre><code># Download and install miniconda (If needed)\ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#create-or-re-load-a-tensorflowkeras-conda-environment","title":"Create or re-load a Tensorflow/Keras Conda environment","text":"<pre><code># Create pytorch conda environment if non-exist on the Conda installation\nconda deactivate\nconda create --name tensorflow\nconda activate tensorflow\nconda install -c anaconda tensorflow-gpu\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#transfer-files-and-folders-ssh-copy-to-vm","title":"Transfer Files and Folders (SSH-Copy) to VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path\\pytorch_folder\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#run-tensorflow-training-in-ddp-mode","title":"Run Tensorflow training in DDP mode:","text":"<p>In this example a model is trained for 10 epocs using 2 GPUs with a model checkpoint being saved with \"ckpt\" folder for each epoc and the final model being saved as \"final_model.keras\". </p> <p>functions \"get_compiled_model\" and \"get_dataset\" need to be changed to adjust training data and model.</p> <pre><code># Run the model with 10 epcos and 2 GPUs\npython multigpu_tensorflow.py 10 2\n\n# Run the model with 10 epcos and all avaiable GPUs\npython multigpu_tensorflow.py 10\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#track-the-gpu-usage","title":"Track the GPU usage","text":"<pre><code>nvidia-smi -l 5 # Will update every 5 seconds\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A      2312      C   python                           1324MiB |\n|    0   N/A  N/A      2381      C   ...a3/envs/rapids/bin/python     1042MiB |\n|    1   N/A  N/A      1011      G   /usr/lib/xorg/Xorg                  4MiB |\n|    1   N/A  N/A      2383      C   ...a3/envs/rapids/bin/python     1042MiB |\n+-----------------------------------------------------------------------------+\nTue Aug 29 11:04:31 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P0    49W /  70W |   2389MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   36C    P0    53W /  70W |   1067MiB / 15360MiB |     93%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Tutorials/GPU/tf_ddp/#transfer-results-and-conda-enviroment-local-machine-ssh-copy","title":"Transfer Results and Conda enviroment local machine (SSH-Copy)","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/SLURM/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"Tutorials/SLURM/#files","title":"Files","text":""},{"location":"Tutorials/SLURM/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"Tutorials/SLURM/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"Tutorials/SLURM/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"Tutorials/SLURM/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"Tutorials/SLURM/Ray/","title":"Ray","text":""},{"location":"Tutorials/SLURM/Ray/#example-using-ray","title":"Example using Ray","text":"<p>In terminal run:</p> <pre><code>python slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Output\n\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"Tutorials/SLURM/Ray/#open-extra-terminal-for-three-nodes","title":"Open extra Terminal for three Nodes","text":""},{"location":"Tutorials/SLURM/Ray/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"Tutorials/SLURM/Ray/#observed-that-the-work-is-disbrubted-across-all-three-nodes","title":"Observed that the work is disbrubted across all three nodes.","text":"<p>This may look different for different backends (e.g. Dask). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"Tutorials/SLURM/Ray/#output-files","title":"Output files","text":""},{"location":"Tutorials/SLURM/Ray/#the-autogenerated-slurm-script-slurmtest_0425-1208sh","title":"The autogenerated SLURM script (SlurmTest_0425-1208.sh)","text":"<pre><code>#!/bin/bash\n# shellcheck disable=SC2206\n# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!\n# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!\n\n#SBATCH --job-name=SlurmTest_0425-1208\n#SBATCH --output=SlurmTest_0425-1208.log\n\n### This script works for any number of nodes, Ray will find and manage all resources\n#SBATCH --nodes=3\n#SBATCH --exclusive\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n##SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE} #De-activated by KGP 230317\n\n# Load modules or your own conda environment here\n# module load pytorch/v1.4.0-gpu\n# conda activate ${CONDA_ENV}\n\n\n# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====\n\necho $SLURM_JOB_NODELIST\n\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\") # Getting the node names\n\nnodes_array=($nodes)\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w \"$node_1\" hostname --ip-address) # making redis-address\n\n# if we detect a space character in the head node IP, we'll\n# convert it to an ipv4 address. This step is optional.\nif [[ \"$ip\" == *\" \"* ]]; then\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$ip\"\nif [[ ${#ADDR[0]} -gt 16 ]]; then\nip=${ADDR[1]}\nelse\nip=${ADDR[0]}\nfi\necho \"IPV6 address detected. We split the IPV4 address as $ip\"\nfi\n\nport=6379\nip_head=$ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"STARTING HEAD at $node_1\"\nsrun --nodes=1 --ntasks=1 -w \"$node_1\" ray start --head --node-ip-address=\"$ip\" --port=$port --block &amp;\nsleep 30\n\n#worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\n#export NB_WORKERS=$((${SLURM_JOB_NUM_NODES-1})) #number of nodes other than the head node\n#echo ${NB_WORKERS}\n\nexport NB_WORKERS=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\necho \"STARTING ${NB_WORKERS} WORKERS\"\nfor ((i = 1; i &lt;= NB_WORKERS; i++)); do\nnode_i=${nodes_array[$i]}\necho \"STARTING WORKER $i at $node_i\"\nsrun --nodes=1 --ntasks=1 -w \"$node_i\" ray start --address \"$ip_head\" --block &amp;\nsleep 5\ndone\n\n# ===== Call your code below =====\necho \"RUNNING CODE: python /work/data/SklearnRay.py\"\npython /work/data/SklearnRay.py\n</code></pre>"},{"location":"Tutorials/SLURM/Ray/#autogenerated-log-file-slurmtest_0425-1208log","title":"Autogenerated log file (SlurmTest_0425-1208.log)","text":"<pre><code>node[0-2]\nIPV6 address detected. We split the IPV4 address as 10.42.47.86\nIP Head: 10.42.47.86:6379\nSTARTING HEAD at node0\n2023-04-25 12:08:40,054 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\nSTARTING 2 WORKERS\nSTARTING WORKER 1 at node1\n2023-04-25 12:08:38,026 INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-04-25 12:08:38,026 INFO scripts.py:710 -- Local node IP: 10.42.47.86\n2023-04-25 12:08:41,222 SUCC scripts.py:747 -- --------------------\n2023-04-25 12:08:41,222 SUCC scripts.py:748 -- Ray runtime started.\n2023-04-25 12:08:41,223 SUCC scripts.py:749 -- --------------------\n2023-04-25 12:08:41,223 INFO scripts.py:751 -- Next steps\n2023-04-25 12:08:41,223 INFO scripts.py:752 -- To connect to this Ray runtime from another node, run\n2023-04-25 12:08:41,223 INFO scripts.py:755 --   ray start --address='10.42.47.86:6379'\n2023-04-25 12:08:41,223 INFO scripts.py:771 -- Alternatively, use the following Python code:\n2023-04-25 12:08:41,223 INFO scripts.py:773 -- import ray\n2023-04-25 12:08:41,223 INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='10.42.47.86')\n2023-04-25 12:08:41,223 INFO scripts.py:790 -- To see the status of the cluster, use\n2023-04-25 12:08:41,223 INFO scripts.py:791 --   ray status\n2023-04-25 12:08:41,223 INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.\n2023-04-25 12:08:41,224 INFO scripts.py:809 -- To terminate the Ray runtime, run\n2023-04-25 12:08:41,224 INFO scripts.py:810 --   ray stop\n2023-04-25 12:08:41,224 INFO scripts.py:891 -- --block\n2023-04-25 12:08:41,224 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:41,224 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nSTARTING WORKER 2 at node2\n2023-04-25 12:08:48,882 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:48,933 I 2244 2244] global_state_accessor.cc:356: This node has an IP address of 10.42.28.36, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:48,859 INFO scripts.py:866 -- Local node IP: 10.42.28.36\n2023-04-25 12:08:48,935 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:48,935 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:48,935 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:48,935 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:48,935 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:48,935 INFO scripts.py:891 -- --block\n2023-04-25 12:08:48,935 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:48,935 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nRUNNING CODE: python /work/data/SklearnRay.py\n2023-04-25 12:08:54,215 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:54,271 I 956 956] global_state_accessor.cc:356: This node has an IP address of 10.42.34.213, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:54,135 INFO scripts.py:866 -- Local node IP: 10.42.34.213\n2023-04-25 12:08:54,274 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:54,275 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:54,275 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:54,275 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:54,275 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:54,275 INFO scripts.py:891 -- --block\n2023-04-25 12:08:54,275 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:54,275 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n2023-04-25 12:09:24,758 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.42.47.86:6379...\n2023-04-25 12:09:24,775 INFO worker.py:1553 -- Connected to Ray cluster.\n2023-04-25 12:09:25,073 WARNING pool.py:604 -- The 'context' argument is not supported using ray. Please refer to the documentation for how to control ray initialization.\nFitting 10 folds for each of 500 candidates, totalling 5000 fits\n209.00055767036974\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nslurmstepd-node2: error: *** STEP 2.3 ON node2 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node0: error: *** STEP 2.1 ON node0 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node1: error: *** STEP 2.2 ON node1 CANCELLED AT 2023-04-25T12:12:54 ***\nsrun: error: node2: task 0: Exited with exit code 1\nsrun: error: node1: task 0: Exited with exit code 1\nsrun: error: node0: task 0: Exited with exit code 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/","title":"UCloud Tutorial: Run Multi-node SLURM Cluster on UCloud","text":"TutorialFiles"},{"location":"Tutorials/SLURM/SLURM/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>In addition to the normal setting fill out the following options (See figure below).</p> <p>In this example launched as cluster consisting of 3 nodes with three folder added to the launch:</p> <ul> <li>\"miniconda3\"  - contains the conda environment I want to deploy across the different nodes.</li> <li>\"SLURM_deployment\" - contains the easy-to-use deployment scripts provided in this tutorial. </li> <li>\"SLURM_scripts\" - contains the user specific script and data to run on the cluster.</li> </ul> <p>In this example Conda is used for package and evironment management. Check here for more information on Conda on UCloud.</p> <p></p>"},{"location":"Tutorials/SLURM/SLURM/#when-the-job-has-started-open-terminal-for-node-1","title":"When the job has started open Terminal for Node 1","text":"<p>Run following commands in the terminal: </p> <pre><code># activate SLURM Cluster if not activated in the step above\ninit_slurm_cluster\n\n# List Avaliable nodes\nsinfo -N -l\n</code></pre> <p>The controller node is always the first node. Called \"node0\" in within SLURM but called \"Node 1\" in the UCloud interface). All additional nodes are named sequentially. For example, a cluster consisting of three full u1-standard nodes is configured as follows:</p> <pre><code>NODELIST   NODES PARTITION     STATE CPUS   S:C:T MEMORY\nnode0         1     CLOUD*     idle   64   1:64:1 385024\nnode1         1     CLOUD*     idle   64   1:64:1 385024\nnode2         1     CLOUD*     idle   64   1:64:1 385024\n</code></pre> <p>But called Node 1, Node 2 and Node 3 in the UCloud interface.</p>"},{"location":"Tutorials/SLURM/SLURM/#acitvate-conda-environment","title":"Acitvate Conda Environment","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which environment is in path (e.g. X = python,R..)\nwhich X # (e.g. X = python,R..)\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/X # (e.g. X = python,R..)\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/#slurm-deployment-scripts","title":"SLURM deployment scripts","text":"<p>The SLURM deployment script (\"slurm-launch.py\") have been adopted from  Ray documentation to support the addition of other python libraries (Dask, ipyparallel) and other languages (e.g. R).</p>"},{"location":"Tutorials/SLURM/SLURM/#slurm-launchpy","title":"slurm-launch.py","text":"<p>\"slurm-launch.py\" auto-generates SLURM scripts and launch. slurm-launch.py uses an underlying template (e.g. \"slurm-template_ray.sh\" or \"slurm-template_dask.sh\") and fills out placeholders given user input.</p> <pre><code># Change path:\ncd /work/SLURM_deployment\n\n# Python with Ray\npython slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Python with Dask\npython slurm-launch.py --script slurm-template_dask.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnDask.py\" --num-nodes 3 --nprocs 8 --nthreads 1\n\n# R with doParallel\npython slurm-launch.py --script slurm-template_R.sh --exp-name SlurmTest --command \"Rscript --vanilla /work/SLURM_scripts/doParallel.r\" --num-nodes 3 --nprocs 8 --nthreads 1 # Example of Output\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/#addditionel-options","title":"Addditionel options","text":"<pre><code>--exp-name          # The experiment name. Will generate {exp-name}_{date}-{time}.sh and {exp-name}_{date}-{time}.log.\n--command           # The command you wish to run. For example: rllib train XXX or python XXX.py.\n--node (-w)         # The specific nodes you wish to use, in the same form as the output of sinfo. Nodes are automatically assigned if not specified.\n--num-nodes (-n)    # The number of nodes you wish to use. Default: 1.\n--partition (-p):   # The partition you wish to use. Default: \u201c\u201d, will use user\u2019s default partition.\n--load-env:         # The command to setup your environment. For example: module load cuda/10.1. Default: \u201c\u201d.\n--nprocs: --nthreads:\n</code></pre>"},{"location":"Tutorials/SLURM/SLURM/#open-extra-terminal-for-the-three-nodes","title":"Open extra terminal for the three nodes","text":""},{"location":"Tutorials/SLURM/SLURM/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"Tutorials/SLURM/SLURM/#observed-that-the-work-is-distibuted-across-all-three-nodes","title":"Observed that the work is distibuted across all three nodes.","text":"<p>This may look different for different frameworks (e.g. Ray, Dask, R). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"Tutorials/SLURM/SLURM/#files","title":"Files","text":""},{"location":"Tutorials/SLURM/SLURM/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"Tutorials/SLURM/SLURM/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"Tutorials/SLURM/SLURM/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"Tutorials/SLURM/SLURM/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"Tutorials/STATA/batch/","title":"UCloud Tutorial: Run Stata in Batch Mode on UCloud","text":"<p>This is an approach to adress the UCloud capacity issues. </p> <p>UCloud batch processing apps are scheduled to run as resources permit without end user interaction. It allows </p> <p>Prerequisite reading:</p> <ul> <li>Install Stata on UCloud</li> </ul>"},{"location":"Tutorials/STATA/batch/#run-stata-scrip-in-batch-mode-n-a-new-terminal-job","title":"Run Stata scrip in batch mode n a new terminal job","text":"<p>Add the \"stata17\" and other relevant folder to the job:</p> <p></p> <p>Add a bash script(.sh) under \"Batch processing\" as one of the \"Optional Parameters\":</p> <p> </p> <p>Below shown bash script can be downloaded from here. Use this as a template or create your own bash script.</p> <p>More information on how to run Stata in batch mode can be found here: https://www.stata.com/support/faqs/unix/batch-mode/</p> <pre><code>#!/bin/bash\n\n\n# Installing dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata17 on UNIX path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Run stata in Batch mode\nstata -b do filename &amp; # USER SHOULD CHANGE THIS LINE (SEE LINK Above)\n</code></pre>"},{"location":"Tutorials/STATA/install/","title":"Install Stata on UCloud","text":"<p>This is a guide on how to install Stata on UCloud.</p>"},{"location":"Tutorials/STATA/install/#get-stata-license-and-installation-file-cbs-users","title":"Get Stata license and Installation file (CBS Users)","text":"<p>Follow the instructions to get a Stata license at CBS https://studentcbs.sharepoint.com/sites/ITandCampus/SitePages/en/Free-software.aspx</p> <p>You will recieve an email with license and installation information (see image below).</p> <p></p> <p>Download the installation file (Stata17Linux64.tar) and upload this to your UCloud directory.</p> <p></p>"},{"location":"Tutorials/STATA/install/#installing-stata-on-ucloud","title":"Installing Stata on UCloud","text":""},{"location":"Tutorials/STATA/install/#launch-a-terminal-app-ucloud-job-and-include-the-stata-installation-file-stata17linux64tar","title":"Launch a \"Terminal App\" UCloud Job and include the stata installation file (Stata17Linux64.tar)","text":"<p>Run following commands in the terminal: </p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Unzip installation file to temp folder\nsudo -s\nmkdir /tmp/statafiles\ncd /tmp/statafiles\ntar -zxf /work/install/Stata17Linux64.tar.gz\n\n# Install Stata on in \"/work/stata17\". Say yes when asked during installtion\nmkdir /work/stata17 cd /work/stata17 /tmp/statafiles/install\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Initialize Stata\nsudo /work/stata17/stinit\n\n# Follow instructions and add \"Serial number\", \"Code\" and \"Authorization\" from the Stata license mail\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata # or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"Tutorials/STATA/install/#end-job-and-copy-the-stata17-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-directory","title":"End job and copy the \u201cstata17\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud directory.","text":""},{"location":"Tutorials/STATA/install/#activate-stata-installation-in-a-new-terminal-job","title":"Activate Stata installation in a new terminal job","text":"<p>Add the stata17 folder to the job</p> <p></p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata # or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"Tutorials/STATA/jupyter/","title":"Run Stata in jupyter-notebooks","text":"<p>This is a guide shows how to setup \"pystata\" in order to run Stata in Python using jupyter notebooks.</p> <p>For more in depth decumnetation see here</p> <p>Prerequisite reading:</p> <ul> <li>Install Stata on UCloud</li> </ul>"},{"location":"Tutorials/STATA/jupyter/#start-a-jupyter-job-in-ucloud","title":"Start a Jupyter Job in UCloud","text":"<p>Add the stata17 folder to the job</p> <p></p>"},{"location":"Tutorials/STATA/jupyter/#activate-stata-and-install-stata-setup","title":"Activate Stata and install stata-setup","text":"<p>Open the Terminal and run the code snippets below.</p> <p></p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Example Output\n/work/stata17/stata # use later\n\n# Install stata-setup using pip\npip install stata-setup\n\n# Install pystata using pip\npip install pystata\n</code></pre>"},{"location":"Tutorials/STATA/jupyter/#run-stata-in-a-jupyter-notebook","title":"Run Stata in a Jupyter notebook","text":""},{"location":"Tutorials/STATA/jupyter/#start-jupyter-interface","title":"Start Jupyter interface","text":""},{"location":"Tutorials/STATA/jupyter/#open-a-new-python-notebook","title":"Open a new python notebook","text":""},{"location":"Tutorials/STATA/jupyter/#configure-the-stata-installation","title":"Configure the stata installation","text":"<pre><code>import stata_setup\n\nstata_setup.config(\"/work/stata17\", \"se\")\n\n# Output\n\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE\u2014Standard Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nCBS Account\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n2. Maximum number of variables is set to 5,000; see help set_maxvar.\n</code></pre>"},{"location":"Tutorials/STATA/jupyter/#run-your-code-using-the-stata-magic-stata-the-configure-the-stata-installation","title":"Run your code using the stata magic (%%stata) the Configure the stata installation","text":"<p>\"%%stata\" - cell magic is used to execute Stata code within a cell.</p> <p>\"%stata\" - line magic provides users a quick way to execute a single-line Stata command.</p> <p>Find more information on the stata magic here.</p> <pre><code>%%stata\n\nsysuse auto, clear\n\nsummarize mpg\n\n# Output\n. . sysuse auto, clear\n(1978 automobile data)\n\n. . summarize mpg\n\nVariable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmpg |         74     21.2973    5.785503         12         41\n\n. </code></pre> <p></p>"},{"location":"Tutorials/Sync/Rsync/","title":"UCloud Tutorial: Transfer large data to UCloud using Rsync","text":"<p>Rsync, short for \"remote synchronization,\" is a powerful and widely used file synchronization and transfer utility. It enables efficient copying and updating of files between different locations, whether they are on the same system or across a network. Rsync is particularly valuable for managing large data sets or performing incremental backups.</p> <p>What sets Rsync apart is its ability to synchronize files by transferring only the differences between the source and destination files. This delta transfer mechanism greatly reduces the amount of data that needs to be transmitted, making Rsync highly efficient, even for large files or slow network connections.</p> <p>Rsync also offers several advanced features, such as compression, encryption, and the ability to preserve various file attributes, such as permissions, timestamps, and symbolic links. It supports both local file copying and remote transfers via SSH, allowing secure synchronization between different systems.</p> <p>With its flexibility, speed, and efficient use of network resources, Rsync has become a go-to tool for tasks like backup and mirroring, remote file distribution, and content deployment. It has a command-line interface, making it scriptable and suitable for both one-time transfers and automated, scheduled tasks.</p> <p>UCloud documentation on Rsync</p>"},{"location":"Tutorials/Sync/Rsync/#installing-ubuntu-on-local-machine-for-windows","title":"Installing Ubuntu on local machine (For Windows)","text":"<p>Rsync is not natively available for Windows. However, you can install Rsync on Windows using a third-party implementation such as Cygwin or DeltaCopy. Alternatively, you can install Ubuntu on Windows which then comes whic includes Rsync.</p> <p>In this guide we will use Rsync through Ubuntu. For more infomation and video tutorials can be found here.</p>"},{"location":"Tutorials/Sync/Rsync/#create-a-ssh-key-within-your-ubuntu-environment","title":"Create a SSH-key within your Ubuntu environment","text":"<p>Despite already having a shh-key (in your windows environment) the easiest will be to create a new SSH-key within your Ubuntu environment. Open a terminal and follow the few steps below.</p> <p>More information on how to generate a SSH key can be found here</p> <pre><code># Activate Ubuntu \nwsl\n\n# For linux only \nsudo apt install openssh-client\n\n# Create key\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#copy-public-ssh-key","title":"Copy public SSH-key","text":"<pre><code># Open Public Key\nvim /home/user/.ssh/id_rsa.pub\n\n# highlight public key with mouse and copy using \"ctrl+c\"\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#add-public-ssh-key-to-ucloud","title":"Add public SSH key to UCloud","text":""},{"location":"Tutorials/Sync/Rsync/#step-1-on-ucloud-go-to-resources-shh-keys-create-ssh-key","title":"Step 1: On UCloud go to \"Resources -&gt; SHH-Keys -&gt; Create SSH key\"","text":""},{"location":"Tutorials/Sync/Rsync/#step-2-paste-pulic-key-give-a-meaningful-name-and-press-add-ssh-key","title":"Step 2: Paste pulic key, give a meaningful name and press \"Add SSH key\".","text":"<p>More information can be found in the UCloud documentation.</p>"},{"location":"Tutorials/Sync/Rsync/#start-rsync-job","title":"Start Rsync Job","text":""},{"location":"Tutorials/Sync/Rsync/#step-1-start-rsync-job-by-filling-out-the-necessary-fields","title":"Step 1: Start Rsync Job by filling out the necessary fields","text":""},{"location":"Tutorials/Sync/Rsync/#step-2-when-job-ready-please-locate-the-ssh-port-which-is-randomly-generated-in-the-cas-below-the-shh-port-is-2167","title":"Step 2: When job ready please locate the SSH port which is randomly generated. In the cas below the SHH port is 2167.","text":""},{"location":"Tutorials/Sync/Rsync/#connect-from-local-machine-using-ssh","title":"Connect from local machine using SSH.","text":"<p>Open Terminal and follow steps below.</p> <pre><code># Activate Ubuntu (for Windows)\nwsl\n\n# SHH connect using the command marked with in the figure above.\nssh ucloud@ssh.cloud.sdu.dk -p 2167\n</code></pre> <p>If sucessfull you should get the output shown in the figure below.</p> <p></p>"},{"location":"Tutorials/Sync/Rsync/#transfer-data-using-rsync","title":"Transfer data using Rsync","text":"<pre><code># Navigate to path contain the folder of files to transfer - Alternatively you can open terminal directly in the right directory to skip step below.\ncd \"path/of/folders-or-files\" # Activate Ubuntu (for Windows)\nwsl\n\n# SSH transfer \"myfolder\" to /work directory on UCloud \n\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p 2167\" ./myfolder/ ucloud@ssh.cloud.sdu.dk:/work/myfolder </code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/","title":"DieC large memory HPC/TYPE 3 (Hippo): Use Conda for easy management of Python and R environments","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Type3/Conda_Jupyter/#install-conda-on-type-3","title":"Install Conda on Type 3","text":"<pre><code># Download miniconda \ncurl -s -L -o /miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash miniconda_installer.sh -b -f -p miniconda3\n</code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code># Set miniconda3 to path\nexport PATH=\"$HOME/miniconda3/bin:$PATH\"\n\n# activate Conda\nsource $HOME/miniconda3/bin/activate\n\n# Initiate Conda\nconda init &amp;&amp; bash -i\n</code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/#install-python-or-r-environments-using-conda","title":"Install Python or R environments using Conda","text":"<pre><code># Python\nconda create -n myenv_py python\nconda activate myenv_py\nconda install ipykernel\nipython kernel install --name myenv_py --user # Make python available to JupyterLab\n\n# R\nconda create --solver=libmamba -n myenv_R -y -c conda-forge r-base=4.2.1 #\nconda activate myenv_R\nconda install -c conda-forge r-irkernel\nconda install jupyterlab\nR -e \"IRkernel::installspec(name = 'myenv_R', displayname = 'myenv_R')\" # Make R available to JupyterLab\n</code></pre>"},{"location":"Tutorials/Type3/Conda_Jupyter/#start-jupyter-interface","title":"Start Jupyter interface","text":""},{"location":"Tutorials/Type3/Conda_Jupyter/#add-token-to-open-jupyter","title":"Add token to open jupyter","text":""},{"location":"Tutorials/Type3/Conda_Jupyter/#now-the-conda-environments-are-available-as-a-jupyter-kernel","title":"Now the conda environments are available as a Jupyter Kernel","text":""},{"location":"Tutorials/Type3/Stata/","title":"Install Stata on DieC large memory HPC/TYPE 3 (Hippo)","text":"<p>This is a guide on how to install Stata on DieC large memory HPC/TYPE 3 (Hippo).</p> <p>Prerequisite reading:</p> <ul> <li>Using Conda for easy management of Python and R environments</li> </ul>"},{"location":"Tutorials/Type3/Stata/#get-stata-license-and-installation-file-cbs-users","title":"Get Stata license and Installation file (CBS Users)","text":"<p>Follow the instructions to get a Stata license at CBS https://studentcbs.sharepoint.com/sites/ITandCampus/SitePages/en/Free-software.aspx</p> <p>You will recieve an email with license and installation information (see image below).</p> <p></p> <p>Download the installation file (Stata17Linux64.tar) and upload this to your UCloud directory.</p> <p></p>"},{"location":"Tutorials/Type3/Stata/#installing-stata-on-type-3","title":"Installing Stata on Type 3","text":""},{"location":"Tutorials/Type3/Stata/#launch-a-terminal-app-ucloud-job-and-include-the-stata-installation-file-stata17linux64tar","title":"Launch a \"Terminal App\" UCloud Job and include the stata installation file (Stata17Linux64.tar)","text":"<p>Run following commands in the terminal: </p> <pre><code># Unzip installation file to temp folder\nmkdir /home/user/statafiles\ncd /home/user/statafiles\ntar -zxf /home/user/Stata17Linux64.tar.gz\n\n# Install Stata on in \"/home/user/stata17\". Say yes when asked during installtion\nmkdir /home/user/stata17 cd /home/user/stata17\n\n/home/user/statafiles/install\n\n# Set stata to Unix path\nexport PATH=\"/home/user/stata17:$PATH\"\n\n# Initialize Stata\n/home/user/stata17/stinit\n\n# Follow instructions and add \"Serial number\", \"Code\" and \"Authorization\" from the Stata license mail\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata\n# or\nstata-se\n# or\nstata-mp\n\n# Get following dependency error: \nstata: error while loading shared libraries: libncurses.so.5: cannot open shared object file: No such file or directory\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#install-dependencies-using-easybuild","title":"Install dependencies using Easybuild","text":"<pre><code>eb ncurses-5.9.eb -r\n\nmodule load ncurses/5.9\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#run-stata","title":"Run Stata","text":"<pre><code># Run stata\nstata\n# or\nstata-se\n# or\nstata-mp\n\n# Output\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       BE\u2014Basic Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nType 3\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n\n.\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#stata17-and-easybuild-will-not-be-placed-on-the-hippo-home-folder","title":"\u201cstata17\u201d and \"easybuild\" will not be placed on the Hippo Home folder","text":""},{"location":"Tutorials/Type3/Stata/#activate-stata-on-a-new-type-3-job","title":"Activate Stata on a new Type 3 Job","text":"<p>Add the stata17 folder to the job</p> <p></p> <pre><code># Set stata to Unix path\nexport PATH=\"/home/user/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Load dependies\nmodule load ncurses/5.9\n\n# Run stata\nstata\n# or\nstata-se\n# or\nstata-mp\n\n# Output\n\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       BE\u2014Basic Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nType 3\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n\n.\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#create-a-conda-stata-environment","title":"Create a Conda Stata environment","text":"<p>Assumes that miniconda3 has been installed. For more information on how to install conda on Type 3 see here.</p> <pre><code># Create conda environment\nconda create -n myenv_stata python\nconda activate myenv_stata\nconda install ipykernel\npip install stata-setup\npip install pystata\nipython kernel install --name myenv_stata --user # Make python available to JupyterLab\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#run-stata-in-a-jupyter-notebook","title":"Run Stata in a Jupyter notebook","text":""},{"location":"Tutorials/Type3/Stata/#start-jupyter-interface","title":"Start Jupyter interface","text":""},{"location":"Tutorials/Type3/Stata/#add-token-to-open-jupyter","title":"Add token to open jupyter","text":""},{"location":"Tutorials/Type3/Stata/#open-a-new-python-notebook","title":"Open a new python notebook","text":""},{"location":"Tutorials/Type3/Stata/#configure-the-stata-installation","title":"Configure the stata installation","text":"<pre><code>import stata_setup\n\nstata_setup.config(\"/work/stata17\", \"se\")\n\n# Output\n\n___  ____  ____  ____  ____ \u00ae\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE\u2014Standard Edition\n\nStatistics and Data Science       Copyright 1985-2021 StataCorp LLC\nStataCorp\n4905 Lakeway Drive\nCollege Station, Texas 77845 USA\n800-STATA-PC        https://www.stata.com\n979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 27 Dec 2023\nSerial number: 401709301397\nLicensed to: Kristoffer Gulmark Poulsen\nCBS Account\n\nNotes:\n1. Unicode is supported; see help unicode_advice.\n2. Maximum number of variables is set to 5,000; see help set_maxvar.\n</code></pre>"},{"location":"Tutorials/Type3/Stata/#run-your-code-using-the-stata-magic-stata-the-configure-the-stata-installation","title":"Run your code using the stata magic (%%stata) the Configure the stata installation","text":"<p>\"%%stata\" - cell magic is used to execute Stata code within a cell.</p> <p>\"%stata\" - line magic provides users a quick way to execute a single-line Stata command.</p> <p>Find more information on the stata magic here.</p> <pre><code>%%stata\n\nsysuse auto, clear\n\nsummarize mpg\n\n# Output\n. . sysuse auto, clear\n(1978 automobile data)\n\n. . summarize mpg\n\nVariable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmpg |         74     21.2973    5.785503         12         41\n</code></pre> <p></p>"},{"location":"Tutorials/VMs/","title":"Virtual Machines on UCloud","text":"<p>How to Generate SSH key</p> <p>Acessing VM using SSH</p>"},{"location":"Tutorials/VMs/condaVM/","title":"Conda: for easy workflow deployment on AAU GPU VMs","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following tutorial provides step-by-step guides on how to install and use Conda for R and Python on the AAU GPU VMs available on UCloud.</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a new AAU GPU VM.</p> <p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> </ul>"},{"location":"Tutorials/VMs/condaVM/#initial-installation-of-conda-on-a-aau-vm-job","title":"Initial installation of Conda on a AAU VM job","text":""},{"location":"Tutorials/VMs/condaVM/#connect-to-vm-using-ssh","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-driver-525 nvidia-utils-525 -y  # Or newer version\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#download-and-install-conda","title":"Download and Install Conda","text":"<pre><code># Download miniconda \ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # Install miniconda\nbash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#create-conda-environment-and-test-gpu-configuration","title":"Create conda environment and test GPU configuration","text":"<pre><code># Create conda environment \nconda deactivate\nconda create --name my_env python\nconda activate my_env\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit cudnn\n\n# Set pre-installed conda libraries to path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#gpu-conda-environment-is-ready-to-use","title":"GPU conda environment is ready to use","text":""},{"location":"Tutorials/VMs/condaVM/#compress-conda-installation-to-targz-file","title":"Compress Conda installation to tar.gz file","text":"<pre><code>tar -czvf /home/ucloud/miniconda3.tar.gz /home/ucloud/miniconda3\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-miniconda3targz-to-local-pc-using-ssh-copy","title":"Transfer \"miniconda3.tar.gz\" to local PC using SSH-Copy","text":"<p>Open a 2nd instance of a terminal app on local machine</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/miniconda3 \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-conda-install-to-a-new-aau-vm-job","title":"Transfer Conda install to a new AAU VM job","text":""},{"location":"Tutorials/VMs/condaVM/#connect-to-vm-using-ssh_1","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#update-vm_1","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-headless-460 nvidia-utils-460 -y\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-miniconda3targz-from-local-pc-to-vm-using-ssh-copy","title":"Transfer \"miniconda3.tar.gz\" from local PC to VM using SSH-Copy","text":"<p>Open a 2nd instance of a terminal app on local machine</p> <pre><code>scp -r \"C:\\path-to-folder\\miniconda.tar.gz ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#unzip-targz","title":"Unzip tar.gz","text":"<p>Move back to the terminal app connected to VM and run following command:</p> <pre><code>tar -xzf miniconda.tar.gz\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#set-conda-on-path-and-initialise","title":"Set Conda on path and initialise","text":"<pre><code># Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# init conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#re-connect-to-vm-using-ssh_1","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#check-nvidia-driver-configuration_1","title":"Check nvidia driver configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output:\nMon Aug  7 09:41:34 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   51C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#activate-conda-environment-and-test-gpu-configuration","title":"Activate conda environment and test GPU configuration","text":"<pre><code># Create conda environment \nconda deactivate\nconda create --name my_env python\nconda activate my_env\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit cudnn\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#gpu-conda-environment-is-ready-to-use_1","title":"GPU conda environment is ready to use","text":""},{"location":"Tutorials/VMs/connectVM/","title":"SSH to Server through local Terminal","text":"<p>Add public SSH key while starting a VM job</p> <p></p> <p>Identify VM IP when UCloud job is ready.</p> <p></p>"},{"location":"Tutorials/VMs/connectVM/#from-local-terminal-connect-to-vm-by","title":"From Local Terminal connect to VM by:","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#transfer-files-and-folders-ssh-copy","title":"Transfer Files and Folders (SSH-Copy)","text":""},{"location":"Tutorials/VMs/connectVM/#to-vm","title":"To VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path-to-folder-or-files\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#from-vm","title":"From VM","text":"<p>Open a second terminal (1st terminal is connected to the VM)</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/","title":"Run Python and R jupyter notebooks on AAU VMs","text":"<p>Prerequisite reading:</p> <ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Conda: for easy workflow deployment on AAU GPU VMs</p> </li> </ul>"},{"location":"Tutorials/VMs/jupyterVM/#connect-to-vm-using-ssh","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#install-or-activate-conda-installation","title":"Install or activate Conda installation","text":"<p>See \"Conda: for easy workflow deployment on AAU GPU VMs\" for more information.</p>"},{"location":"Tutorials/VMs/jupyterVM/#install-andor-activate-existing-python-or-r-environment-using-conda","title":"Install and/or activate existing Python or R Environment using Conda","text":"<pre><code># Python \nconda deactivate\nconda create --name myenv_python python\nconda activate myenv_python\nconda install ipykernel\nconda install nb_conda_kernels\n\n# R \nconda deactivate\nconda create --solver=libmamba -n myenv_R -y -c conda-forge r-base\nconda activate myenv_R\nconda install -c conda-forge r-irkernel\nconda install nb_conda_kernels\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit cudnn\n\n# Set pre-installed conda libraries to path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#check-jupyter-installtion-and-get-config-directory","title":"Check jupyter installtion and get config-directory","text":"<pre><code>which jupyter\n\n# Example Output:\n/home/ucloud/miniconda3/envs/my_env/bin/jupyter\n\n# Get config-directory\njupyter --config-dir\n\n# Example Output:\n/home/ucloud/.jupyter/\n\n# Create folder if does not exist\nmkdir -p /home/ucloud/.jupyter/\n\n# Create jupyter_config.json  in config-dir\necho '{\"CondaKernelSpecManager\": {\"kernelspec_path\": \"--user\"}}' &gt; /home/ucloud/.jupyter/jupyter_config.json\n\n# Check content of jupyter_config.json\n\ncat /home/ucloud/.jupyter/jupyter_config.json\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#install-nb_conda_kernels","title":"Install nb_conda_kernels","text":"<p>https://github.com/Anaconda-Platform/nb_conda_kernels#installation</p> <pre><code># Export all existing conda environment with ipykernel or r-irkernel installed\npython -m nb_conda_kernels list\n\n# Example Output: \n[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n[ListKernelSpecs] Removing existing kernelspec in /home/ucloud/.local/share/jupyter/kernels/conda-env-jupyter-py\n[ListKernelSpecs] Installed kernelspec conda-env-jupyter-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-jupyter-py\n[ListKernelSpecs] Installed kernelspec conda-env-my_env-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-my_env-py\n[ListKernelSpecs] Removing existing kernelspec in /home/ucloud/.local/share/jupyter/kernels/conda-env-myenv-py\n[ListKernelSpecs] Installed kernelspec conda-env-myenv-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-myenv-py\n[ListKernelSpecs] Removing existing kernelspec in /home/ucloud/.local/share/jupyter/kernels/conda-env-rapids-py\n[ListKernelSpecs] Installed kernelspec conda-env-rapids-py in /home/ucloud/.local/share/jupyter/kernels/conda-env-rapids-py\n[ListKernelSpecs] [nb_conda_kernels] enabled, 4 kernels found\nAvailable kernels:\nconda-env-jupyter-py    /home/ucloud/miniconda3/envs/jupyter/share/jupyter/kernels/python3\nconda-env-myenv-py      /home/ucloud/miniconda3/envs/myenv/share/jupyter/kernels/python3\nconda-env-rapids-py     /home/ucloud/miniconda3/envs/rapids/share/jupyter/kernels/python3\nconda-env-my_env-py     /home/ucloud/miniconda3/envs/my_env/share/jupyter/kernels/python3\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#check-that-the-conda-environment-kernels-are-discovered-by-jupyter","title":"Check that the conda environment kernels are discovered by jupyter:","text":"<pre><code>jupyter kernelspec list\n\n# Example output:\n[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\nAvailable kernels:\npython3                 /home/ucloud/miniconda3/envs/my_env/share/jupyter/kernels/python3\nconda-env-jupyter-py    /home/ucloud/.local/share/jupyter/kernels/conda-env-jupyter-py\nconda-env-my_env-py     /home/ucloud/.local/share/jupyter/kernels/conda-env-my_env-py\nconda-env-myenv-py      /home/ucloud/.local/share/jupyter/kernels/conda-env-myenv-py\nconda-env-rapids-py     /home/ucloud/.local/share/jupyter/kernels/conda-env-rapids-py\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#start-jupyter-notebook-from-remote-server","title":"Start Jupyter Notebook from remote server","text":"<pre><code>jupyter notebook --no-browser --port=8080 # Change the port number if multiple jupyter notebook are started within the same session\n\n# Output\n\n[I 10:26:32.873 NotebookApp] Serving notebooks from local directory: /home/ucloud\n[I 10:26:32.873 NotebookApp] The Jupyter Notebook is running at:\n[I 10:26:32.873 NotebookApp] http://localhost:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n[I 10:26:32.873 NotebookApp]  or http://127.0.0.1:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n[I 10:26:32.873 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:26:32.899 NotebookApp]\n\nTo access the notebook, open this file in a browser:\nfile:///home/ucloud/.local/share/jupyter/runtime/nbserver-3074-open.html\nOr copy and paste one of these URLs:\nhttp://localhost:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\nor http://127.0.0.1:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#ssh-connect-to-vm-using-a-new-terminal-app-on-local-machine","title":"SSH connect to VM using a new terminal app on local machine","text":"<p>Open a 2nd instance of Terminal on Local machine</p> <pre><code>ssh -L 8080:localhost:8080 ucloud@IP_address_from_the_red_mark # Change the port number if multiple jupyter notebook are started within the same session\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#open-jupyter-notebook","title":"Open Jupyter Notebook","text":"<p>Press the link in the output above and it should open a jupyter notebook</p> <p>Now the R and Python kernel should be available (see figure below)</p> <p></p> <p></p>"},{"location":"Tutorials/VMs/shh/","title":"How to Generate a SSH key","text":"<p>In order to access the VM it is necessary to create Secure Shell Protocol (SSH) keys more specifically a public key (shareable part) and a private key (kept safe locally).</p>"},{"location":"Tutorials/VMs/shh/#on-windows-linux-systems","title":"On Windows &amp; Linux Systems","text":"<p>To generate your public key, find and open terminal and type: </p> <pre><code># For linux only\nsudo apt install openssh-client\n\n\n# For both Windows &amp; Linux\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in C:\\Users\\user/.ssh/id_rsa.\nYour public key has been saved in C:\\Users\\user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/VMs/shh/#manually-locate-open-and-copy-our-public-key-from-id_rsapub-file","title":"Manually locate, open and copy our public key from id_rsa.pub file.","text":""},{"location":"Tutorials/VMs/shh/#on-windows","title":"On Windows","text":"<p>the file might be here: \"C:\\Users\\write_your_user_name.ssh\"</p>"},{"location":"Tutorials/VMs/shh/#on-linux","title":"On Linux","text":"<p>The generated SSH key will be by stored under ~/.ssh/id_rsa.pub by default.</p> <p>More information can be found at https://genome.au.dk/docs/getting-started/#public-key-authentication </p>"},{"location":"UCloud_SlurmCluster/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"UCloud_SlurmCluster/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"UCloud_SlurmCluster/Ray/","title":"Ray","text":""},{"location":"UCloud_SlurmCluster/Ray/#example-using-ray","title":"Example using Ray","text":"<p>In terminal run:</p> <pre><code>python slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Output\n\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#open-extra-terminal-for-three-nodes","title":"Open extra Terminal for three Nodes","text":""},{"location":"UCloud_SlurmCluster/Ray/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/Ray/#observed-that-the-work-is-disbrubted-across-all-three-nodes","title":"Observed that the work is disbrubted across all three nodes.","text":"<p>This may look different for different backends (e.g. Dask). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"UCloud_SlurmCluster/Ray/#output-files","title":"Output files","text":""},{"location":"UCloud_SlurmCluster/Ray/#the-autogenerated-slurm-script-slurmtest_0425-1208sh","title":"The autogenerated SLURM script (SlurmTest_0425-1208.sh)","text":"<pre><code>#!/bin/bash\n# shellcheck disable=SC2206\n# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!\n# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!\n\n#SBATCH --job-name=SlurmTest_0425-1208\n#SBATCH --output=SlurmTest_0425-1208.log\n\n### This script works for any number of nodes, Ray will find and manage all resources\n#SBATCH --nodes=3\n#SBATCH --exclusive\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n##SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE} #De-activated by KGP 230317\n\n# Load modules or your own conda environment here\n# module load pytorch/v1.4.0-gpu\n# conda activate ${CONDA_ENV}\n\n\n# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====\n\necho $SLURM_JOB_NODELIST\n\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\") # Getting the node names\n\nnodes_array=($nodes)\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w \"$node_1\" hostname --ip-address) # making redis-address\n\n# if we detect a space character in the head node IP, we'll\n# convert it to an ipv4 address. This step is optional.\nif [[ \"$ip\" == *\" \"* ]]; then\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$ip\"\nif [[ ${#ADDR[0]} -gt 16 ]]; then\nip=${ADDR[1]}\nelse\nip=${ADDR[0]}\nfi\necho \"IPV6 address detected. We split the IPV4 address as $ip\"\nfi\n\nport=6379\nip_head=$ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"STARTING HEAD at $node_1\"\nsrun --nodes=1 --ntasks=1 -w \"$node_1\" ray start --head --node-ip-address=\"$ip\" --port=$port --block &amp;\nsleep 30\n\n#worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\n#export NB_WORKERS=$((${SLURM_JOB_NUM_NODES-1})) #number of nodes other than the head node\n#echo ${NB_WORKERS}\n\nexport NB_WORKERS=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\necho \"STARTING ${NB_WORKERS} WORKERS\"\nfor ((i = 1; i &lt;= NB_WORKERS; i++)); do\nnode_i=${nodes_array[$i]}\necho \"STARTING WORKER $i at $node_i\"\nsrun --nodes=1 --ntasks=1 -w \"$node_i\" ray start --address \"$ip_head\" --block &amp;\nsleep 5\ndone\n\n# ===== Call your code below =====\necho \"RUNNING CODE: python /work/data/SklearnRay.py\"\npython /work/data/SklearnRay.py\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#autogenerated-log-file-slurmtest_0425-1208log","title":"Autogenerated log file (SlurmTest_0425-1208.log)","text":"<pre><code>node[0-2]\nIPV6 address detected. We split the IPV4 address as 10.42.47.86\nIP Head: 10.42.47.86:6379\nSTARTING HEAD at node0\n2023-04-25 12:08:40,054 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\nSTARTING 2 WORKERS\nSTARTING WORKER 1 at node1\n2023-04-25 12:08:38,026 INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-04-25 12:08:38,026 INFO scripts.py:710 -- Local node IP: 10.42.47.86\n2023-04-25 12:08:41,222 SUCC scripts.py:747 -- --------------------\n2023-04-25 12:08:41,222 SUCC scripts.py:748 -- Ray runtime started.\n2023-04-25 12:08:41,223 SUCC scripts.py:749 -- --------------------\n2023-04-25 12:08:41,223 INFO scripts.py:751 -- Next steps\n2023-04-25 12:08:41,223 INFO scripts.py:752 -- To connect to this Ray runtime from another node, run\n2023-04-25 12:08:41,223 INFO scripts.py:755 --   ray start --address='10.42.47.86:6379'\n2023-04-25 12:08:41,223 INFO scripts.py:771 -- Alternatively, use the following Python code:\n2023-04-25 12:08:41,223 INFO scripts.py:773 -- import ray\n2023-04-25 12:08:41,223 INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='10.42.47.86')\n2023-04-25 12:08:41,223 INFO scripts.py:790 -- To see the status of the cluster, use\n2023-04-25 12:08:41,223 INFO scripts.py:791 --   ray status\n2023-04-25 12:08:41,223 INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.\n2023-04-25 12:08:41,224 INFO scripts.py:809 -- To terminate the Ray runtime, run\n2023-04-25 12:08:41,224 INFO scripts.py:810 --   ray stop\n2023-04-25 12:08:41,224 INFO scripts.py:891 -- --block\n2023-04-25 12:08:41,224 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:41,224 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nSTARTING WORKER 2 at node2\n2023-04-25 12:08:48,882 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:48,933 I 2244 2244] global_state_accessor.cc:356: This node has an IP address of 10.42.28.36, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:48,859 INFO scripts.py:866 -- Local node IP: 10.42.28.36\n2023-04-25 12:08:48,935 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:48,935 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:48,935 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:48,935 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:48,935 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:48,935 INFO scripts.py:891 -- --block\n2023-04-25 12:08:48,935 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:48,935 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nRUNNING CODE: python /work/data/SklearnRay.py\n2023-04-25 12:08:54,215 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:54,271 I 956 956] global_state_accessor.cc:356: This node has an IP address of 10.42.34.213, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:54,135 INFO scripts.py:866 -- Local node IP: 10.42.34.213\n2023-04-25 12:08:54,274 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:54,275 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:54,275 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:54,275 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:54,275 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:54,275 INFO scripts.py:891 -- --block\n2023-04-25 12:08:54,275 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:54,275 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n2023-04-25 12:09:24,758 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.42.47.86:6379...\n2023-04-25 12:09:24,775 INFO worker.py:1553 -- Connected to Ray cluster.\n2023-04-25 12:09:25,073 WARNING pool.py:604 -- The 'context' argument is not supported using ray. Please refer to the documentation for how to control ray initialization.\nFitting 10 folds for each of 500 candidates, totalling 5000 fits\n209.00055767036974\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nslurmstepd-node2: error: *** STEP 2.3 ON node2 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node0: error: *** STEP 2.1 ON node0 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node1: error: *** STEP 2.2 ON node1 CANCELLED AT 2023-04-25T12:12:54 ***\nsrun: error: node2: task 0: Exited with exit code 1\nsrun: error: node1: task 0: Exited with exit code 1\nsrun: error: node0: task 0: Exited with exit code 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/","title":"UCloud Tutorial: Run Multi-node SLURM Cluster on UCloud","text":"TutorialFiles"},{"location":"UCloud_SlurmCluster/SLURM/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>In addition to the normal setting fill out the following options (See figure below).</p> <p>In this example launched as cluster consisting of 3 nodes with three folder added to the launch:</p> <ul> <li>\"miniconda3\"  - contains the conda environment I want to deploy across the different nodes.</li> <li>\"SLURM_deployment\" - contains the easy-to-use deployment scripts provided in this tutorial. </li> <li>\"SLURM_scripts\" - contains the user specific script and data to run on the cluster.</li> </ul> <p>In this example Conda is used for package and evironment management. Check here for more information on Conda on UCloud.</p> <p></p>"},{"location":"UCloud_SlurmCluster/SLURM/#when-the-job-has-started-open-terminal-for-node-1","title":"When the job has started open Terminal for Node 1","text":"<p>Run following commands in the terminal: </p> <pre><code># activate SLURM Cluster if not activated in the step above\ninit_slurm_cluster\n\n# List Avaliable nodes\nsinfo -N -l\n</code></pre> <p>The controller node is always the first node. Called \"node0\" in within SLURM but called \"Node 1\" in the UCloud interface). All additional nodes are named sequentially. For example, a cluster consisting of three full u1-standard nodes is configured as follows:</p> <pre><code>NODELIST   NODES PARTITION     STATE CPUS   S:C:T MEMORY\nnode0         1     CLOUD*     idle   64   1:64:1 385024\nnode1         1     CLOUD*     idle   64   1:64:1 385024\nnode2         1     CLOUD*     idle   64   1:64:1 385024\n</code></pre> <p>But called Node 1, Node 2 and Node 3 in the UCloud interface.</p>"},{"location":"UCloud_SlurmCluster/SLURM/#acitvate-conda-environment","title":"Acitvate Conda Environment","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which environment is in path (e.g. X = python,R..)\nwhich X # (e.g. X = python,R..)\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/X # (e.g. X = python,R..)\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-deployment-scripts","title":"SLURM deployment scripts","text":"<p>The SLURM deployment script (\"slurm-launch.py\") have been adopted from  Ray documentation to support the addition of other python libraries (Dask, ipyparallel) and other languages (e.g. R).</p>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-launchpy","title":"slurm-launch.py","text":"<p>\"slurm-launch.py\" auto-generates SLURM scripts and launch. slurm-launch.py uses an underlying template (e.g. \"slurm-template_ray.sh\" or \"slurm-template_dask.sh\") and fills out placeholders given user input.</p> <pre><code># Change path:\ncd /work/SLURM_deployment\n\n# Python with Ray\npython slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Python with Dask\npython slurm-launch.py --script slurm-template_dask.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnDask.py\" --num-nodes 3 --nprocs 8 --nthreads 1\n\n# R with doParallel\npython slurm-launch.py --script slurm-template_R.sh --exp-name SlurmTest --command \"Rscript --vanilla /work/SLURM_scripts/doParallel.r\" --num-nodes 3 --nprocs 8 --nthreads 1 # Example of Output\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#addditionel-options","title":"Addditionel options","text":"<pre><code>--exp-name          # The experiment name. Will generate {exp-name}_{date}-{time}.sh and {exp-name}_{date}-{time}.log.\n--command           # The command you wish to run. For example: rllib train XXX or python XXX.py.\n--node (-w)         # The specific nodes you wish to use, in the same form as the output of sinfo. Nodes are automatically assigned if not specified.\n--num-nodes (-n)    # The number of nodes you wish to use. Default: 1.\n--partition (-p):   # The partition you wish to use. Default: \u201c\u201d, will use user\u2019s default partition.\n--load-env:         # The command to setup your environment. For example: module load cuda/10.1. Default: \u201c\u201d.\n--nprocs: --nthreads:\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#open-extra-terminal-for-the-three-nodes","title":"Open extra terminal for the three nodes","text":""},{"location":"UCloud_SlurmCluster/SLURM/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/SLURM/#observed-that-the-work-is-distibuted-across-all-three-nodes","title":"Observed that the work is distibuted across all three nodes.","text":"<p>This may look different for different frameworks (e.g. Ray, Dask, R). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"UCloud_SlurmCluster/SLURM/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/SLURM/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/SLURM/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/SLURM/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/SLURM/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"}]}