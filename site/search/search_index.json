{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC &amp; Data Science Support at CBS","text":"<p>This is the GitHub repository for HPC &amp; Data Science Support at CBS. The team is dedicated to providing assistance and support to CBS researchers and students in their research utilizing the different HPC systems available at CBS. The repository contains various resources and information related to our services and activities.</p>"},{"location":"#activities","title":"Activities","text":"Tutorials &amp; User UtilitiesTeaching ActivitiesDevelopment of HPCDaily User SupportResearch Consultancy <p> The HPC &amp; Da Science Support team provides tutorials and user utilities to assist and support CBS researchers and students in their research utilizing the different HPC systems available at CBS. The tutorials and utilities covering various topics such as:</p> <ul> <li>Use cases for different HPC systems</li> <li>Efficient and secure data tranfer</li> <li>parallel computing</li> <li>environment management (e.g Conda)</li> </ul> <p>These tutorials offer step-by-step guidance, empowering users to effectively utilize HPC resources for their research. </p> <p> We conducts teaching activities through researcher and student webinars. Titles include \"High Performance Computing\", \"HPC &amp; Parallel Programming in R,\" and \"HPC &amp; Parallel Programming in Python\" with more in the pipeline.</p> <p>See \"Events\" section for more information.</p> <p>Upon receiving requests from course coordinators, we are also available to participate in teaching activities for courses at CBS.</p> <p> We are committed to the continuous development of HPC resources at CBS. This is both by ensuring that researchers have access to the right facilities, both short- and long-term, but also by providing a clear learning strategy for research to develop their HPC &amp; data science skillset.</p> <p> As Deic Front Office at CBS are we in charge off all communications with HPC system adminstrators (Back Office) and DeiC.</p> <p>Ideally, all user requests and troubleshooting should be send to the CBS Front Office(rdm@cbs.dk) as a Single Point of Contact (SPOC) where resulting tickets will be directed accordingly. </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p> <p> We provide consulting services to researchers and research projects, assisting them with their HPC requirements. Our support includes, but is not limited to, the following examples:</p> <ul> <li>HPC grant application guidance</li> <li>Assessing user needs for HPC resources</li> <li>Workflow and code optimization assistance</li> </ul> <p>By offering expert consultation, we help researchers identify and address their specific HPC needs, ensuring they can effectively utilize the available resources and optimize their workflows and code for maximum performance and efficiency.</p>"},{"location":"#most-relevant-links","title":"Most Relevant Links","text":"RPythonSTATAMatlabGPUs on UCloud <ul> <li>Getting Started with HPC (UCloud)</li> <li>Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> </ul> <ul> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul> <ul> <li>Getting Started with HPC (UCloud)</li> <li>SLURM Clusters on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> </ul> <ul> <li>Getting Started with HPC (UCloud)</li> <li>SLURM Clusters on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> </ul> <p>#</p>"},{"location":"#hpc-operational-status","title":"HPC Operational Status","text":"<ul> <li>TYPE 1 (UCloud)</li> <li>TYPE 3 (Hippo)</li> <li>TYPE 5 (LUMI)</li> </ul>"},{"location":"contact/","title":"Contact","text":"<ul> <li>Research &amp; Data Management @ CBS (rdm@cbs.dk)</li> <li>Kristoffer Gulmark Poulsen (kgp.lib@cbs.dk)</li> <li>Lars Nondal (ln.lib@cbs.dk)</li> </ul>"},{"location":"events/","title":"Events","text":""},{"location":"events/#upcoming","title":"Upcoming","text":"<p>No events are planned in the near future</p>"},{"location":"events/#past","title":"Past","text":"<p>\u00a0\u00a023-06-15 @ 11.00-12.00: - HPC &amp; Parallel Programming in Python (For Researchers)</p> <p>\u00a0\u00a023-06-14 @ 11.00-12.00: - HPC &amp; Parallel Programming in R (For Researchers)</p> <p>\u00a0\u00a023-06-08 @ 11.00-12.00: - High Performance Computing (For Researchers)</p>"},{"location":"getresources/","title":"Get Resources","text":""},{"location":"getresources/#local-resources","title":"Local Resources","text":"<p>Twice a year CBS is awarded Local HPC ressources that can be freely distributed to our researchers and students. </p> <ul> <li> <p>TYPE 1 (UCloud):  Currently, CBS primarily have Local Type 1 resources as the reflects our current user needs. See here to apply.</p> </li> <li> <p>TYPE 3 (Hippo):  Please contact RDM Support if you would like to CBS to request Local resources to Type 3.</p> </li> <li> <p>Other:   Please contact RDM Support if you would like to CBS to request Local resources to Type 2 and 5.</p> </li> </ul>"},{"location":"getresources/#sandbox-resources","title":"Sandbox Resources","text":"<p>CBS researchers wanting to test out HPC systems Type 2 to 5 can gain acess to sandbox ressources by contacting RDM Support. Find more information here.</p>"},{"location":"getresources/#grant-applications","title":"Grant Applications","text":"<p>Find an overview of currently open application rounds below. Please contact RDM Support as soon as possible if you consider applying as we can aid in the application process.</p> <ul> <li> <p>TYPE 1-3: Researcher can apply for the bi-annual application round for the national HPC resources. </p> </li> <li> <p>TYPE 5 &amp; other international HPC systems: Researcher can apply for resources at LUMI and other international HPC facilities. </p> </li> </ul>"},{"location":"getresources/#current-calls","title":"Current Calls","text":"<ul> <li>DeiC - Call H1-2024 Call for applications for access to the e- resources - (Deadline: September 5th, 2023)</li> </ul>"},{"location":"getresources/#external-links","title":"External Links","text":"<ul> <li>Apply for national HPC resources</li> <li>Acknowledge the use of national HPC </li> <li>The EuroCC Knowledge Pool (Hosted by DeiC)</li> </ul>"},{"location":"hpc/","title":"HPC Facilites","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>National HPC Facilities (DeiC)</li> <li>WRDS - Wharton Research Data Services</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> </ul>"},{"location":"news/","title":"News","text":"<ul> <li>23-07-24 - CodeRefinery workshop September 19-21 and 26-28, 2023</li> <li>23-07-14 - DeiC - Call H1-2024 Call for applications for access to the e- resources</li> <li>23-06-27 - Interactive HPC lives up to highest international standards with ISO 27001</li> <li>23-06-12 - New way to use SSH for accessing apps on Interactive HPC</li> <li>23-05-31 - UCloud Maintenance notice - 27/06/2023</li> <li>23-04-26 - UCloud scheduled maintenance between 8:00-10:00 on Wednesday 26/04/2023.</li> <li>23-04-12 - New milestone as DeiC Interactive HPC reaches 6,000 users</li> <li>23-03-17 - The cost of success \u2013 user overload on DeiC Interactive HPC</li> <li>23-03-15 - Launch of the DeiC Integration Portal</li> </ul>"},{"location":"tut_docs/","title":"Tutorials &amp; Documentation","text":""},{"location":"tut_docs/#cbs-tutorials","title":"CBS Tutorials","text":"<ul> <li>Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library</li> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Access GPUs on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul>"},{"location":"tut_docs/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>SDU</p> <ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul> <p>AAU</p> <ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"tut_docs/#type-2-throughput","title":"TYPE 2 (Throughput)","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"tut_docs/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"tut_docs/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"tut_docs/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"tut_docs/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"HPC_Facilities/DeiC/","title":"National HPC Facilities","text":"<p>This page provides an overview of the national HPC facilities (Content is provided by DeiC). </p>"},{"location":"HPC_Facilities/DeiC/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>The type 1 system is mainly focused on interactive computing and easy access for users. The system is made of the YouGene cluster hosted at SDU. CBS staff and students can access the cluster resources via UCloud. </p> <p>Get for Type 1 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/DeiC/#type-2-throughput-hpc","title":"TYPE 2 (Throughput HPC)","text":"<p>Three Type 2 HPC systems are available (Computerome 2.0,GenomeDK and Sophia). This type of HPC system typically has a large number of cores which can be a mix between cost-effective and calculation-efficient units. Type 2 also has the ability to handle large amounts of data and its main focus is on high-throughput performance. </p> <p>Get for Type 2 resources here.</p> <p>More information is found here:</p> <p>Computerome 2.0  \u00a0\u00a0 | \u00a0\u00a0 GenomeDK \u00a0\u00a0 | \u00a0\u00a0 Sophia</p>"},{"location":"HPC_Facilities/DeiC/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<p>This is a large memory HPC. This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area. </p> <p>Type 3 is hosted and maintained at SDU. For the cluster specs check here. </p> <p>User guide can be found here. </p> <p>Get for Type 3 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/DeiC/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<p>Type 5 is the European pre-exascale supercomputer LUMI. LUMI is an abbreviation for \"Large Unified Modern Infrastructure\", and will be located in CSC's data center in Kajaani, Finland. LUMI is one of three pre-exascale supercomputers to be build as part of the European EuroHPC project.</p> <p>LUMI Capability HPC provides a similar setup to DeiC Throughput HPC but with increased possibilities by virtue of state-of-the-art hardware. Specifically the interconnections between compute nodes is designed to minimize latency thereby addressing the issue of communication induced latency in distributed-memory programs running on separate nodes. Additionally the user can obtain access to large amounts of disk space also with low-latency interconnects. In this way Capability HPC enables computations that are prohibitive with DeiC Throughput HPC due to communication latency. </p> <p>Get for Type 5 resources here.</p> <p>More information is found here.</p>"},{"location":"HPC_Facilities/GrantApp/","title":"Applying for ressources in 6 Simple Steps","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/GrantApp/#step-1-select-apply-for-resources-on-the-ucloud-frontpage","title":"Step 1: Select \"Apply for resources\" on the UCloud frontpage","text":""},{"location":"HPC_Facilities/GrantApp/#step-2-select-apply-for-new-project-instead","title":"Step 2: Select \"Apply for new project instead\"","text":""},{"location":"HPC_Facilities/GrantApp/#step-3-provide-a-title-and-choice-hpc-type-1-or-3","title":"Step 3: Provide a Title and choice HPC type (1 or 3)","text":""},{"location":"HPC_Facilities/GrantApp/#step-4-choice-storage-amount-and-machine-typein-dkk","title":"Step 4: Choice Storage amount and Machine Type(in DKK).","text":""},{"location":"HPC_Facilities/GrantApp/#only-deic-interactive-hpc-sdu-u1-standard-cpu-deic-interactive-hpc-aau-uc-t4-gpu-are-relevant","title":"Only \"DeiC Interactive HPC (SDU) / u1-standard\" (CPU) &amp; \"DeiC Interactive HPC (AAU) / uc-t4\" (GPU) are relevant.","text":""},{"location":"HPC_Facilities/GrantApp/#step-5-provide-a-meaningfull-project-description-select-sas-or-stata-license-if-needed","title":"Step 5: Provide a meaningfull project description &amp; select SAS or STATA License (if needed).","text":""},{"location":"HPC_Facilities/GrantApp/#step-6-press-submit-application","title":"Step 6: Press \"Submit application\"","text":""},{"location":"HPC_Facilities/GrantApp/#now-the-application-will-be-evaluated-by-the-cbs-front-office-at-first-given-opportunity-the-application-will-either-be-accepted-otherwise-you-will-be-contacted-cbs-mail","title":"Now the application will be evaluated by the CBS front office at first given opportunity. The application will either be accepted otherwise you will be contacted (CBS mail).","text":""},{"location":"HPC_Facilities/Hippo/","title":"Type 3 \u2013 Large Memory HPC (Hippo)","text":"<p>This type of HPC system focuses on problem solving, with a structure that cannot be easily or efficiently distributed between many computer nodes. This is a type of system that is characterized by typically relatively few cores with access to a large globally addressable memory area.  Type 3 is hosted and maintained at SDU. </p> <p>The following guide covers the follow:</p> <p>Facility Overview \u00a0\u00a0| \u00a0\u00a0 Get started with Large Memory HPC (Hippo) \u00a0\u00a0| \u00a0\u00a0 User Support \u00a0\u00a0| \u00a0\u00a0 Apply for Funds</p>"},{"location":"HPC_Facilities/Hippo/#facility-overview","title":"Facility Overview","text":"<p>The DeiC Large Memory HPC system is a system consisting of large memory nodes (between 1 and 4 TB RAM per node) configured as a traditional Slurm cluster. See here for more information</p>"},{"location":"HPC_Facilities/Hippo/#get-started-with-large-memory-hpc-hippo","title":"Get started with Large Memory HPC (Hippo)","text":"<p>The user guide can be found at this link.</p>"},{"location":"HPC_Facilities/Hippo/#user-support","title":"User Support","text":"<p>All UCloud support should go through the CBS front office(rdm@cbs.dk). If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/Hippo/#apply-for-funds","title":"Apply for Funds","text":""},{"location":"HPC_Facilities/Hippo/#students","title":"Students","text":"<p>In order for CBS students to gain  can only have direct access to Type 3 ress the initial 1000kr credit and 50 GB storage. When you need more/other resources, it must go through your thesis supervisor who needs to apply for funds and invite you to join the UCloud project. </p> <p>If this is not possible then you welcome to contact RDM Support to discuss further.</p> <p>The ressources will be provided within a UCloud project and not to a user \"My Workspace\".</p>"},{"location":"HPC_Facilities/Hippo/#researchers-staff","title":"Researchers &amp; Staff","text":"<p>Further funds can be obtianed in two ways: </p> <ol> <li> <p>Apply to the local CBS ressource pool. You apply from UCloud by sending a UCloud grant application. Information on machine type selection be found here. Otherwise please contact RDM Support.</p> </li> <li> <p>Apply for the bi-annual application round for the national HPC resources. Please contact RDM Support as soon as possible if you consider applying.</p> </li> </ol> <p>For both ways the ressources will be provided to a UCloud project and not to a user \"My Workspace\". Each UCloud project will be given a reference number (DeiC-XX-Y NUMBER).</p> <p>This number should be used to acknowledge the use of national HPC in publications.</p>"},{"location":"HPC_Facilities/License/","title":"Add License to STATA and SAS application","text":"<p>If you have any further questions you are welcome to contact RDM Support.</p>"},{"location":"HPC_Facilities/License/#add-local-license","title":"Add Local License","text":""},{"location":"HPC_Facilities/License/#step-1-upload-local-stata-lic-file-or-sas-txt-file-license-to-ucloud","title":"Step 1: Upload local STATA (.lic file) or SAS (.txt file) license to UCloud","text":""},{"location":"HPC_Facilities/License/#step-2-select-the-license-file-lic-or-txt-while-setting-up-ucloud-job","title":"Step 2: Select the license file (.lic or .txt) while setting up UCloud Job","text":""},{"location":"HPC_Facilities/License/#add-server-license","title":"Add Server License","text":""},{"location":"HPC_Facilities/License/#step-1-apply-for-server-license-through-ucloud-grant-application","title":"Step 1: Apply for Server License through UCloud Grant Application","text":""},{"location":"HPC_Facilities/License/#step-2-activate-license-sas-94-license-shown-as-example","title":"Step 2: Activate License (SAS 9.4 license shown as example)","text":""},{"location":"HPC_Facilities/License/#step-3-select-server-license-while-setting-up-ucloud-job","title":"Step 3: Select server license while setting up UCloud Job","text":""},{"location":"HPC_Facilities/MachineType/","title":"Type 1 - DeiC Interactive HPC","text":"<p>Find more information here.</p>"},{"location":"HPC_Facilities/MachineType/#deic-interactive-hpc-sdu-u1-standard-cpu","title":"DeiC Interactive HPC (SDU) / u1-standard (CPU)","text":"<p>SDU provides CPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as you would on your laptop. See all apps. </p> <p>The following machines are available:</p> <p></p>"},{"location":"HPC_Facilities/MachineType/#u1-standard-64-specs","title":"u1-standard-64 Specs","text":"<p>Dell PowerEdge C6420</p> <p>vCPU:   64 (32 virtual cores) 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz</p> <p>RAM: 384 GB  DDR 4-2666</p> <p>Price: 5,49 DKK/hour</p> <p>Description: The full node consists of 2x Intel(R) Xeon(R) Gold 6130 CPU@2.10 GHz, 32 virtual cores/CPU, and 384 GB of memory.</p>"},{"location":"HPC_Facilities/MachineType/#deic-interactive-hpc-aau-uc-t4-gpu","title":"DeiC Interactive HPC (AAU) / uc-t4 (GPU)","text":"<p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab.</p> <p>The following machines are available:</p> <p></p>"},{"location":"HPC_Facilities/MachineType/#uc-t4-4-specs","title":"uc-t4-4 Specs","text":"<p>vCPU:   40 cores</p> <p>RAM: 160 GB</p> <p>GPU:    4</p> <p>Price:  33,99 DKK/hour</p> <p>Description:    Virtual machine with four NVIDIA T4 GPUs deployed on the AAU OpenStack system.</p>"},{"location":"HPC_Facilities/NGC/","title":"Nationalt Genom Center HPC (Danish Statistics Data)","text":"<p>It is possible to access and process data from Danish Statistics through the Nationalt Genom Center HPC. This service is not funded by CBS. See pricing list.</p> <p>A techinal guide is found here.</p> <p>The linked information is only avaliable in danish.</p> <p>For further information or support please contact RDM Support.</p>"},{"location":"HPC_Facilities/Overview/","title":"Overview","text":"<ul> <li>Type 1 \u2013 Interactive HPC (UCloud)</li> <li>National HPC Facilities (DeiC)</li> <li>WRDS - Wharton Research Data Services</li> <li>Nationalt Genom Center HPC (Danish Statistics Data)</li> <li>HPC Operational Status</li> </ul>"},{"location":"HPC_Facilities/UCloud/","title":"Type 1 \u2013 Interactive HPC (UCloud)","text":"<p>The easiest-to-use HPC service is DeiC Interactive HPC (Type 1) also known as UCloud. This service is provided by the Danish universities SDU and AAU.</p>"},{"location":"HPC_Facilities/UCloud/#facility-overview","title":"Facility Overview","text":"<p>SDU provides CPU based containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI), in the same way as they would on your laptop. See all apps. </p> <p>AAU provides primary GPU based virtual machines. Access is obtained through terminal and SSH. It is possible to set up interactive enviroments such as JupyterLab. More information can be found here.</p>"},{"location":"HPC_Facilities/UCloud/#login-on-ucloud","title":"Login on UCloud","text":"<p>You can login on to UCloud using WAYF (Where Are You From). Press here to login.</p> <ul> <li>Select Copenhagen Business School as your affiliate institution on the login page. </li> <li>Sign in using your CBS mail account</li> </ul> <p>Upon the first login it is necessary to approve the SDU eScience terms of service. Afterwards, the user is redirected to the UCloud user interface.</p> <p>Note: After login the user can activate two factor authentication by clicking on the avatar icon in the top-right corner of the home screen.</p>"},{"location":"HPC_Facilities/UCloud/#geting-started","title":"Geting started","text":"<p>All new users in UCloud are awarded a \"My Workspace\" with 1000 DKK of computing (CPU only) resources to the \"DeiC Interactive HPC (SDU)\", as well as 50 GB remote storage. You can use these resources to get acquainted with the system, run test jobs, etc. </p> <p>\"DeiC Interactive HPC (SDU)\" provides broadest ranges of containerized applications such as MATLAB, STATA, RStudio, and JupyterLab through a graphical user interface (GUI). See all apps.</p> <p>The largest machine (64 cores &amp; 384 GB memory) cost 5.49kr/Hour. So the free 1000 DKK will give you access to approx. 182 hours of inital run time.</p> <p>Start by watching the following UCloud tutorials:</p> <ul> <li>Manage Files and Folders</li> <li>Manage Applications</li> <li>Manage Workspaces</li> <li>Use Cases</li> <li>Webinars</li> <li>UCloud Documentation</li> </ul> <p>More Tutorials and Documentation can be found here</p>"},{"location":"HPC_Facilities/UCloud/#user-support","title":"User Support","text":"<p>All UCloud support should go through the RDM Support. If problems cannot be solved locally the CBS Front office will take contact to the UCloud system adminstrators (Back Office). </p> <p>This setup provides a better service to users and saves valuable time for Back Office technicians who can concentrate on highly technical issues.</p>"},{"location":"HPC_Facilities/UCloud/#collaboration","title":"Collaboration","text":"<p> International Collaborators</p> <p>International researchers need a \"visiting researcher premission\"(g\u00e6steforskeradgang) to CBS to gain access to UCloud. One can be obtained by contacting CBS HR (hr@cbs.dk).</p> <p>Once this is in place CBS HPC support will contact the UCloud Research Support Team and provide the below shown information. </p> <ul> <li> <p>Full name:</p> </li> <li> <p>Occupation:</p> </li> <li> <p>Organisation (University):</p> </li> <li> <p>Email (University):</p> </li> </ul> <p>Subsequently, the UCloud Research Support Team will contact the researcher to verify their identity through a video meeting. A valid ID is needed. </p>"},{"location":"HPC_Facilities/UCloud/#apply-for-funds","title":"Apply for Funds","text":"StudentsResearchers &amp; Staff <p> CBS student can only have direct access to the initial 1000kr credit and 50 GB storage. When you need more/other resources, it must go through your thesis supervisor who needs to apply for funds and invite you to join the UCloud project. </p> <p>If this is not possible then you welcome to contact RDM Support to discuss further.</p> <p>The ressources will be provided within a UCloud project and not to a user \"My Workspace\".</p> <p> Further funds can be obtianed in two ways: </p> <ol> <li> <p>Apply to the local CBS ressource pool. You apply from UCloud by sending a UCloud grant application. Information on machine type selection be found here. Otherwise please contact RDM Support.</p> </li> <li> <p>Apply for the bi-annual application round for the national HPC resources. Please contact RDM Support as soon as possible if you consider applying.</p> </li> </ol> <p>For both ways the ressources will be provided to a UCloud project and not to a user \"My Workspace\". Each UCloud project will be given a reference number (DeiC-XX-Y NUMBER).</p> <p>This number should be used to acknowledge the use of national HPC in publications.</p>"},{"location":"HPC_Facilities/UCloud/#license-software","title":"License Software","text":"<p>There are several types of licensed software that can be run on UCloud. </p> MATLABSTATASAS &amp; SAS Studio <p>  A Matlab server license is needed in order to run the application on UCloud. Once can be acquired through CBS IT help desk at own expense.</p> <ul> <li> <p>Matlab UCloud Application</p> </li> <li> <p>UCloud Matlab Documentation</p> </li> <li> <p>UCloud video tutorial </p> <ul> <li>Matlab walkthrough starts at 16:00 minutes into the video. </li> <li>Shows how activate Matlab with a personal license.</li> </ul> </li> </ul> <p> Users can either upload their own personal STATA license (.lic file) to UCloud or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>STATA UCloud Application</p> </li> <li> <p>UCloud STATA Documentation</p> </li> </ul> <p> Users can either upload their own personal SAS license (.txt file) or apply for one through a UCloud Grant Application.</p> <p>After being granted the license the user should perform the following steps. </p> <ul> <li> <p>SAS UCloud Application</p> </li> <li> <p>UCloud SAS Documentation</p> </li> </ul>"},{"location":"HPC_Facilities/WRDS/","title":"WRDS - Wharton Research Data Services","text":"<p>WRDS- Wharton Research Data Services provides access to financial data, accounting figures as well as banking and management information. CBS students and staff must register to get access. </p> <p>More information can be found here.</p> <p>For further support lease contact RDM Support.</p>"},{"location":"HPC_Facilities/WRDS/#accessing-wrds-databases","title":"Accessing WRDS databases:","text":""},{"location":"HPC_Facilities/WRDS/#local-pc-ucloud","title":"Local PC &amp; UCloud","text":"<ul> <li>Python</li> <li>R</li> <li>Matlab</li> <li>SAS</li> <li>STATA</li> <li>UCloud Templates/Scripts </li> </ul>"},{"location":"HPC_Facilities/WRDS/#wrds-cloud","title":"WRDS Cloud","text":"<p>WRDS Cloud is a HPC service with the possibility to process the data avaliable on WRDS. WRDS Cloud is only acessable for CBS staff and researchers.</p>"},{"location":"HPC_Facilities/WRDS/#available-software","title":"Available Software","text":"<p>The WRDS Cloud provides the following software:</p> <ul> <li>SAS 9.4</li> <li>R 3.5</li> <li>Python 3.6 and 2.7</li> <li>Stata 15 (requires special subscription agreement with StataCorp)</li> </ul> <p>In addition, it further supports remote access from:</p> <ul> <li>MATLAB 2016a +</li> <li>Native PostgreSQL clients</li> <li>ODBC- or JDBC-compliant clients</li> </ul>"},{"location":"HPC_Facilities/WRDS/#wrds-cloud-documentation","title":"WRDS Cloud Documentation","text":"<ul> <li>Introduction to the WRDS Cloud</li> <li>Using SSH to Connect to the WRDS Cloud </li> </ul>"},{"location":"HPC_Facilities/status/","title":"HPC Operational Status","text":"<ul> <li>Type 1 (UCloud)</li> <li>Type 3 (Hippo)</li> <li>Type 5 (LUMI)</li> </ul>"},{"location":"Tutorial_Docs/BLAS/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library","text":"<p>When it comes to numerical computations and linear algebra in the R programming language, the BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries play crucial roles. These libraries provide a collection of efficient and optimized routines for various linear algebra operations, such as matrix multiplication, solving linear systems, eigenvalue computations, and more.</p> <p>BLAS, the Basic Linear Algebra Subprograms, is a standard interface specification for low-level linear algebra operations. It defines a set of routines that perform basic vector and matrix operations efficiently. The BLAS routines are highly optimized and implemented in highly efficient machine code to take advantage of the specific hardware architecture. R relies on the BLAS library for fundamental linear algebra operations, providing performance improvements for numerical computations.</p> <p>LAPACK, the Linear Algebra Package, builds upon the BLAS library and provides higher-level routines for solving more complex linear algebra problems. LAPACK offers a comprehensive set of algorithms for solving systems of linear equations, eigenvalue problems, least-squares problems, and singular value decompositions. These routines are widely used in various scientific and engineering applications.</p>"},{"location":"Tutorial_Docs/BLAS/#r","title":"R","text":"<ul> <li>Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.</li> </ul>"},{"location":"Tutorial_Docs/BatchMode/","title":"UCloud Tutorial: Batch Processing on UCloud","text":"<p>Running non-interactive script in \"batch mode\" can help overcome the UCloud capacity issues as jobs can be started any time and then executed whenever the systems resources permits.  </p> <p>Many applications already have a \"Batch Processing\" UCloud functionality:</p> <ul> <li>Batch Processing</li> <li>RStudio</li> <li>JupyterLab</li> <li>Matlab</li> <li>PyTorch</li> <li>Tensorflow</li> <li>Spark Cluster</li> <li>Terminal/SLURM Cluster</li> </ul> <p>Application that currently does not have this UCloud functionality (e.g. Stata) can instead run batch mode computations using the terminal app. See the following tutorials:</p> <ul> <li>Stata</li> </ul>"},{"location":"Tutorial_Docs/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <p>R</p> <p>R (using Mamba add-in)</p> <p>Python</p> <p>Further documentation can be found on UCloud:</p> <p>Conda on UCloud </p>"},{"location":"Tutorial_Docs/SLURM/","title":"SLURM Clusters on UCloud","text":"TutorialLaunch File (slurm-launch.py)Ray (Python)Dask (Python)doParallel (R) slurm-template_ray.shSklearnRay.py slurm-template_dask.shSklearnDask.py slurm-template_R.shdoParallel.rtidyModel_RF.r"},{"location":"Tutorial_Docs/VMs/","title":"Access GPUs on UCloud","text":"<p>GPUs are accessible on UCloud through virtual machines hosted by AAU. The user requires computing ressources for \"DeiC Interactive HPC (AAU) / uc-t4 (GPU)\" in order to access this application. These can be obtained through a UCloud grant application. Information on hardware specification can be found here.</p>"},{"location":"Tutorial_Docs/VMs/#tutorials","title":"Tutorials","text":"<ul> <li> <p>How to Generate SSH key</p> </li> <li> <p>Access VM using SSH</p> </li> <li> <p>Using Conda for easy workflow deployment on AAU GPU VMs</p> </li> <li> <p>Setting up an interactive jupyter notebook session on AAU VM</p> </li> <li> <p>Setting up jupyter-notebook with GPUs on AAU using Docker images (Hosted by RUC)</p> </li> </ul>"},{"location":"Tutorials/","title":"Index","text":""},{"location":"Tutorials/#tutorials-documentation","title":"Tutorials &amp; Documentation","text":""},{"location":"Tutorials/#cbs-tutorials","title":"CBS Tutorials","text":"<ul> <li>Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library</li> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Virtual Machines on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul>"},{"location":"Tutorials/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":"<p>SDU</p> <ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul> <p>AAU</p> <ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"Tutorials/#type-2","title":"TYPE 2","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"Tutorials/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"Tutorials/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"Tutorials/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"Tutorials/Overview/","title":"Overview","text":""},{"location":"Tutorials/Overview/#tutorials-documentation","title":"Tutorials &amp; Documentation","text":""},{"location":"Tutorials/Overview/#cbs-tutorials","title":"CBS Tutorials","text":"<ul> <li>Getting Started with HPC (UCloud)</li> <li>Using Conda on UCloud to manage R-packages and Python-libraries</li> <li>SLURM Clusters on UCloud</li> <li>Virtual Machines on UCloud</li> <li>Batch Processing on UCloud</li> <li>Rsync - Large data transfer to UCloud</li> </ul>"},{"location":"Tutorials/Overview/#type-1-ucloud","title":"TYPE 1 (UCloud)","text":""},{"location":"Tutorials/Overview/#sdu","title":"SDU","text":"<ul> <li>Manage Files and Folders (Hosted by UCloud)</li> <li>Manage Applications (Hosted by UCloud)</li> <li>Manage Workspaces (Hosted by UCloud)</li> <li>Use Cases (Hosted by UCloud)</li> <li>Webinars (Hosted by UCloud)</li> <li>UCloud Documentation (Hosted by UCloud)</li> <li>Synchronization to UCloud (Hosted by UCloud)</li> <li>Quick guide on running JupyterLab on UCloud (Hosted by RUC) </li> </ul>"},{"location":"Tutorials/Overview/#aau","title":"AAU","text":"<ul> <li>Setting up jupyter-notebook with GPUs on AAU (Hosted by RUC)</li> </ul>"},{"location":"Tutorials/Overview/#type-2","title":"TYPE 2","text":"<ul> <li>Computerome 2.0 - Documentation</li> <li>GenomeDK - Documentation</li> <li>Sophia - Documentation</li> </ul>"},{"location":"Tutorials/Overview/#type-3-hippo","title":"TYPE 3 (Hippo)","text":"<ul> <li>User Guide (Hosted by UCloud)</li> </ul>"},{"location":"Tutorials/Overview/#type-5-lumi","title":"TYPE 5 (LUMI)","text":"<ul> <li>Cotainr (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/Overview/#general-hpc-documentation","title":"General HPC Documentation","text":"<ul> <li>ENCCS Lessons</li> <li>Virtual SLURM Learning (Hosted by DeiC)</li> </ul>"},{"location":"Tutorials/Overview/#python","title":"Python","text":"<ul> <li>High Performance Data Analytics in Python (Hosted by ENCCS) </li> </ul>"},{"location":"Tutorials/Overview/#other-links","title":"Other Links","text":"<ul> <li>DeiC HPC GitHub</li> <li>RUC HPC</li> <li>Code Refinery</li> </ul>"},{"location":"Tutorials/BLAS/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations by choosing the right BLAS/LAPACK Library","text":"<p>When it comes to numerical computations and linear algebra in the R programming language, the BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries play crucial roles. These libraries provide a collection of efficient and optimized routines for various linear algebra operations, such as matrix multiplication, solving linear systems, eigenvalue computations, and more.</p> <p>BLAS, the Basic Linear Algebra Subprograms, is a standard interface specification for low-level linear algebra operations. It defines a set of routines that perform basic vector and matrix operations efficiently. The BLAS routines are highly optimized and implemented in highly efficient machine code to take advantage of the specific hardware architecture. R relies on the BLAS library for fundamental linear algebra operations, providing performance improvements for numerical computations.</p> <p>LAPACK, the Linear Algebra Package, builds upon the BLAS library and provides higher-level routines for solving more complex linear algebra problems. LAPACK offers a comprehensive set of algorithms for solving systems of linear equations, eigenvalue problems, least-squares problems, and singular value decompositions. These routines are widely used in various scientific and engineering applications.</p>"},{"location":"Tutorials/BLAS/#r","title":"R","text":"<ul> <li>Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.</li> </ul>"},{"location":"Tutorials/BLAS/#python","title":"Python","text":"<ul> <li>Speed up your Numpy calculations</li> </ul>"},{"location":"Tutorials/BLAS/BLAS_Python/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.","text":"<p>In the context of R, there are several libraries available that interface with the BLAS and LAPACK libraries to enhance their functionality or provide alternative implementations. Here are a few notable libraries:</p> <ul> <li> <p>base R: The base R installation includes a reference BLAS implementation that provides basic linear algebra functionality. While this implementation is not as optimized as other libraries, it serves as a fallback option when more optimized libraries are not available.</p> </li> <li> <p>OpenBLAS: OpenBLAS is an optimized BLAS and LAPACK library that offers high-performance linear algebra routines. It is widely used and provides significant speed improvements over the reference BLAS implementation in base R.</p> </li> <li> <p>Intel MKL: The Intel Math Kernel Library (MKL) is a highly optimized set of mathematical functions for various platforms, including CPUs from Intel. It includes efficient implementations of BLAS and LAPACK routines and is known for its excellent performance. MKL is often favored for its outstanding performance on Intel architectures.</p> </li> <li> <p>ACML: The AMD Core Math Library (ACML) is an optimized library for AMD processors. It provides optimized BLAS and LAPACK routines tailored for AMD architectures.</p> </li> <li> <p>vecLib: vecLib is the BLAS and LAPACK library included with macOS. It provides optimized routines for Apple hardware.</p> </li> <li> <p>ATLAS: ATLAS (Automatically Tuned Linear Algebra Software) is another popular BLAS implementation that can be used as an alternative to the reference BLAS library. ATLAS utilizes an automated tuning process to generate highly optimized code specifically tailored to the host system's architecture. It offers improved performance for various linear algebra operations.</p> </li> </ul> <p>These libraries can be linked with R during installation or dynamically loaded during runtime, allowing users to choose the most suitable implementation for their specific hardware and performance requirements.</p> <p>In summary, the BLAS and LAPACK libraries are essential components for numerical computations and linear algebra in R. They provide efficient and optimized routines for fundamental linear algebra operations, and various libraries such as OpenBLAS, Intel MKL, ACML, vecLib, and ATLAS enhance their functionality or provide alternative implementations for improved performance.</p>"},{"location":"Tutorials/BLAS/BLAS_Python/#rstudio-on-ucloud","title":"RStudio on UCloud","text":"<p>When looking at the different R/RStudio version available on UCloud (see below) it can be observed that some have the suffix \"_MKL\". These have \"Intel MKL\" as their default BLAS/LAPACK library, while versions with out the suffix have the \"LibBLAST\" library whihc is the base R library on linux systems.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_Python/#use-the-sessioninfo-function-to-check-which-blaslapack-library-is-deployed","title":"Use the \"sessionInfo()\" function to check which BLAS/LAPACK library is deployed:","text":"<p>MKL</p> <p></p> <p>LibBLAS</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_Python/#benchmarking-the-two-versions","title":"Benchmarking the two versions","text":"<p>Benchmarking a simple matrix multiplication shows that the \"MKL\" is close 78 times faster than the \"LibBLAS\"!!</p> <p>MKL mean = 7.63 milliseconds</p> <p></p> <p>LiBLAS mean = 592.666 milliseconds</p> <p></p> <pre><code>library(microbenchmark)\n\nn &lt;- 1000\nmat1 &lt;- matrix(rnorm(n*n), ncol=n)\nmat2 &lt;- matrix(rnorm(n*n), ncol=n)\n\ntic()\nf &lt;- function() mat1 %*% mat2\nres &lt;- microbenchmark(f(), times=100L)\nres\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#installing-and-chaning-blas-lapack-library","title":"Installing and Chaning BLAS &amp; LAPACK Library","text":"<p>Select between the available BLAS/LAPACK libraies by posting the command below i the job terminal. </p> <pre><code># BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n# LAPACK Selection\nsudo update-alternatives --config liblapack.so.3-x86_64-linux-gnu\n\n\n\n# BLAS Selection Output: \nThere are 2 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#install-other-libraries","title":"Install other libraries.","text":"<p>Changing the BLAS/LAPACK libraries is not possible for the \"MKL\" versions of the RStudio applications on UCloud. </p> <pre><code># install OpenBLAS\nsudo apt-get install libopenblas-base\n# install ATLAS\nsudo apt-get install libatlas3-base liblapack3\n\n# BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n\n# BLAS Selection Output: \nThere are 4 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3               35        manual mode\n  3            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n  4            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number: ^C\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_Python/#select-an-alternative-library-atlas-libraries-in-this-case","title":"Select an alternative library (Atlas libraries in this case)","text":"<p>Open R and confirm the change of BLAS/LAPACK library using \"sessionInfo()\" function.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/","title":"UCloud Tutorial: Speed up your Linear Alegbra calculations 78 times by choosing the right RStudio version on UCloud.","text":"<p>In the context of R, there are several libraries available that interface with the BLAS and LAPACK libraries to enhance their functionality or provide alternative implementations. Here are a few notable libraries:</p> <ul> <li> <p>base R: The base R installation includes a reference BLAS implementation that provides basic linear algebra functionality. While this implementation is not as optimized as other libraries, it serves as a fallback option when more optimized libraries are not available.</p> </li> <li> <p>OpenBLAS: OpenBLAS is an optimized BLAS and LAPACK library that offers high-performance linear algebra routines. It is widely used and provides significant speed improvements over the reference BLAS implementation in base R.</p> </li> <li> <p>Intel MKL: The Intel Math Kernel Library (MKL) is a highly optimized set of mathematical functions for various platforms, including CPUs from Intel. It includes efficient implementations of BLAS and LAPACK routines and is known for its excellent performance. MKL is often favored for its outstanding performance on Intel architectures.</p> </li> <li> <p>ACML: The AMD Core Math Library (ACML) is an optimized library for AMD processors. It provides optimized BLAS and LAPACK routines tailored for AMD architectures.</p> </li> <li> <p>vecLib: vecLib is the BLAS and LAPACK library included with macOS. It provides optimized routines for Apple hardware.</p> </li> <li> <p>ATLAS: ATLAS (Automatically Tuned Linear Algebra Software) is another popular BLAS implementation that can be used as an alternative to the reference BLAS library. ATLAS utilizes an automated tuning process to generate highly optimized code specifically tailored to the host system's architecture. It offers improved performance for various linear algebra operations.</p> </li> </ul> <p>These libraries can be linked with R during installation or dynamically loaded during runtime, allowing users to choose the most suitable implementation for their specific hardware and performance requirements.</p> <p>In summary, the BLAS and LAPACK libraries are essential components for numerical computations and linear algebra in R. They provide efficient and optimized routines for fundamental linear algebra operations, and various libraries such as OpenBLAS, Intel MKL, ACML, vecLib, and ATLAS enhance their functionality or provide alternative implementations for improved performance.</p>"},{"location":"Tutorials/BLAS/BLAS_R/#rstudio-on-ucloud","title":"RStudio on UCloud","text":"<p>When looking at the different R/RStudio version available on UCloud (see below) it can be observed that some have the suffix \"_MKL\". These have \"Intel MKL\" as their default BLAS/LAPACK library, while versions with out the suffix have the \"LibBLAST\" library whihc is the base R library on linux systems.</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/#use-the-sessioninfo-function-to-check-which-blaslapack-library-is-deployed","title":"Use the \"sessionInfo()\" function to check which BLAS/LAPACK library is deployed:","text":"<p>MKL</p> <p></p> <p>LibBLAS</p> <p></p>"},{"location":"Tutorials/BLAS/BLAS_R/#benchmarking-the-two-versions","title":"Benchmarking the two versions","text":"<p>Benchmarking a simple matrix multiplication shows that the \"MKL\" is close 78 times faster than the \"LibBLAS\"!!</p> <p>MKL mean = 7.63 milliseconds</p> <p></p> <p>LiBLAS mean = 592.666 milliseconds</p> <p></p> <pre><code>library(microbenchmark)\n\nn &lt;- 1000\nmat1 &lt;- matrix(rnorm(n*n), ncol=n)\nmat2 &lt;- matrix(rnorm(n*n), ncol=n)\n\ntic()\nf &lt;- function() mat1 %*% mat2\nres &lt;- microbenchmark(f(), times=100L)\nres\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#installing-and-chaning-blas-lapack-library","title":"Installing and Chaning BLAS &amp; LAPACK Library","text":"<p>Select between the available BLAS/LAPACK libraies by posting the command below i the job terminal. </p> <pre><code># BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n# LAPACK Selection\nsudo update-alternatives --config liblapack.so.3-x86_64-linux-gnu\n\n\n\n# BLAS Selection Output: \nThere are 2 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number:\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#install-other-libraries","title":"Install other libraries.","text":"<p>Changing the BLAS/LAPACK libraries is not possible for the \"MKL\" versions of the RStudio applications on UCloud. </p> <pre><code># Update\nsudo apt-get update\n# install OpenBLAS\nsudo apt-get install libopenblas-base\n# install ATLAS\nsudo apt-get install libatlas3-base liblapack3\n\n# BLAS Selection\nsudo update-alternatives --config libblas.so.3-x86_64-linux-gnu\n\n\n# BLAS Selection Output: \nThere are 4 choices for the alternative libblas.so.3-x86_64-linux-gnu (providing /usr/lib/x86_64-linux-gnu/libblas.so.3).\n\n  Selection    Path                                                      Priority   Status\n------------------------------------------------------------\n* 0            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       auto mode\n  1            /opt/intel/oneapi/mkl/2022.1.0/lib/intel64//libmkl_rt.so   50        manual mode\n  2            /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3               35        manual mode\n  3            /usr/lib/x86_64-linux-gnu/blas/libblas.so.3                10        manual mode\n  4            /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3    100       manual mode\n\nPress &lt;enter&gt; to keep the current choice[*], or type selection number: ^C\n</code></pre>"},{"location":"Tutorials/BLAS/BLAS_R/#select-an-alternative-library-atlas-libraries-in-this-case","title":"Select an alternative library (Atlas libraries in this case)","text":"<p>Open R and confirm the change of BLAS/LAPACK library using \"sessionInfo()\" function.</p> <p></p>"},{"location":"Tutorials/Conda/","title":"Conda on UCloud","text":"<p>Package, dependency and environment management for any language\u2014Python, R and more.</p> <p>The following links provides step-by-step guides on how to install and use Conda for R and Python on a range of different UCloud applications (R Studio, VScode, JupyterLab and Terminal App).</p> <p>Using a Conda environement elimnates the need for re-installing all the needed packages/libraries when starting a UCloud Job.</p> <p>This approach is also highly useful when running multi-node Slurm Clusters. </p> <p>R</p> <p>Python</p> <p>Further documentation can be found on UCloud:</p> <p>Conda on UCloud </p>"},{"location":"Tutorials/Conda/Conda_Python/","title":"UCloud Tutorial: Using Conda for easy management of Python environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_Python/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_Python/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#installing-and-activate-python-environments","title":"Installing and activate Python environments","text":"<p>https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html </p> <pre><code># Showing available python versions\nconda search python\n\n# Installing a Python environment (Python 3.9 in this example) \nconda create -n myenv python=3.9\n\n# Or install packages during installation.\nconda create -n myenv python=3.9 numpy=1.16\n\n# Shows already installed environments (R-4.2.3 show be displayed)\nconda env list\n\n# Activate environment\nconda activate myenv\n\n#Check which Python is in path\nwhich python\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#vscode-on-ucloud","title":"VScode on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-coder-python-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Coder python UCloud job.","text":"<p>https://docs.cloud.sdu.dk/hands-on/conda-coder.html?highlight=coder</p> <p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-vscode-interface-and-open-file-and-activate-myenv-as-python-interpreter","title":"Now you can launch VSCode interface and open file and activate \u201cmyenv\u201d as python interpreter:","text":"<p>Select the menu View -&gt; Command Palette:</p> <p></p> <p>Execute the command &gt; Python: Select Intepreter:</p> <p></p>"},{"location":"Tutorials/Conda/Conda_Python/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info --envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install ipykernel:\n\nconda install ipykernel\n\n# \npython -m ipykernel install --user --name myenv --display-name \"myenv\"\n\n# De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_Python/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_Python/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which Python is in path:\nwhich python\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/python\n</code></pre>"},{"location":"Tutorials/Conda/Conda_Python/#install-libraries-and-run-python_1","title":"Install libraries and run python:","text":"<pre><code># Install conda libraries:\nconda install scikit-learn\n\n# Install pip libraries:\npip install --upgrade pip\npip install pandas\n\n# Start Python:\npython\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Conda_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Conda_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#installing-and-activate-r-environments","title":"Installing and activate R environments","text":"<p>https://docs.anaconda.com/free/anaconda/packages/using-r-language/</p> <pre><code># Installing a R environment\nconda create -n  myenv r-essentials r-base\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#install-packages-through-conda","title":"install packages through Conda","text":"<p>When using conda to install R packages, you will need to add r- before the regular package name:</p> <pre><code># For instance, if you want to install rbokeh:\nconda install r-rbokeh\n\n# or for rJava:\nconda install r-rjava\n\n# Update packages:\n\nconda update r-caret\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Conda_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Conda_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Conda_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Conda_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/","title":"UCloud Tutorial: Using Conda for easy management of R environments","text":"<p>Introduction text</p> <p>https://docs.cloud.sdu.dk/hands-on/conda-setup.html?highlight=conda</p> <p>The Conda package and environment management system is already included in few applications available on UCloud (see, e.g., JupyerLab and PyTorch). For more general uses of Conda and its powerful package manager it is convenient to create a local installation and save it in a UCloud project. Conda is included in all versions of Anaconda and Miniconda. For example, to install the latest version of Miniconda, just start any interactive app on UCloud, such as Terminal, and run the following shell commands:</p>"},{"location":"Tutorials/Conda/Mamba_R/#installing-conda-on-ucloud","title":"Installing Conda on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>Run following commands in the terminal: </p> <pre><code># Download miniconda \ncurl -s -L -o /tmp/miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install miniconda\nbash /tmp/miniconda_installer.sh -b -f -p /work/miniconda3\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#when-the-job-is-finished-copy-the-miniconda3-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-project","title":"When the job is finished copy the \u201cminiconda3\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud project.","text":""},{"location":"Tutorials/Conda/Mamba_R/#activating-conda-in-a-new-ucloud-job","title":"Activating Conda in a new UCloud Job","text":"<pre><code>#Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Initiate Conda and reboot \nconda init &amp;&amp; bash -i\n</code></pre> <pre><code>#Shows already installed environments:\nconda env list\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#installing-r-environment-using-conda","title":"Installing R environment using Conda","text":""},{"location":"Tutorials/Conda/Mamba_R/#installing-mamba-add-in","title":"Installing Mamba add-in","text":"<p>Managing R environment using Conda is facilitated by a add-in library \u201cmamba\u201d (https://astrobiomike.github.io/unix/conda-intro#bonus-mamba-no-5).</p> <pre><code># Installing mamba add-in:\nconda install -n base -c conda-forge mamba\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#installing-and-activate-r-environment-with-mamba","title":"Installing and activate R environment with mamba","text":"<p>https://astrobiomike.github.io/R/managing-r-and-rstudio-with-conda</p> <pre><code>#Showing available R versions\nmamba search -c conda-forge r-base\n\n#Installing a R environment (R-4.2.3 in this example) \nmamba create -n myenv -y -c conda-forge r-base=4.2.3\n\n#Or install packages during installation.\nmamba create -n myenv -y -c conda-forge r-base=4.2.3 r-tidyverse\n\n#Shows already installed environments (\"myenv\" should be displayed)\nconda env list\n\n#Activate environment\nconda activate myenv\n\n#Check which R is in path\nwhich R\n\n#Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#start-r-and-run-code-or-install-packages","title":"Start R and run code or install packages:","text":"<pre><code># Install packages:\nR install.packages(\u201ctidymodels\u201d)\n\n# If the user wish to run this environment with \u201cJupyterLab\u201d then it is advised to install \u201ciRkernel\u201d at this point:\nR install.packages(\"IRkernel\")\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#r-studio-on-ucloud","title":"R Studio on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-rstudio-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Rstudio UCloud job.","text":"<p>Make sure that Rstudio UCloud job is based on the save R version ad the installed Conda R environment (\u201cmyenv\u201d).</p> <p>Navigate to the R console: </p> <p></p> <pre><code># Setting \"myenv\" library into library path of the active R kernel \n.libPaths(\"/work/miniconda3/envs/myenv/lib/R/library\")\n\n# Check if right path is set: \n.libPaths()\n\n# Now \"myenv\" packages are available and new packages can be installed:\ninstall.packages(\"googlesheets4\")\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#be-attentive-that-some-dependencies-may-be-pre-installed-in-the-r-studio-ucloud-job-which-may-be-missing-when-loading-this-packages-in-another-ucloud-app-eg-terminal-or-jupyterlab-app","title":"Be attentive that some dependencies may be pre-installed in the \u201cR studio\u201d UCloud job which may be missing when loading this packages in another UCloud app (e.g. Terminal or JupyterLab app).","text":""},{"location":"Tutorials/Conda/Mamba_R/#jupyterlab-on-ucloud","title":"JupyterLab on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-jupyterlab-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new JupyterLab UCloud job.","text":"<p>In terminal add conda environment:</p> <pre><code># Init conda:\nconda init &amp;&amp; bash -i\n\n# JupyterLab app on UCloud is Conda based with a installation found on the following path: \nconda info \u2013-envs\n\n# Output should be: \n/opt/conda\n\n# Create symbolic link for R environment between the two conda installations: \nsudo ln -s /work/miniconda3/envs/myenv /opt/conda/envs\n\n# Shows already installed environments (Now \u201cmyenv\u201d is available):\nconda env list\n\n# Activate environment:\nconda activate myenv\n</code></pre> <pre><code># Install iRkernel R package:\n\nR install.packages(\"IRkernel\") # Can be problematic to install at this point\nR -e \"IRkernel::installspec(name = 'myenv', displayname = 'myenv')\"\n</code></pre> <pre><code># De-activate environment:\nconda deactivate\n</code></pre>"},{"location":"Tutorials/Conda/Mamba_R/#now-you-can-launch-jupyterlab-interface-and-the-myenv-environment-should-be-available-on-the-frontpage","title":"Now you can launch JupyterLab interface and the \u201cmyenv\u201d environment should be available on the frontpage.","text":""},{"location":"Tutorials/Conda/Mamba_R/#terminal-app-on-ucloud","title":"Terminal app on UCloud","text":""},{"location":"Tutorials/Conda/Mamba_R/#add-the-miniconda3-folder-when-starting-the-new-terminal-app-ucloud-job","title":"Add the \u201cminiconda3\u201d folder when starting the new Terminal App UCloud job.","text":"<pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which R is in path:\nwhich R\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/R\n</code></pre> <pre><code># Start R and run code or install packages:\nR install.packages(\u201ctidymodels\u201d)\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/","title":"UCloud Tutorial: Transfer large data to UCloud using Rsync","text":"<p>Rsync, short for \"remote synchronization,\" is a powerful and widely used file synchronization and transfer utility. It enables efficient copying and updating of files between different locations, whether they are on the same system or across a network. Rsync is particularly valuable for managing large data sets or performing incremental backups.</p> <p>What sets Rsync apart is its ability to synchronize files by transferring only the differences between the source and destination files. This delta transfer mechanism greatly reduces the amount of data that needs to be transmitted, making Rsync highly efficient, even for large files or slow network connections.</p> <p>Rsync also offers several advanced features, such as compression, encryption, and the ability to preserve various file attributes, such as permissions, timestamps, and symbolic links. It supports both local file copying and remote transfers via SSH, allowing secure synchronization between different systems.</p> <p>With its flexibility, speed, and efficient use of network resources, Rsync has become a go-to tool for tasks like backup and mirroring, remote file distribution, and content deployment. It has a command-line interface, making it scriptable and suitable for both one-time transfers and automated, scheduled tasks.</p> <p>UCloud documentation on Rsync</p>"},{"location":"Tutorials/Sync/Rsync/#installing-ubuntu-on-local-machine-for-windows","title":"Installing Ubuntu on local machine (For Windows)","text":"<p>Rsync is not natively available for Windows. However, you can install Rsync on Windows using a third-party implementation such as Cygwin or DeltaCopy. Alternatively, you can install Ubuntu on Windows which then comes whic includes Rsync.</p> <p>In this guide we will use Rsync through Ubuntu. For more infomation and video tutorials can be found here.</p>"},{"location":"Tutorials/Sync/Rsync/#create-a-ssh-key-within-your-ubuntu-environment","title":"Create a SSH-key within your Ubuntu environment","text":"<p>Despite already having a shh-key (in your windows environment) the easiest will be to create a new SSH-key within your Ubuntu environment. Open a terminal and follow the few steps below.</p> <p>More information on how to generate a SSH key can be found here</p> <pre><code># Activate Ubuntu \nwsl\n\n# For linux only \nsudo apt install openssh-client\n\n# Create key\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#copy-public-ssh-key","title":"Copy public SSH-key","text":"<pre><code># Open Public Key\nvim /home/user/.ssh/id_rsa.pub\n\n# highlight public key with mouse and copy using \"ctrl+c\"\n</code></pre>"},{"location":"Tutorials/Sync/Rsync/#add-public-ssh-key-to-ucloud","title":"Add public SSH key to UCloud","text":""},{"location":"Tutorials/Sync/Rsync/#step-1-on-ucloud-go-to-resources-shh-keys-create-ssh-key","title":"Step 1: On UCloud go to \"Resources -&gt; SHH-Keys -&gt; Create SSH key\"","text":""},{"location":"Tutorials/Sync/Rsync/#step-2-paste-pulic-key-give-a-meaningful-name-and-press-add-ssh-key","title":"Step 2: Paste pulic key, give a meaningful name and press \"Add SSH key\".","text":"<p>More information can be found in the UCloud documentation.</p>"},{"location":"Tutorials/Sync/Rsync/#start-rsync-job","title":"Start Rsync Job","text":""},{"location":"Tutorials/Sync/Rsync/#step-1-start-rsync-job-by-filling-out-the-necessary-fields","title":"Step 1: Start Rsync Job by filling out the necessary fields","text":""},{"location":"Tutorials/Sync/Rsync/#step-2-when-job-ready-please-locate-the-ssh-port-which-is-randomly-generated-in-the-cas-below-the-shh-port-is-2167","title":"Step 2: When job ready please locate the SSH port which is randomly generated. In the cas below the SHH port is 2167.","text":""},{"location":"Tutorials/Sync/Rsync/#connect-from-local-machine-using-ssh","title":"Connect from local machine using SSH.","text":"<p>Open Terminal and follow steps below.</p> <pre><code># Activate Ubuntu (for Windows)\nwsl\n\n# SHH connect using the command marked with in the figure above.\nssh ucloud@ssh.cloud.sdu.dk -p 2167\n</code></pre> <p>If sucessfull you should get the output shown in the figure below.</p> <p></p>"},{"location":"Tutorials/Sync/Rsync/#transfer-data-using-rsync","title":"Transfer data using Rsync","text":"<pre><code># Navigate to path contain the folder of files to transfer - Alternatively you can open terminal directly in the right directory to skip step below.\ncd \"path/of/folders-or-files\" # Activate Ubuntu (for Windows)\nwsl\n\n# SSH transfer \"myfolder\" to /work directory on UCloud \n\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p 2167\" ./myfolder/ ucloud@ssh.cloud.sdu.dk:/work/myfolder </code></pre>"},{"location":"Tutorials/VMs/","title":"Virtual Machines on UCloud","text":"<p>How to Generate SSH key</p> <p>Acessing VM using SSH</p>"},{"location":"Tutorials/VMs/condaVM/","title":"UCloud Tutorial: Using Conda for easy workflow deployment on AAU GPU VMs","text":""},{"location":"Tutorials/VMs/condaVM/#initial-installation-of-conda-on-a-aau-vm-job","title":"Initial installation of Conda on a AAU VM job","text":""},{"location":"Tutorials/VMs/condaVM/#connect-to-vm-using-ssh","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#update-vm","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-headless-460 nvidia-utils-460 -y\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#download-and-install-conda","title":"Download and Install Conda","text":"<pre><code># Download miniconda \ncurl -s -L -o miniconda_installer.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # Install miniconda\nbash miniconda_installer.sh -b -f -p miniconda3\n\n# Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# initialize conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#re-connect-to-vm-using-ssh","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#check-nvidia-driver-configuration","title":"Check nvidia driver Configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output\nMon Aug  7 09:38:25 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    31W /  70W |      0MiB / 15109MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#create-conda-environment-and-test-gpu-configuration","title":"Create conda environment and test GPU configuration","text":"<pre><code># Create conda environment \nconda deactivate\nconda create --name my_env python\nconda activate my_env\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n\n# install tensorflow\npip install --upgrade pip; pip install tensorflow\n\n# test if tensorflow is probably configurated:\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n# Expected Output\n2023-08-07 09:58:29.129577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -&gt; device: 0, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\ntf.Tensor(-875.0791, shape=(), dtype=float32)\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#gpu-conda-environment-is-ready-to-use","title":"GPU conda environment is ready to use","text":""},{"location":"Tutorials/VMs/condaVM/#compress-conda-installation-to-targz-file","title":"Compress Conda installation to tar.gz file","text":"<pre><code>tar -czvf /home/ucloud/miniconda3.tar.gz /home/ucloud/miniconda3\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-miniconda3targz-to-local-pc-using-ssh-copy","title":"Transfer \"miniconda3.tar.gz\" to local PC using SSH-Copy","text":"<p>Open a 2nd instance of a terminal app on local machine</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/miniconda3 \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-conda-install-to-a-new-aau-vm-job","title":"Transfer Conda install to a new AAU VM job","text":""},{"location":"Tutorials/VMs/condaVM/#connect-to-vm-using-ssh_1","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#update-vm_1","title":"Update VM","text":"<pre><code>sudo apt update\nsudo apt upgrade -y sudo apt install nvidia-headless-460 nvidia-utils-460 -y\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#transfer-miniconda3targz-from-local-pc-to-vm-using-ssh-copy","title":"Transfer \"miniconda3.tar.gz\" from local PC to VM using SSH-Copy","text":"<p>Open a 2nd instance of a terminal app on local machine</p> <pre><code>scp -r \"C:\\path-to-folder\\miniconda.tar.gz ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#unzip-targz","title":"Unzip tar.gz","text":"<p>Move back to the terminal app connected to VM and run following command:</p> <pre><code>tar -xzf miniconda.tar.gz\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#set-conda-on-path-and-initialise","title":"Set Conda on path and initialise","text":"<pre><code># Set conda to path\nexport PATH=/home/ucloud/miniconda3/bin:$PATH # Set conda to path\n\n# init conda\nconda init &amp;&amp; bash -i\n\n# Reboot VM\nsudo reboot\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#re-connect-to-vm-using-ssh_1","title":"Re-connect to VM using SSH","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#check-nvidia-driver-configuration_1","title":"Check nvidia driver configuration","text":"<pre><code>nvidia-smi\n\n# Expected Output:\nMon Aug  7 09:41:34 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   51C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#activate-conda-environment-and-test-gpu-configuration","title":"Activate conda environment and test GPU configuration","text":"<pre><code># Create conda environment \nconda deactivate\nconda create --name my_env python\nconda activate my_env\n\n# Install cudatoolkit and cudnn\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n\n# Set pre-installed conda libraries to path (including cudatoolkit=11.2 cudnn=8.1.0 )\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n\n# install tensorflow\npip install --upgrade pip; pip install tensorflow\n\n# test if tensorflow is probably configurated:\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n\n# Expected Output\n2023-08-07 09:58:29.129577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -&gt; device: 0, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\ntf.Tensor(-875.0791, shape=(), dtype=float32)\n</code></pre>"},{"location":"Tutorials/VMs/condaVM/#gpu-conda-environment-is-ready-to-use_1","title":"GPU conda environment is ready to use","text":""},{"location":"Tutorials/VMs/connectVM/","title":"SSH to Server through local Terminal","text":"<p>Add public SSH key while starting a VM job</p> <p></p> <p>Identify VM IP when UCloud job is ready.</p> <p></p>"},{"location":"Tutorials/VMs/connectVM/#from-local-terminal-connect-to-vm-by","title":"From Local Terminal connect to VM by:","text":"<pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#transfer-files-and-folders-ssh-copy","title":"Transfer Files and Folders (SSH-Copy)","text":""},{"location":"Tutorials/VMs/connectVM/#to-vm","title":"To VM","text":"<p>Open a second terminal (1st terminal is connected to the VM):</p> <pre><code>scp -r \"C:\\path-to-folder-or-files\" ucloud@IP_address_from_the_red_mark:\n</code></pre>"},{"location":"Tutorials/VMs/connectVM/#from-vm","title":"From VM","text":"<p>Open a second terminal (1st terminal is connected to the VM)</p> <pre><code>scp -r ucloud@IP_address_from_the_red_mark:/home/ucloud/folder \"C:\\path-to-folder\"\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/","title":"UCloud Tutorial: Setting up an interactive jupyter notebook session on AAU VMs","text":""},{"location":"Tutorials/VMs/jupyterVM/#connect-to-vm-using-ssh","title":"Connect to VM using SSH","text":"<p>Open a terminal app on local machine and SSH onto the VM:</p> <pre><code>ssh ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#install-jupyter-if-needed","title":"Install jupyter if needed:","text":"<pre><code># Using pip\npip install jupyterlab\n\n# Using conda if conda environment are utilised \nconda install jupyter\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#make-sure-jupyter-is-installed","title":"Make sure jupyter is installed","text":"<p>In this case jupyter is installed and activated through a conda environment. Please see </p> <pre><code>which jupyter\n\n# Example Output:\n/home/ucloud/miniconda3/envs/my_env/bin/jupyter\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#start-jupyter-notebook-from-remote-server","title":"Start Jupyter Notebook from remote server","text":"<pre><code>jupyter notebook --no-browser --port=8080\n\n# Output\n\n[I 10:26:32.873 NotebookApp] Serving notebooks from local directory: /home/ucloud\n[I 10:26:32.873 NotebookApp] The Jupyter Notebook is running at:\n[I 10:26:32.873 NotebookApp] http://localhost:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n[I 10:26:32.873 NotebookApp]  or http://127.0.0.1:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n[I 10:26:32.873 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:26:32.899 NotebookApp]\n\nTo access the notebook, open this file in a browser:\nfile:///home/ucloud/.local/share/jupyter/runtime/nbserver-3074-open.html\nOr copy and paste one of these URLs:\nhttp://localhost:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\nor http://127.0.0.1:8080/?token=b754cbea9f5a6640e647f21c7d2e7112a6954eb26f032d73\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#ssh-connect-to-vm-using-a-new-terminal-app-on-local-machine","title":"SSH connect to VM using a new terminal app on local machine","text":"<p>Open a 2nd instance of Terminal on Local machine</p> <pre><code>ssh -L 8080:localhost:8080 ucloud@IP_address_from_the_red_mark\n</code></pre>"},{"location":"Tutorials/VMs/jupyterVM/#now-press-the-links-in-the-output-above-and-it-should-open-a-jupyter-notebook","title":"Now press the links in the output above and it should open a jupyter notebook","text":""},{"location":"Tutorials/VMs/shh/","title":"How to Generate a SSH key","text":"<p>In order to access the VM it is necessary to create Secure Shell Protocol (SSH) keys more specifically a public key (shareable part) and a private key (kept safe locally).</p>"},{"location":"Tutorials/VMs/shh/#on-windows-linux-systems","title":"On Windows &amp; Linux Systems","text":"<p>To generate your public key, find and open terminal and type: </p> <pre><code># For linux only\nsudo apt install openssh-client\n\n\n# For both Windows &amp; Linux\nssh-keygen\n\n# Output: \nGenerating public/private rsa key pair.\nEnter file in which to save the key (C:\\Users\\user/.ssh/id_rsa): # press enter\nEnter passphrase (empty for no passphrase):                         # press enter\nEnter same passphrase again:                                        # press enter\nYour identification has been saved in C:\\Users\\user/.ssh/id_rsa.\nYour public key has been saved in C:\\Users\\user/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:V4jnGjEIpUYU4tghvdfdkJj+hnd8t/E70SNGdsdepmX7E ggs\\use@CBSxxxx\nThe key's randomart image is:\n+---[RSA 3072]----+\n|o o.=o....       |\n|+O++.o . .. .    |\n|=+=*o .. + o .   |\n|..oo.    = + .   |\n| ..o . .S = o o  |\n|  o . o .O o E   |\n|       o= . + .  |\n|   ..   .  = .   |\n|         .. o    |\n+----[SHA256]-----+\n</code></pre>"},{"location":"Tutorials/VMs/shh/#manually-locate-open-and-copy-our-public-key-from-id_rsapub-file","title":"Manually locate, open and copy our public key from id_rsa.pub file.","text":""},{"location":"Tutorials/VMs/shh/#on-windows","title":"On Windows","text":"<p>the file might be here: \"C:\\Users\\write_your_user_name.ssh\"</p>"},{"location":"Tutorials/VMs/shh/#on-linux","title":"On Linux","text":"<p>The generated SSH key will be by stored under ~/.ssh/id_rsa.pub by default.</p> <p>More information can be found at https://genome.au.dk/docs/getting-started/#public-key-authentication </p>"},{"location":"UCloud_BatchMode/","title":"UCloud Tutorial: Batch Processing on UCloud","text":"<p>Running non-interactive script in \"batch mode\" can help overcome the UCloud capacity issues as jobs can be started any time and then executed whenever the systems resources permits.  </p> <p>Many applications already have a \"Batch Processing\" UCloud functionality:</p> <ul> <li>Batch Processing</li> <li>RStudio</li> <li>JupyterLab</li> <li>Matlab</li> <li>PyTorch</li> <li>Tensorflow</li> <li>Spark Cluster</li> <li>Terminal/SLURM Cluster</li> </ul> <p>Application that currently does not have this UCloud functionality (e.g. Stata) can instead run batch mode computations using the terminal app. See the following tutorials:</p> <ul> <li>Stata</li> </ul>"},{"location":"UCloud_BatchMode/Stata/","title":"UCloud Tutorial: Run Stata in Batch Mode on UCloud","text":"<p>This is an approach to adress the UCloud capacity issues. </p> <p>UCloud batch processing apps are scheduled to run as resources permit without end user interaction. It allows </p>"},{"location":"UCloud_BatchMode/Stata/#get-stata-license-and-installation-file-cbs-users","title":"Get Stata license and Installation file (CBS Users)","text":"<p>Follow the instructions to get a Stata license at CBS https://studentcbs.sharepoint.com/sites/ITandCampus/SitePages/en/Free-software.aspx</p> <p>You will recieve an email with license and installation information (see image below).</p> <p></p> <p>Download the installation file (Stata17Linux64.tar) and upload this to your UCloud directory.</p> <p></p>"},{"location":"UCloud_BatchMode/Stata/#installing-stata-on-ucloud","title":"Installing Stata on UCloud","text":""},{"location":"UCloud_BatchMode/Stata/#launch-a-terminal-app-ucloud-job-and-include-the-stata-installation-file-stata17linux64tar","title":"Launch a \"Terminal App\" UCloud Job and include the stata installation file (Stata17Linux64.tar)","text":"<p>Run following commands in the terminal: </p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Unzip installation file to temp folder\nsudo -s\nmkdir /tmp/statafiles\ncd /tmp/statafiles\ntar -zxf /work/install/Stata17Linux64.tar.gz\n\n# Install Stata on in \"/work/stata17\". Say yes when asked during installtion\nmkdir /work/stata17 cd /work/stata17 /tmp/statafiles/install\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Initialize Stata\nsudo /work/stata17/stinit\n\n# Follow instructions and add \"Serial number\", \"Code\" and \"Authorization\" from the Stata license mail\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata # or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"UCloud_BatchMode/Stata/#end-job-and-copy-the-stata17-folder-from-ucloud-job-folder-to-a-folder-you-want-within-your-ucloud-directory","title":"End job and copy the \u201cstata17\u201d folder from UCloud \u201cJob\u201d folder to a folder you want within your UCloud directory.","text":""},{"location":"UCloud_BatchMode/Stata/#activate-stata-installation-in-a-new-terminal-job","title":"Activate Stata installation in a new terminal job","text":"<p>Add the stata17 folder to the job</p> <p></p> <pre><code># Install dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata to Unix path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Check stata installation\nwhich stata\n\n# Run stata\nstata # or\nstata-se\n# or\nstata-mp\n</code></pre>"},{"location":"UCloud_BatchMode/Stata/#run-stata-scrip-in-batch-mode-n-a-new-terminal-job","title":"Run Stata scrip in batch mode n a new terminal job","text":"<p>Add the \"stata17\" and other relevant folder to the job:</p> <p></p> <p>Add a bash script(.sh) under \"Batch processing\" as one of the \"Optional Parameters\":</p> <p> </p> <p>Below shown bash script can be downloaded from here. Use this as a template or create your own bash script.</p> <p>More information on how to run Stata in batch mode can be found here: https://www.stata.com/support/faqs/unix/batch-mode/</p> <pre><code>#!/bin/bash\n\n\n# Installing dependencies\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libncurses5 libncurses5:i386 -y\n\n# Set stata17 on UNIX path\nexport PATH=\"/work/stata17:$PATH\"\n\n# Run stata in Batch mode\nstata -b do filename &amp; # USER SHOULD CHANGE THIS LINE (SEE LINK Above)\n</code></pre>"},{"location":"UCloud_SlurmCluster/","title":"SLURM Clusters on UCloud","text":"<ul> <li>Run Multi-node SLURM Cluster on UCloud</li> </ul>"},{"location":"UCloud_SlurmCluster/#files","title":"Files","text":""},{"location":"UCloud_SlurmCluster/#launch-file","title":"Launch File","text":"<ul> <li>slurm-launch.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#ray-python","title":"Ray (Python)","text":"<ul> <li>slurm-template_ray.sh</li> <li>SklearnRay.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#dask-python","title":"Dask (Python)","text":"<ul> <li>slurm-template_dask.sh</li> <li>SklearnDask.py</li> </ul>"},{"location":"UCloud_SlurmCluster/#doparallel-r","title":"doParallel (R)","text":"<ul> <li>slurm-template_R.sh</li> <li>doParallel.r</li> <li>tidyModel_RF.r</li> <li>tidyModel_NN.r</li> </ul>"},{"location":"UCloud_SlurmCluster/Ray/","title":"Ray","text":""},{"location":"UCloud_SlurmCluster/Ray/#example-using-ray","title":"Example using Ray","text":"<p>In terminal run:</p> <pre><code>python slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Output\n\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#open-extra-terminal-for-three-nodes","title":"Open extra Terminal for three Nodes","text":""},{"location":"UCloud_SlurmCluster/Ray/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/Ray/#observed-that-the-work-is-disbrubted-across-all-three-nodes","title":"Observed that the work is disbrubted across all three nodes.","text":"<p>This may look different for different backends (e.g. Dask). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"},{"location":"UCloud_SlurmCluster/Ray/#output-files","title":"Output files","text":""},{"location":"UCloud_SlurmCluster/Ray/#the-autogenerated-slurm-script-slurmtest_0425-1208sh","title":"The autogenerated SLURM script (SlurmTest_0425-1208.sh)","text":"<pre><code>#!/bin/bash\n# shellcheck disable=SC2206\n# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!\n# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!\n\n#SBATCH --job-name=SlurmTest_0425-1208\n#SBATCH --output=SlurmTest_0425-1208.log\n\n### This script works for any number of nodes, Ray will find and manage all resources\n#SBATCH --nodes=3\n#SBATCH --exclusive\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n##SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE} #De-activated by KGP 230317\n\n# Load modules or your own conda environment here\n# module load pytorch/v1.4.0-gpu\n# conda activate ${CONDA_ENV}\n\n\n# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====\n\necho $SLURM_JOB_NODELIST\n\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\") # Getting the node names\n\nnodes_array=($nodes)\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w \"$node_1\" hostname --ip-address) # making redis-address\n\n# if we detect a space character in the head node IP, we'll\n# convert it to an ipv4 address. This step is optional.\nif [[ \"$ip\" == *\" \"* ]]; then\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$ip\"\nif [[ ${#ADDR[0]} -gt 16 ]]; then\nip=${ADDR[1]}\nelse\nip=${ADDR[0]}\nfi\necho \"IPV6 address detected. We split the IPV4 address as $ip\"\nfi\n\nport=6379\nip_head=$ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"STARTING HEAD at $node_1\"\nsrun --nodes=1 --ntasks=1 -w \"$node_1\" ray start --head --node-ip-address=\"$ip\" --port=$port --block &amp;\nsleep 30\n\n#worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\n#export NB_WORKERS=$((${SLURM_JOB_NUM_NODES-1})) #number of nodes other than the head node\n#echo ${NB_WORKERS}\n\nexport NB_WORKERS=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\necho \"STARTING ${NB_WORKERS} WORKERS\"\nfor ((i = 1; i &lt;= NB_WORKERS; i++)); do\nnode_i=${nodes_array[$i]}\necho \"STARTING WORKER $i at $node_i\"\nsrun --nodes=1 --ntasks=1 -w \"$node_i\" ray start --address \"$ip_head\" --block &amp;\nsleep 5\ndone\n\n# ===== Call your code below =====\necho \"RUNNING CODE: python /work/data/SklearnRay.py\"\npython /work/data/SklearnRay.py\n</code></pre>"},{"location":"UCloud_SlurmCluster/Ray/#autogenerated-log-file-slurmtest_0425-1208log","title":"Autogenerated log file (SlurmTest_0425-1208.log)","text":"<pre><code>node[0-2]\nIPV6 address detected. We split the IPV4 address as 10.42.47.86\nIP Head: 10.42.47.86:6379\nSTARTING HEAD at node0\n2023-04-25 12:08:40,054 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\nSTARTING 2 WORKERS\nSTARTING WORKER 1 at node1\n2023-04-25 12:08:38,026 INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-04-25 12:08:38,026 INFO scripts.py:710 -- Local node IP: 10.42.47.86\n2023-04-25 12:08:41,222 SUCC scripts.py:747 -- --------------------\n2023-04-25 12:08:41,222 SUCC scripts.py:748 -- Ray runtime started.\n2023-04-25 12:08:41,223 SUCC scripts.py:749 -- --------------------\n2023-04-25 12:08:41,223 INFO scripts.py:751 -- Next steps\n2023-04-25 12:08:41,223 INFO scripts.py:752 -- To connect to this Ray runtime from another node, run\n2023-04-25 12:08:41,223 INFO scripts.py:755 --   ray start --address='10.42.47.86:6379'\n2023-04-25 12:08:41,223 INFO scripts.py:771 -- Alternatively, use the following Python code:\n2023-04-25 12:08:41,223 INFO scripts.py:773 -- import ray\n2023-04-25 12:08:41,223 INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='10.42.47.86')\n2023-04-25 12:08:41,223 INFO scripts.py:790 -- To see the status of the cluster, use\n2023-04-25 12:08:41,223 INFO scripts.py:791 --   ray status\n2023-04-25 12:08:41,223 INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.\n2023-04-25 12:08:41,224 INFO scripts.py:809 -- To terminate the Ray runtime, run\n2023-04-25 12:08:41,224 INFO scripts.py:810 --   ray stop\n2023-04-25 12:08:41,224 INFO scripts.py:891 -- --block\n2023-04-25 12:08:41,224 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:41,224 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nSTARTING WORKER 2 at node2\n2023-04-25 12:08:48,882 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:48,933 I 2244 2244] global_state_accessor.cc:356: This node has an IP address of 10.42.28.36, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:48,859 INFO scripts.py:866 -- Local node IP: 10.42.28.36\n2023-04-25 12:08:48,935 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:48,935 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:48,935 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:48,935 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:48,935 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:48,935 INFO scripts.py:891 -- --block\n2023-04-25 12:08:48,935 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:48,935 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\nRUNNING CODE: python /work/data/SklearnRay.py\n2023-04-25 12:08:54,215 WARNING utils.py:652 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 7.5 to 7.\n[2023-04-25 12:08:54,271 I 956 956] global_state_accessor.cc:356: This node has an IP address of 10.42.34.213, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\n2023-04-25 12:08:54,135 INFO scripts.py:866 -- Local node IP: 10.42.34.213\n2023-04-25 12:08:54,274 SUCC scripts.py:878 -- --------------------\n2023-04-25 12:08:54,275 SUCC scripts.py:879 -- Ray runtime started.\n2023-04-25 12:08:54,275 SUCC scripts.py:880 -- --------------------\n2023-04-25 12:08:54,275 INFO scripts.py:882 -- To terminate the Ray runtime, run\n2023-04-25 12:08:54,275 INFO scripts.py:883 --   ray stop\n2023-04-25 12:08:54,275 INFO scripts.py:891 -- --block\n2023-04-25 12:08:54,275 INFO scripts.py:892 -- This command will now block forever until terminated by a signal.\n2023-04-25 12:08:54,275 INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n2023-04-25 12:09:24,758 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.42.47.86:6379...\n2023-04-25 12:09:24,775 INFO worker.py:1553 -- Connected to Ray cluster.\n2023-04-25 12:09:25,073 WARNING pool.py:604 -- The 'context' argument is not supported using ray. Please refer to the documentation for how to control ray initialization.\nFitting 10 folds for each of 500 candidates, totalling 5000 fits\n209.00055767036974\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nslurmstepd-node2: error: *** STEP 2.3 ON node2 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node0: error: *** STEP 2.1 ON node0 CANCELLED AT 2023-04-25T12:12:54 ***\nslurmstepd-node1: error: *** STEP 2.2 ON node1 CANCELLED AT 2023-04-25T12:12:54 ***\nsrun: error: node2: task 0: Exited with exit code 1\nsrun: error: node1: task 0: Exited with exit code 1\nsrun: error: node0: task 0: Exited with exit code 1\n</code></pre> <pre><code>\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/","title":"UCloud Tutorial: Run Multi-node SLURM Cluster on UCloud","text":"TutorialLaunch File (slurm-launch.py)Ray (Python)Dask (Python)doParallel (R) slurm-template_ray.shSklearnRay.py slurm-template_dask.shSklearnDask.py slurm-template_R.shdoParallel.rtidyModel_RF.r"},{"location":"UCloud_SlurmCluster/SLURM/#launch-a-terminal-app-ucloud-job","title":"Launch a \"Terminal App\" UCloud Job","text":"<p>In addition to the normal setting fill out the following options (See figure below).</p> <p>In this example launched as cluster consisting of 3 nodes with three folder added to the launch:</p> <ul> <li>\"miniconda3\"  - contains the conda environment I want to deploy across the different nodes.</li> <li>\"SLURM_deployment\" - contains the easy-to-use deployment scripts provided in this tutorial. </li> <li>\"SLURM_scripts\" - contains the user specific script and data to run on the cluster.</li> </ul> <p>In this example Conda is used for package and evironment management. Check here for more information on Conda on UCloud.</p> <p></p>"},{"location":"UCloud_SlurmCluster/SLURM/#when-the-job-has-started-open-terminal-for-node-1","title":"When the job has started open Terminal for Node 1","text":"<p>Run following commands in the terminal: </p> <pre><code># activate SLURM Cluster if not activated in the step above\ninit_slurm_cluster\n\n# List Avaliable nodes\nsinfo -N -l\n</code></pre> <p>The controller node is always the first node. Called \"node0\" in within SLURM but called \"Node 1\" in the UCloud interface). All additional nodes are named sequentially. For example, a cluster consisting of three full u1-standard nodes is configured as follows:</p> <pre><code>NODELIST   NODES PARTITION     STATE CPUS   S:C:T MEMORY\nnode0         1     CLOUD*     idle   64   1:64:1 385024\nnode1         1     CLOUD*     idle   64   1:64:1 385024\nnode2         1     CLOUD*     idle   64   1:64:1 385024\n</code></pre> <p>But called Node 1, Node 2 and Node 3 in the UCloud interface.</p>"},{"location":"UCloud_SlurmCluster/SLURM/#acitvate-conda-environment","title":"Acitvate Conda Environment","text":"<p>In terminal add conda environment:</p> <pre><code># Running a new UCloud run the following lines in the terminal to activate Conda:\nsudo ln -s /work/miniconda3/bin/conda /usr/bin/conda\n\n# Init Conda:\nconda init &amp;&amp; bash -i\n\n# Shows already installed environments:\nconda env list\n\n# Activate environment:\nconda activate myenv\n\n# Check which environment is in path (e.g. X = python,R..)\nwhich X # (e.g. X = python,R..)\n\n# Output should be: \n/work/miniconda3/envs/myenv/bin/X # (e.g. X = python,R..)\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-deployment-scripts","title":"SLURM deployment scripts","text":"<p>The SLURM deployment script (\"slurm-launch.py\") have been adopted from  Ray documentation to support the addition of other python libraries (Dask, ipyparallel) and other languages (e.g. R).</p>"},{"location":"UCloud_SlurmCluster/SLURM/#slurm-launchpy","title":"slurm-launch.py","text":"<p>\"slurm-launch.py\" auto-generates SLURM scripts and launch. slurm-launch.py uses an underlying template (e.g. \"slurm-template_ray.sh\" or \"slurm-template_dask.sh\") and fills out placeholders given user input.</p> <pre><code># Change path:\ncd /work/SLURM_deployment\n\n# Python with Ray\npython slurm-launch.py --script slurm-template_ray.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnRay.py\" --num-nodes 3\n\n# Python with Dask\npython slurm-launch.py --script slurm-template_dask.sh --exp-name SlurmTest --command \"python /work/SLURM_scripts/SklearnDask.py\" --num-nodes 3 --nprocs 8 --nthreads 1\n\n# R with doParallel\npython slurm-launch.py --script slurm-template_R.sh --exp-name SlurmTest --command \"Rscript --vanilla /work/SLURM_scripts/doParallel.r\" --num-nodes 3 --nprocs 8 --nthreads 1 # Example of Output\nStarting to submit job!\nJob submitted! Script file is at: &lt;SlurmTest_0425-1208.sh&gt;. Log file is at: &lt;SlurmTest_0425-1208.log&gt;\nSubmitted batch job 2\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#addditionel-options","title":"Addditionel options","text":"<pre><code>--exp-name          # The experiment name. Will generate {exp-name}_{date}-{time}.sh and {exp-name}_{date}-{time}.log.\n--command           # The command you wish to run. For example: rllib train XXX or python XXX.py.\n--node (-w)         # The specific nodes you wish to use, in the same form as the output of sinfo. Nodes are automatically assigned if not specified.\n--num-nodes (-n)    # The number of nodes you wish to use. Default: 1.\n--partition (-p):   # The partition you wish to use. Default: \u201c\u201d, will use user\u2019s default partition.\n--load-env:         # The command to setup your environment. For example: module load cuda/10.1. Default: \u201c\u201d.\n--nprocs: --nthreads:\n</code></pre>"},{"location":"UCloud_SlurmCluster/SLURM/#open-extra-terminal-for-the-three-nodes","title":"Open extra terminal for the three nodes","text":""},{"location":"UCloud_SlurmCluster/SLURM/#run-top-command-is-used-to-show-the-linux-processes","title":"Run \"top\" command is used to show the Linux processes.","text":""},{"location":"UCloud_SlurmCluster/SLURM/#observed-that-the-work-is-distibuted-across-all-three-nodes","title":"Observed that the work is distibuted across all three nodes.","text":"<p>This may look different for different frameworks (e.g. Ray, Dask, R). It should be noted that in this example on 8 core nodes were used. Full nodes (64 cores) will generate alot more processes.</p> <p></p>"}]}